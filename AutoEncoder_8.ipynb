{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4000\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train_data.csv\")\n",
    "test = pd.read_csv(\"./data/test_data.csv\")\n",
    "train = train.drop(train.iloc[581:597,:].index.values) # motor vibe 중 이상치 데이터 삭제\n",
    "\n",
    "ss = RobustScaler()\n",
    "\n",
    "scaled_train = ss.fit_transform(train)\n",
    "scaled_test = ss.transform(test)\n",
    "\n",
    "train_data = scaled_train\n",
    "test_data = scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = keras.losses.MeanSquaredError()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = keras.layers.Dense(8, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(1e-9))\n",
    "        self.decoder = keras.layers.Dense(units=8,input_shape=[2463, 1, 8])\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        if training:\n",
    "            self.encoder.trainable = True\n",
    "            self.decoder.trainable = True\n",
    "\n",
    "        else:\n",
    "            self.encoder.trainable = False\n",
    "            self.decoder.trainable = False\n",
    "\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train(self):\n",
    "        self.encoder.trainable = True\n",
    "        self.decoder.trainable = True\n",
    "\n",
    "    def encoder_freeze(self):\n",
    "        self.encoder.trainable = False\n",
    "\n",
    "    def decoder_freeze(self):\n",
    "        self.decoder.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, epochs, batch, loss_function, optimizer, patience=10):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch = batch\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.min_loss = np.Inf\n",
    "        self.counter = 0\n",
    "        self.history = {'train_loss':[], 'val_loss':[]}\n",
    "\n",
    "    def train(self, train_data, valid=False):\n",
    "        if valid == False:\n",
    "            valid_dataset = []\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_data)).batch(batch_size)\n",
    "        else:\n",
    "            train_dataset, valid_dataset = train_test_split(train_data, test_size=0.2, stratify=train_data[:, 7], random_state=0)\n",
    "            n_train = len(train_dataset)\n",
    "            n_val = len(valid_dataset)\n",
    "            # train_dataset's shape : [step, batch_x_train, batch_y_train]\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((train_dataset, train_dataset)).batch(batch_size)\n",
    "            valid_dataset = tf.data.Dataset.from_tensor_slices((valid_dataset, valid_dataset)).batch(batch_size)\n",
    "            \n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "            # progress bar\n",
    "            progBar = Progbar(len(train_dataset) * self.batch, stateful_metrics=['train_loss', 'val_loss'])\n",
    "            total_loss = 0\n",
    "            step = 0\n",
    "\n",
    "            # start train\n",
    "            for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    logits = self.model(x_batch_train, training=True)\n",
    "                    loss = self.loss_function(y_batch_train, logits)\n",
    "                grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "\n",
    "                total_loss += loss\n",
    "                if valid == False:\n",
    "                    values = [('train_loss', loss)]\n",
    "                    progBar.update((step + 1) * self.batch, values=values)\n",
    "            train_loss = total_loss / n_train\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            if n_val:\n",
    "                # if validation set exists, do validation\n",
    "                valid_loss = self.evaluate(valid_dataset)\n",
    "                valid_loss /= n_val\n",
    "                self.history['val_loss'].append(valid_loss)\n",
    "                \n",
    "                # Check if validation loss has improved, if not increase counter\n",
    "                if valid_loss >= self.min_loss:\n",
    "                    self.counter += 1\n",
    "                else:\n",
    "                    self.min_loss = valid_loss\n",
    "                    self.counter = 0\n",
    "                \n",
    "                # Stop training if counter has reached patience\n",
    "                if self.counter >= self.patience:\n",
    "                    print(\"Validation loss did not improve for %d epochs. Training stopped.\" % (self.patience,))\n",
    "                    print(f\"Best Validation Loss : {self.min_loss}\")\n",
    "                    break\n",
    "\n",
    "                values = [('train_loss', train_loss), ('val_loss', valid_loss)]\n",
    "                progBar.update(len(train_dataset) * self.batch, values=values, finalize=True)\n",
    "\n",
    "        plot_graph(self.history)\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "        total_loss = 0\n",
    "        for x, y in dataset:\n",
    "            logits = self.model(x, training=False)\n",
    "            loss = self.loss_function(y, logits)\n",
    "            total_loss += loss\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "def plot_graph(history, start=0):\n",
    "    train_data = history[\"train_loss\"][start:]\n",
    "    val_data = history[\"val_loss\"][start:]\n",
    "    n_train = len(train_data)\n",
    "\n",
    "    gap = n_train // 5\n",
    "\n",
    "    plt.plot(train_data, linewidth=2, label=\"train\")\n",
    "    plt.plot(val_data, linewidth=2, label=\"val\")\n",
    "    \n",
    "    plt.xticks(range(0, n_train+1, gap), range(start, start+n_train+1, gap))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             multiple                  72        \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  72        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 144\n",
      "Trainable params: 144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 0.0077 - val_loss: 0.0070\n",
      "\n",
      "Start of epoch 1\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 0.0071 - val_loss: 0.0065\n",
      "\n",
      "Start of epoch 2\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 0.0067 - val_loss: 0.0061\n",
      "\n",
      "Start of epoch 3\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 0.0063 - val_loss: 0.0057\n",
      "\n",
      "Start of epoch 4\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 0.0059 - val_loss: 0.0054\n",
      "\n",
      "Start of epoch 5\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 0.0055 - val_loss: 0.0051\n",
      "\n",
      "Start of epoch 6\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 0.0052 - val_loss: 0.0048\n",
      "\n",
      "Start of epoch 7\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 0.0049 - val_loss: 0.0045\n",
      "\n",
      "Start of epoch 8\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0046 - val_loss: 0.0043\n",
      "\n",
      "Start of epoch 9\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0043 - val_loss: 0.0040\n",
      "\n",
      "Start of epoch 10\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0041 - val_loss: 0.0038\n",
      "\n",
      "Start of epoch 11\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 0.0038 - val_loss: 0.0036\n",
      "\n",
      "Start of epoch 12\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0036 - val_loss: 0.0034\n",
      "\n",
      "Start of epoch 13\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 0.0034 - val_loss: 0.0032\n",
      "\n",
      "Start of epoch 14\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 0.0032 - val_loss: 0.0030\n",
      "\n",
      "Start of epoch 15\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 0.0030 - val_loss: 0.0028\n",
      "\n",
      "Start of epoch 16\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 0.0028 - val_loss: 0.0026\n",
      "\n",
      "Start of epoch 17\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0026 - val_loss: 0.0025\n",
      "\n",
      "Start of epoch 18\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 0.0025 - val_loss: 0.0024\n",
      "\n",
      "Start of epoch 19\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 0.0023 - val_loss: 0.0022\n",
      "\n",
      "Start of epoch 20\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 0.0022 - val_loss: 0.0021\n",
      "\n",
      "Start of epoch 21\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0020 - val_loss: 0.0020\n",
      "\n",
      "Start of epoch 22\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0019 - val_loss: 0.0019\n",
      "\n",
      "Start of epoch 23\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0018 - val_loss: 0.0018\n",
      "\n",
      "Start of epoch 24\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 0.0017 - val_loss: 0.0017\n",
      "\n",
      "Start of epoch 25\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 0.0016 - val_loss: 0.0016\n",
      "\n",
      "Start of epoch 26\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 0.0016 - val_loss: 0.0015\n",
      "\n",
      "Start of epoch 27\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0015 - val_loss: 0.0015\n",
      "\n",
      "Start of epoch 28\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0014 - val_loss: 0.0014\n",
      "\n",
      "Start of epoch 29\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 0.0014 - val_loss: 0.0014\n",
      "\n",
      "Start of epoch 30\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0013 - val_loss: 0.0013\n",
      "\n",
      "Start of epoch 31\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 0.0012 - val_loss: 0.0012\n",
      "\n",
      "Start of epoch 32\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 0.0012 - val_loss: 0.0012\n",
      "\n",
      "Start of epoch 33\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0011 - val_loss: 0.0012\n",
      "\n",
      "Start of epoch 34\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 0.0011 - val_loss: 0.0011\n",
      "\n",
      "Start of epoch 35\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 0.0010 - val_loss: 0.0011\n",
      "\n",
      "Start of epoch 36\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 0.0010 - val_loss: 0.0010\n",
      "\n",
      "Start of epoch 37\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 9.6542e-04 - val_loss: 9.8226e-04\n",
      "\n",
      "Start of epoch 38\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 9.2636e-04 - val_loss: 9.4402e-04\n",
      "\n",
      "Start of epoch 39\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 8.8899e-04 - val_loss: 9.0734e-04\n",
      "\n",
      "Start of epoch 40\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.5324e-04 - val_loss: 8.7215e-04\n",
      "\n",
      "Start of epoch 41\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.1895e-04 - val_loss: 8.3823e-04\n",
      "\n",
      "Start of epoch 42\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.8605e-04 - val_loss: 8.0548e-04\n",
      "\n",
      "Start of epoch 43\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 7.5466e-04 - val_loss: 7.7423e-04\n",
      "\n",
      "Start of epoch 44\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 7.2489e-04 - val_loss: 7.4441e-04\n",
      "\n",
      "Start of epoch 45\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 6.9666e-04 - val_loss: 7.1616e-04\n",
      "\n",
      "Start of epoch 46\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 6.6988e-04 - val_loss: 6.8917e-04\n",
      "\n",
      "Start of epoch 47\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 6.4435e-04 - val_loss: 6.6338e-04\n",
      "\n",
      "Start of epoch 48\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 6.1996e-04 - val_loss: 6.3868e-04\n",
      "\n",
      "Start of epoch 49\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.9658e-04 - val_loss: 6.1495e-04\n",
      "\n",
      "Start of epoch 50\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.7416e-04 - val_loss: 5.9218e-04\n",
      "\n",
      "Start of epoch 51\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.5271e-04 - val_loss: 5.7038e-04\n",
      "\n",
      "Start of epoch 52\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3216e-04 - val_loss: 5.4946e-04\n",
      "\n",
      "Start of epoch 53\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.1247e-04 - val_loss: 5.2929e-04\n",
      "\n",
      "Start of epoch 54\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.9360e-04 - val_loss: 5.0993e-04\n",
      "\n",
      "Start of epoch 55\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.7548e-04 - val_loss: 4.9131e-04\n",
      "\n",
      "Start of epoch 56\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.5808e-04 - val_loss: 4.7338e-04\n",
      "\n",
      "Start of epoch 57\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.4135e-04 - val_loss: 4.5618e-04\n",
      "\n",
      "Start of epoch 58\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2522e-04 - val_loss: 4.3963e-04\n",
      "\n",
      "Start of epoch 59\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0964e-04 - val_loss: 4.2372e-04\n",
      "\n",
      "Start of epoch 60\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.9469e-04 - val_loss: 4.0852e-04\n",
      "\n",
      "Start of epoch 61\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.8041e-04 - val_loss: 3.9403e-04\n",
      "\n",
      "Start of epoch 62\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.6679e-04 - val_loss: 3.8020e-04\n",
      "\n",
      "Start of epoch 63\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5374e-04 - val_loss: 3.6697e-04\n",
      "\n",
      "Start of epoch 64\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.4122e-04 - val_loss: 3.5428e-04\n",
      "\n",
      "Start of epoch 65\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2924e-04 - val_loss: 3.4215e-04\n",
      "\n",
      "Start of epoch 66\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.1773e-04 - val_loss: 3.3050e-04\n",
      "\n",
      "Start of epoch 67\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 3.0665e-04 - val_loss: 3.1926e-04\n",
      "\n",
      "Start of epoch 68\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.9600e-04 - val_loss: 3.0842e-04\n",
      "\n",
      "Start of epoch 69\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.8576e-04 - val_loss: 2.9804e-04\n",
      "\n",
      "Start of epoch 70\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7592e-04 - val_loss: 2.8813e-04\n",
      "\n",
      "Start of epoch 71\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.6652e-04 - val_loss: 2.7869e-04\n",
      "\n",
      "Start of epoch 72\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5752e-04 - val_loss: 2.6969e-04\n",
      "\n",
      "Start of epoch 73\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4891e-04 - val_loss: 2.6107e-04\n",
      "\n",
      "Start of epoch 74\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4066e-04 - val_loss: 2.5283e-04\n",
      "\n",
      "Start of epoch 75\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3276e-04 - val_loss: 2.4497e-04\n",
      "\n",
      "Start of epoch 76\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.2519e-04 - val_loss: 2.3738e-04\n",
      "\n",
      "Start of epoch 77\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.1792e-04 - val_loss: 2.3009e-04\n",
      "\n",
      "Start of epoch 78\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 2.1094e-04 - val_loss: 2.2311e-04\n",
      "\n",
      "Start of epoch 79\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.0426e-04 - val_loss: 2.1638e-04\n",
      "\n",
      "Start of epoch 80\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.9785e-04 - val_loss: 2.0994e-04\n",
      "\n",
      "Start of epoch 81\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.9170e-04 - val_loss: 2.0378e-04\n",
      "\n",
      "Start of epoch 82\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.8581e-04 - val_loss: 1.9787e-04\n",
      "\n",
      "Start of epoch 83\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.8018e-04 - val_loss: 1.9221e-04\n",
      "\n",
      "Start of epoch 84\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7477e-04 - val_loss: 1.8677e-04\n",
      "\n",
      "Start of epoch 85\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.6959e-04 - val_loss: 1.8157e-04\n",
      "\n",
      "Start of epoch 86\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.6464e-04 - val_loss: 1.7660e-04\n",
      "\n",
      "Start of epoch 87\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.5992e-04 - val_loss: 1.7183e-04\n",
      "\n",
      "Start of epoch 88\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.5539e-04 - val_loss: 1.6726e-04\n",
      "\n",
      "Start of epoch 89\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.5105e-04 - val_loss: 1.6288e-04\n",
      "\n",
      "Start of epoch 90\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.4689e-04 - val_loss: 1.5868e-04\n",
      "\n",
      "Start of epoch 91\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4290e-04 - val_loss: 1.5464e-04\n",
      "\n",
      "Start of epoch 92\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.3907e-04 - val_loss: 1.5075e-04\n",
      "\n",
      "Start of epoch 93\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3540e-04 - val_loss: 1.4701e-04\n",
      "\n",
      "Start of epoch 94\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3187e-04 - val_loss: 1.4341e-04\n",
      "\n",
      "Start of epoch 95\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2849e-04 - val_loss: 1.3994e-04\n",
      "\n",
      "Start of epoch 96\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2524e-04 - val_loss: 1.3658e-04\n",
      "\n",
      "Start of epoch 97\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2211e-04 - val_loss: 1.3334e-04\n",
      "\n",
      "Start of epoch 98\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1910e-04 - val_loss: 1.3021e-04\n",
      "\n",
      "Start of epoch 99\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1621e-04 - val_loss: 1.2719e-04\n",
      "\n",
      "Start of epoch 100\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1341e-04 - val_loss: 1.2428e-04\n",
      "\n",
      "Start of epoch 101\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1072e-04 - val_loss: 1.2146e-04\n",
      "\n",
      "Start of epoch 102\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0812e-04 - val_loss: 1.1873e-04\n",
      "\n",
      "Start of epoch 103\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.0562e-04 - val_loss: 1.1612e-04\n",
      "\n",
      "Start of epoch 104\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0323e-04 - val_loss: 1.1359e-04\n",
      "\n",
      "Start of epoch 105\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0093e-04 - val_loss: 1.1117e-04\n",
      "\n",
      "Start of epoch 106\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.8731e-05 - val_loss: 1.0884e-04\n",
      "\n",
      "Start of epoch 107\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.6610e-05 - val_loss: 1.0656e-04\n",
      "\n",
      "Start of epoch 108\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 9.4562e-05 - val_loss: 1.0437e-04\n",
      "\n",
      "Start of epoch 109\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 9.2592e-05 - val_loss: 1.0226e-04\n",
      "\n",
      "Start of epoch 110\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.0697e-05 - val_loss: 1.0023e-04\n",
      "\n",
      "Start of epoch 111\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.8871e-05 - val_loss: 9.8248e-05\n",
      "\n",
      "Start of epoch 112\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.7107e-05 - val_loss: 9.6327e-05\n",
      "\n",
      "Start of epoch 113\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.5400e-05 - val_loss: 9.4463e-05\n",
      "\n",
      "Start of epoch 114\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.3747e-05 - val_loss: 9.2658e-05\n",
      "\n",
      "Start of epoch 115\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.2148e-05 - val_loss: 9.0902e-05\n",
      "\n",
      "Start of epoch 116\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.0598e-05 - val_loss: 8.9189e-05\n",
      "\n",
      "Start of epoch 117\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.9099e-05 - val_loss: 8.7526e-05\n",
      "\n",
      "Start of epoch 118\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.7651e-05 - val_loss: 8.5903e-05\n",
      "\n",
      "Start of epoch 119\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.6247e-05 - val_loss: 8.4335e-05\n",
      "\n",
      "Start of epoch 120\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.4893e-05 - val_loss: 8.2805e-05\n",
      "\n",
      "Start of epoch 121\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 7.3578e-05 - val_loss: 8.1307e-05\n",
      "\n",
      "Start of epoch 122\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 7.2302e-05 - val_loss: 7.9846e-05\n",
      "\n",
      "Start of epoch 123\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.1060e-05 - val_loss: 7.8422e-05\n",
      "\n",
      "Start of epoch 124\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.9859e-05 - val_loss: 7.7051e-05\n",
      "\n",
      "Start of epoch 125\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.8694e-05 - val_loss: 7.5710e-05\n",
      "\n",
      "Start of epoch 126\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.7566e-05 - val_loss: 7.4405e-05\n",
      "\n",
      "Start of epoch 127\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.6470e-05 - val_loss: 7.3130e-05\n",
      "\n",
      "Start of epoch 128\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.5403e-05 - val_loss: 7.1893e-05\n",
      "\n",
      "Start of epoch 129\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.4367e-05 - val_loss: 7.0683e-05\n",
      "\n",
      "Start of epoch 130\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.3355e-05 - val_loss: 6.9503e-05\n",
      "\n",
      "Start of epoch 131\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.2365e-05 - val_loss: 6.8351e-05\n",
      "\n",
      "Start of epoch 132\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.1403e-05 - val_loss: 6.7225e-05\n",
      "\n",
      "Start of epoch 133\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.0466e-05 - val_loss: 6.6130e-05\n",
      "\n",
      "Start of epoch 134\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.9555e-05 - val_loss: 6.5060e-05\n",
      "\n",
      "Start of epoch 135\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8667e-05 - val_loss: 6.4007e-05\n",
      "\n",
      "Start of epoch 136\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.7796e-05 - val_loss: 6.2974e-05\n",
      "\n",
      "Start of epoch 137\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.6947e-05 - val_loss: 6.1961e-05\n",
      "\n",
      "Start of epoch 138\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.6119e-05 - val_loss: 6.0975e-05\n",
      "\n",
      "Start of epoch 139\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.5311e-05 - val_loss: 6.0009e-05\n",
      "\n",
      "Start of epoch 140\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.4522e-05 - val_loss: 5.9070e-05\n",
      "\n",
      "Start of epoch 141\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.3755e-05 - val_loss: 5.8156e-05\n",
      "\n",
      "Start of epoch 142\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3009e-05 - val_loss: 5.7261e-05\n",
      "\n",
      "Start of epoch 143\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2279e-05 - val_loss: 5.6385e-05\n",
      "\n",
      "Start of epoch 144\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.1566e-05 - val_loss: 5.5530e-05\n",
      "\n",
      "Start of epoch 145\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.0865e-05 - val_loss: 5.4692e-05\n",
      "\n",
      "Start of epoch 146\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.0172e-05 - val_loss: 5.3877e-05\n",
      "\n",
      "Start of epoch 147\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.9493e-05 - val_loss: 5.3086e-05\n",
      "\n",
      "Start of epoch 148\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.8830e-05 - val_loss: 5.2309e-05\n",
      "\n",
      "Start of epoch 149\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.8182e-05 - val_loss: 5.1553e-05\n",
      "\n",
      "Start of epoch 150\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.7544e-05 - val_loss: 5.0813e-05\n",
      "\n",
      "Start of epoch 151\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.6918e-05 - val_loss: 5.0085e-05\n",
      "\n",
      "Start of epoch 152\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.6302e-05 - val_loss: 4.9379e-05\n",
      "\n",
      "Start of epoch 153\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 4.5702e-05 - val_loss: 4.8690e-05\n",
      "\n",
      "Start of epoch 154\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.5118e-05 - val_loss: 4.8014e-05\n",
      "\n",
      "Start of epoch 155\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.4541e-05 - val_loss: 4.7342e-05\n",
      "\n",
      "Start of epoch 156\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.3977e-05 - val_loss: 4.6684e-05\n",
      "\n",
      "Start of epoch 157\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.3425e-05 - val_loss: 4.6041e-05\n",
      "\n",
      "Start of epoch 158\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2882e-05 - val_loss: 4.5415e-05\n",
      "\n",
      "Start of epoch 159\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.2350e-05 - val_loss: 4.4793e-05\n",
      "\n",
      "Start of epoch 160\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.1831e-05 - val_loss: 4.4187e-05\n",
      "\n",
      "Start of epoch 161\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.1325e-05 - val_loss: 4.3597e-05\n",
      "\n",
      "Start of epoch 162\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0831e-05 - val_loss: 4.3022e-05\n",
      "\n",
      "Start of epoch 163\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.0351e-05 - val_loss: 4.2458e-05\n",
      "\n",
      "Start of epoch 164\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.9884e-05 - val_loss: 4.1912e-05\n",
      "\n",
      "Start of epoch 165\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.9424e-05 - val_loss: 4.1383e-05\n",
      "\n",
      "Start of epoch 166\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.8973e-05 - val_loss: 4.0860e-05\n",
      "\n",
      "Start of epoch 167\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.8535e-05 - val_loss: 4.0340e-05\n",
      "\n",
      "Start of epoch 168\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.8110e-05 - val_loss: 3.9824e-05\n",
      "\n",
      "Start of epoch 169\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.7698e-05 - val_loss: 3.9309e-05\n",
      "\n",
      "Start of epoch 170\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.7294e-05 - val_loss: 3.8804e-05\n",
      "\n",
      "Start of epoch 171\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 3.6898e-05 - val_loss: 3.8316e-05\n",
      "\n",
      "Start of epoch 172\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.6512e-05 - val_loss: 3.7838e-05\n",
      "\n",
      "Start of epoch 173\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.6135e-05 - val_loss: 3.7369e-05\n",
      "\n",
      "Start of epoch 174\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.5766e-05 - val_loss: 3.6910e-05\n",
      "\n",
      "Start of epoch 175\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.5407e-05 - val_loss: 3.6464e-05\n",
      "\n",
      "Start of epoch 176\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5053e-05 - val_loss: 3.6020e-05\n",
      "\n",
      "Start of epoch 177\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.4708e-05 - val_loss: 3.5592e-05\n",
      "\n",
      "Start of epoch 178\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.4373e-05 - val_loss: 3.5179e-05\n",
      "\n",
      "Start of epoch 179\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.4049e-05 - val_loss: 3.4781e-05\n",
      "\n",
      "Start of epoch 180\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.3733e-05 - val_loss: 3.4389e-05\n",
      "\n",
      "Start of epoch 181\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.3413e-05 - val_loss: 3.4010e-05\n",
      "\n",
      "Start of epoch 182\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.3102e-05 - val_loss: 3.3649e-05\n",
      "\n",
      "Start of epoch 183\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.2805e-05 - val_loss: 3.3287e-05\n",
      "\n",
      "Start of epoch 184\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2516e-05 - val_loss: 3.2924e-05\n",
      "\n",
      "Start of epoch 185\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2234e-05 - val_loss: 3.2571e-05\n",
      "\n",
      "Start of epoch 186\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.1959e-05 - val_loss: 3.2230e-05\n",
      "\n",
      "Start of epoch 187\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.1693e-05 - val_loss: 3.1887e-05\n",
      "\n",
      "Start of epoch 188\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.1433e-05 - val_loss: 3.1549e-05\n",
      "\n",
      "Start of epoch 189\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.1180e-05 - val_loss: 3.1220e-05\n",
      "\n",
      "Start of epoch 190\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.0935e-05 - val_loss: 3.0900e-05\n",
      "\n",
      "Start of epoch 191\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0697e-05 - val_loss: 3.0594e-05\n",
      "\n",
      "Start of epoch 192\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.0465e-05 - val_loss: 3.0290e-05\n",
      "\n",
      "Start of epoch 193\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.0232e-05 - val_loss: 2.9992e-05\n",
      "\n",
      "Start of epoch 194\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0003e-05 - val_loss: 2.9699e-05\n",
      "\n",
      "Start of epoch 195\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.9769e-05 - val_loss: 2.9411e-05\n",
      "\n",
      "Start of epoch 196\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.9535e-05 - val_loss: 2.9133e-05\n",
      "\n",
      "Start of epoch 197\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.9304e-05 - val_loss: 2.8861e-05\n",
      "\n",
      "Start of epoch 198\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.9079e-05 - val_loss: 2.8597e-05\n",
      "\n",
      "Start of epoch 199\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8863e-05 - val_loss: 2.8343e-05\n",
      "\n",
      "Start of epoch 200\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.8654e-05 - val_loss: 2.8081e-05\n",
      "\n",
      "Start of epoch 201\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.8452e-05 - val_loss: 2.7831e-05\n",
      "\n",
      "Start of epoch 202\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8256e-05 - val_loss: 2.7585e-05\n",
      "\n",
      "Start of epoch 203\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8064e-05 - val_loss: 2.7342e-05\n",
      "\n",
      "Start of epoch 204\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7878e-05 - val_loss: 2.7101e-05\n",
      "\n",
      "Start of epoch 205\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.7696e-05 - val_loss: 2.6859e-05\n",
      "\n",
      "Start of epoch 206\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7516e-05 - val_loss: 2.6623e-05\n",
      "\n",
      "Start of epoch 207\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7332e-05 - val_loss: 2.6377e-05\n",
      "\n",
      "Start of epoch 208\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.7143e-05 - val_loss: 2.6136e-05\n",
      "\n",
      "Start of epoch 209\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.6954e-05 - val_loss: 2.5904e-05\n",
      "\n",
      "Start of epoch 210\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.6767e-05 - val_loss: 2.5676e-05\n",
      "\n",
      "Start of epoch 211\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.6583e-05 - val_loss: 2.5450e-05\n",
      "\n",
      "Start of epoch 212\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.6402e-05 - val_loss: 2.5235e-05\n",
      "\n",
      "Start of epoch 213\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.6227e-05 - val_loss: 2.5027e-05\n",
      "\n",
      "Start of epoch 214\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.6057e-05 - val_loss: 2.4822e-05\n",
      "\n",
      "Start of epoch 215\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.5888e-05 - val_loss: 2.4624e-05\n",
      "\n",
      "Start of epoch 216\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.5719e-05 - val_loss: 2.4420e-05\n",
      "\n",
      "Start of epoch 217\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.5550e-05 - val_loss: 2.4221e-05\n",
      "\n",
      "Start of epoch 218\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.5388e-05 - val_loss: 2.4028e-05\n",
      "\n",
      "Start of epoch 219\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5224e-05 - val_loss: 2.3832e-05\n",
      "\n",
      "Start of epoch 220\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5058e-05 - val_loss: 2.3636e-05\n",
      "\n",
      "Start of epoch 221\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4890e-05 - val_loss: 2.3439e-05\n",
      "\n",
      "Start of epoch 222\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.4716e-05 - val_loss: 2.3240e-05\n",
      "\n",
      "Start of epoch 223\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.4543e-05 - val_loss: 2.3049e-05\n",
      "\n",
      "Start of epoch 224\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4366e-05 - val_loss: 2.2849e-05\n",
      "\n",
      "Start of epoch 225\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4188e-05 - val_loss: 2.2647e-05\n",
      "\n",
      "Start of epoch 226\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.4012e-05 - val_loss: 2.2424e-05\n",
      "\n",
      "Start of epoch 227\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3831e-05 - val_loss: 2.2204e-05\n",
      "\n",
      "Start of epoch 228\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.3649e-05 - val_loss: 2.1987e-05\n",
      "\n",
      "Start of epoch 229\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.3462e-05 - val_loss: 2.1774e-05\n",
      "\n",
      "Start of epoch 230\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3275e-05 - val_loss: 2.1561e-05\n",
      "\n",
      "Start of epoch 231\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.3088e-05 - val_loss: 2.1344e-05\n",
      "\n",
      "Start of epoch 232\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2903e-05 - val_loss: 2.1135e-05\n",
      "\n",
      "Start of epoch 233\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2716e-05 - val_loss: 2.0923e-05\n",
      "\n",
      "Start of epoch 234\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2520e-05 - val_loss: 2.0708e-05\n",
      "\n",
      "Start of epoch 235\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2315e-05 - val_loss: 2.0485e-05\n",
      "\n",
      "Start of epoch 236\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2100e-05 - val_loss: 2.0268e-05\n",
      "\n",
      "Start of epoch 237\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1881e-05 - val_loss: 2.0030e-05\n",
      "\n",
      "Start of epoch 238\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1660e-05 - val_loss: 1.9779e-05\n",
      "\n",
      "Start of epoch 239\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1430e-05 - val_loss: 1.9522e-05\n",
      "\n",
      "Start of epoch 240\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1190e-05 - val_loss: 1.9252e-05\n",
      "\n",
      "Start of epoch 241\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0934e-05 - val_loss: 1.8977e-05\n",
      "\n",
      "Start of epoch 242\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0670e-05 - val_loss: 1.8703e-05\n",
      "\n",
      "Start of epoch 243\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.0395e-05 - val_loss: 1.8422e-05\n",
      "\n",
      "Start of epoch 244\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0122e-05 - val_loss: 1.8152e-05\n",
      "\n",
      "Start of epoch 245\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.9847e-05 - val_loss: 1.7883e-05\n",
      "\n",
      "Start of epoch 246\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.9557e-05 - val_loss: 1.7602e-05\n",
      "\n",
      "Start of epoch 247\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.9244e-05 - val_loss: 1.7306e-05\n",
      "\n",
      "Start of epoch 248\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.8917e-05 - val_loss: 1.7007e-05\n",
      "\n",
      "Start of epoch 249\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8583e-05 - val_loss: 1.6713e-05\n",
      "\n",
      "Start of epoch 250\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8251e-05 - val_loss: 1.6429e-05\n",
      "\n",
      "Start of epoch 251\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7926e-05 - val_loss: 1.6149e-05\n",
      "\n",
      "Start of epoch 252\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7608e-05 - val_loss: 1.5860e-05\n",
      "\n",
      "Start of epoch 253\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7282e-05 - val_loss: 1.5562e-05\n",
      "\n",
      "Start of epoch 254\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6949e-05 - val_loss: 1.5273e-05\n",
      "\n",
      "Start of epoch 255\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.6594e-05 - val_loss: 1.4980e-05\n",
      "\n",
      "Start of epoch 256\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6221e-05 - val_loss: 1.4675e-05\n",
      "\n",
      "Start of epoch 257\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5839e-05 - val_loss: 1.4365e-05\n",
      "\n",
      "Start of epoch 258\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.5444e-05 - val_loss: 1.4050e-05\n",
      "\n",
      "Start of epoch 259\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5058e-05 - val_loss: 1.3751e-05\n",
      "\n",
      "Start of epoch 260\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.4669e-05 - val_loss: 1.3454e-05\n",
      "\n",
      "Start of epoch 261\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4286e-05 - val_loss: 1.3160e-05\n",
      "\n",
      "Start of epoch 262\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3912e-05 - val_loss: 1.2852e-05\n",
      "\n",
      "Start of epoch 263\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3535e-05 - val_loss: 1.2549e-05\n",
      "\n",
      "Start of epoch 264\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3159e-05 - val_loss: 1.2257e-05\n",
      "\n",
      "Start of epoch 265\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2792e-05 - val_loss: 1.1977e-05\n",
      "\n",
      "Start of epoch 266\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2434e-05 - val_loss: 1.1699e-05\n",
      "\n",
      "Start of epoch 267\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2093e-05 - val_loss: 1.1435e-05\n",
      "\n",
      "Start of epoch 268\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1778e-05 - val_loss: 1.1184e-05\n",
      "\n",
      "Start of epoch 269\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1485e-05 - val_loss: 1.0941e-05\n",
      "\n",
      "Start of epoch 270\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1212e-05 - val_loss: 1.0709e-05\n",
      "\n",
      "Start of epoch 271\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0956e-05 - val_loss: 1.0492e-05\n",
      "\n",
      "Start of epoch 272\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0718e-05 - val_loss: 1.0287e-05\n",
      "\n",
      "Start of epoch 273\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0497e-05 - val_loss: 1.0087e-05\n",
      "\n",
      "Start of epoch 274\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0290e-05 - val_loss: 9.9020e-06\n",
      "\n",
      "Start of epoch 275\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.0093e-05 - val_loss: 9.7230e-06\n",
      "\n",
      "Start of epoch 276\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.9071e-06 - val_loss: 9.5576e-06\n",
      "\n",
      "Start of epoch 277\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.7341e-06 - val_loss: 9.3964e-06\n",
      "\n",
      "Start of epoch 278\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.5734e-06 - val_loss: 9.2472e-06\n",
      "\n",
      "Start of epoch 279\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.4175e-06 - val_loss: 9.1041e-06\n",
      "\n",
      "Start of epoch 280\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.2646e-06 - val_loss: 8.9643e-06\n",
      "\n",
      "Start of epoch 281\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.1173e-06 - val_loss: 8.8173e-06\n",
      "\n",
      "Start of epoch 282\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.9805e-06 - val_loss: 8.6748e-06\n",
      "\n",
      "Start of epoch 283\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.8534e-06 - val_loss: 8.5393e-06\n",
      "\n",
      "Start of epoch 284\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.7333e-06 - val_loss: 8.4110e-06\n",
      "\n",
      "Start of epoch 285\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.6205e-06 - val_loss: 8.2925e-06\n",
      "\n",
      "Start of epoch 286\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.5132e-06 - val_loss: 8.1808e-06\n",
      "\n",
      "Start of epoch 287\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.4113e-06 - val_loss: 8.0753e-06\n",
      "\n",
      "Start of epoch 288\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.3144e-06 - val_loss: 7.9749e-06\n",
      "\n",
      "Start of epoch 289\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.2223e-06 - val_loss: 7.8791e-06\n",
      "\n",
      "Start of epoch 290\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.1356e-06 - val_loss: 7.7907e-06\n",
      "\n",
      "Start of epoch 291\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.0530e-06 - val_loss: 7.7065e-06\n",
      "\n",
      "Start of epoch 292\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.9748e-06 - val_loss: 7.6253e-06\n",
      "\n",
      "Start of epoch 293\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.8998e-06 - val_loss: 7.5492e-06\n",
      "\n",
      "Start of epoch 294\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.8266e-06 - val_loss: 7.4765e-06\n",
      "\n",
      "Start of epoch 295\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 7.7560e-06 - val_loss: 7.4083e-06\n",
      "\n",
      "Start of epoch 296\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 7.6877e-06 - val_loss: 7.3425e-06\n",
      "\n",
      "Start of epoch 297\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.6233e-06 - val_loss: 7.2802e-06\n",
      "\n",
      "Start of epoch 298\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.5624e-06 - val_loss: 7.2214e-06\n",
      "\n",
      "Start of epoch 299\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.5044e-06 - val_loss: 7.1667e-06\n",
      "\n",
      "Start of epoch 300\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.4479e-06 - val_loss: 7.1130e-06\n",
      "\n",
      "Start of epoch 301\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.3932e-06 - val_loss: 7.0612e-06\n",
      "\n",
      "Start of epoch 302\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.3396e-06 - val_loss: 7.0124e-06\n",
      "\n",
      "Start of epoch 303\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.2868e-06 - val_loss: 6.9660e-06\n",
      "\n",
      "Start of epoch 304\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.2355e-06 - val_loss: 6.9208e-06\n",
      "\n",
      "Start of epoch 305\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.1852e-06 - val_loss: 6.8768e-06\n",
      "\n",
      "Start of epoch 306\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.1365e-06 - val_loss: 6.8342e-06\n",
      "\n",
      "Start of epoch 307\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.0894e-06 - val_loss: 6.7917e-06\n",
      "\n",
      "Start of epoch 308\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.0435e-06 - val_loss: 6.7492e-06\n",
      "\n",
      "Start of epoch 309\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.9987e-06 - val_loss: 6.7082e-06\n",
      "\n",
      "Start of epoch 310\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.9546e-06 - val_loss: 6.6678e-06\n",
      "\n",
      "Start of epoch 311\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.9112e-06 - val_loss: 6.6266e-06\n",
      "\n",
      "Start of epoch 312\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.8690e-06 - val_loss: 6.5864e-06\n",
      "\n",
      "Start of epoch 313\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.8278e-06 - val_loss: 6.5478e-06\n",
      "\n",
      "Start of epoch 314\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.7874e-06 - val_loss: 6.5104e-06\n",
      "\n",
      "Start of epoch 315\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.7478e-06 - val_loss: 6.4729e-06\n",
      "\n",
      "Start of epoch 316\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.7088e-06 - val_loss: 6.4356e-06\n",
      "\n",
      "Start of epoch 317\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.6710e-06 - val_loss: 6.3985e-06\n",
      "\n",
      "Start of epoch 318\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.6334e-06 - val_loss: 6.3623e-06\n",
      "\n",
      "Start of epoch 319\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.5967e-06 - val_loss: 6.3278e-06\n",
      "\n",
      "Start of epoch 320\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.5614e-06 - val_loss: 6.2940e-06\n",
      "\n",
      "Start of epoch 321\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.5263e-06 - val_loss: 6.2603e-06\n",
      "\n",
      "Start of epoch 322\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.4919e-06 - val_loss: 6.2267e-06\n",
      "\n",
      "Start of epoch 323\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.4574e-06 - val_loss: 6.1931e-06\n",
      "\n",
      "Start of epoch 324\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.4234e-06 - val_loss: 6.1587e-06\n",
      "\n",
      "Start of epoch 325\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.3904e-06 - val_loss: 6.1253e-06\n",
      "\n",
      "Start of epoch 326\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.3584e-06 - val_loss: 6.0925e-06\n",
      "\n",
      "Start of epoch 327\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.3271e-06 - val_loss: 6.0606e-06\n",
      "\n",
      "Start of epoch 328\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.2963e-06 - val_loss: 6.0297e-06\n",
      "\n",
      "Start of epoch 329\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.2663e-06 - val_loss: 5.9997e-06\n",
      "\n",
      "Start of epoch 330\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.2368e-06 - val_loss: 5.9699e-06\n",
      "\n",
      "Start of epoch 331\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.2077e-06 - val_loss: 5.9410e-06\n",
      "\n",
      "Start of epoch 332\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.1775e-06 - val_loss: 5.9114e-06\n",
      "\n",
      "Start of epoch 333\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.1469e-06 - val_loss: 5.8820e-06\n",
      "\n",
      "Start of epoch 334\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.1160e-06 - val_loss: 5.8526e-06\n",
      "\n",
      "Start of epoch 335\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.0856e-06 - val_loss: 5.8236e-06\n",
      "\n",
      "Start of epoch 336\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.0539e-06 - val_loss: 5.7942e-06\n",
      "\n",
      "Start of epoch 337\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.0220e-06 - val_loss: 5.7668e-06\n",
      "\n",
      "Start of epoch 338\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9894e-06 - val_loss: 5.7396e-06\n",
      "\n",
      "Start of epoch 339\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.9567e-06 - val_loss: 5.7116e-06\n",
      "\n",
      "Start of epoch 340\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.9233e-06 - val_loss: 5.6795e-06\n",
      "\n",
      "Start of epoch 341\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.8901e-06 - val_loss: 5.6476e-06\n",
      "\n",
      "Start of epoch 342\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8572e-06 - val_loss: 5.6153e-06\n",
      "\n",
      "Start of epoch 343\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.8252e-06 - val_loss: 5.5832e-06\n",
      "\n",
      "Start of epoch 344\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.7942e-06 - val_loss: 5.5511e-06\n",
      "\n",
      "Start of epoch 345\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.7637e-06 - val_loss: 5.5199e-06\n",
      "\n",
      "Start of epoch 346\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7340e-06 - val_loss: 5.4892e-06\n",
      "\n",
      "Start of epoch 347\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7050e-06 - val_loss: 5.4581e-06\n",
      "\n",
      "Start of epoch 348\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.6766e-06 - val_loss: 5.4277e-06\n",
      "\n",
      "Start of epoch 349\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.6486e-06 - val_loss: 5.3982e-06\n",
      "\n",
      "Start of epoch 350\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.6216e-06 - val_loss: 5.3696e-06\n",
      "\n",
      "Start of epoch 351\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5951e-06 - val_loss: 5.3417e-06\n",
      "\n",
      "Start of epoch 352\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5692e-06 - val_loss: 5.3148e-06\n",
      "\n",
      "Start of epoch 353\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.5429e-06 - val_loss: 5.2876e-06\n",
      "\n",
      "Start of epoch 354\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.5164e-06 - val_loss: 5.2600e-06\n",
      "\n",
      "Start of epoch 355\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.4896e-06 - val_loss: 5.2329e-06\n",
      "\n",
      "Start of epoch 356\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.4632e-06 - val_loss: 5.2067e-06\n",
      "\n",
      "Start of epoch 357\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.4370e-06 - val_loss: 5.1804e-06\n",
      "\n",
      "Start of epoch 358\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.4100e-06 - val_loss: 5.1544e-06\n",
      "\n",
      "Start of epoch 359\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.3834e-06 - val_loss: 5.1275e-06\n",
      "\n",
      "Start of epoch 360\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.3564e-06 - val_loss: 5.1006e-06\n",
      "\n",
      "Start of epoch 361\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3293e-06 - val_loss: 5.0739e-06\n",
      "\n",
      "Start of epoch 362\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3013e-06 - val_loss: 5.0473e-06\n",
      "\n",
      "Start of epoch 363\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2743e-06 - val_loss: 5.0217e-06\n",
      "\n",
      "Start of epoch 364\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.2474e-06 - val_loss: 4.9954e-06\n",
      "\n",
      "Start of epoch 365\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.2208e-06 - val_loss: 4.9681e-06\n",
      "\n",
      "Start of epoch 366\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.1949e-06 - val_loss: 4.9422e-06\n",
      "\n",
      "Start of epoch 367\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.1696e-06 - val_loss: 4.9169e-06\n",
      "\n",
      "Start of epoch 368\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.1452e-06 - val_loss: 4.8908e-06\n",
      "\n",
      "Start of epoch 369\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.1214e-06 - val_loss: 4.8647e-06\n",
      "\n",
      "Start of epoch 370\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.0984e-06 - val_loss: 4.8395e-06\n",
      "\n",
      "Start of epoch 371\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.0755e-06 - val_loss: 4.8141e-06\n",
      "\n",
      "Start of epoch 372\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.0527e-06 - val_loss: 4.7899e-06\n",
      "\n",
      "Start of epoch 373\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.0308e-06 - val_loss: 4.7665e-06\n",
      "\n",
      "Start of epoch 374\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.0092e-06 - val_loss: 4.7435e-06\n",
      "\n",
      "Start of epoch 375\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.9866e-06 - val_loss: 4.7208e-06\n",
      "\n",
      "Start of epoch 376\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.9639e-06 - val_loss: 4.6984e-06\n",
      "\n",
      "Start of epoch 377\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.9414e-06 - val_loss: 4.6765e-06\n",
      "\n",
      "Start of epoch 378\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.9178e-06 - val_loss: 4.6543e-06\n",
      "\n",
      "Start of epoch 379\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.8933e-06 - val_loss: 4.6327e-06\n",
      "\n",
      "Start of epoch 380\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.8688e-06 - val_loss: 4.6119e-06\n",
      "\n",
      "Start of epoch 381\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.8444e-06 - val_loss: 4.5893e-06\n",
      "\n",
      "Start of epoch 382\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.8202e-06 - val_loss: 4.5671e-06\n",
      "\n",
      "Start of epoch 383\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.7964e-06 - val_loss: 4.5451e-06\n",
      "\n",
      "Start of epoch 384\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.7736e-06 - val_loss: 4.5235e-06\n",
      "\n",
      "Start of epoch 385\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.7517e-06 - val_loss: 4.5011e-06\n",
      "\n",
      "Start of epoch 386\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.7305e-06 - val_loss: 4.4789e-06\n",
      "\n",
      "Start of epoch 387\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.7097e-06 - val_loss: 4.4570e-06\n",
      "\n",
      "Start of epoch 388\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.6889e-06 - val_loss: 4.4351e-06\n",
      "\n",
      "Start of epoch 389\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.6687e-06 - val_loss: 4.4140e-06\n",
      "\n",
      "Start of epoch 390\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.6494e-06 - val_loss: 4.3934e-06\n",
      "\n",
      "Start of epoch 391\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.6305e-06 - val_loss: 4.3728e-06\n",
      "\n",
      "Start of epoch 392\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.6117e-06 - val_loss: 4.3531e-06\n",
      "\n",
      "Start of epoch 393\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.5934e-06 - val_loss: 4.3327e-06\n",
      "\n",
      "Start of epoch 394\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.5757e-06 - val_loss: 4.3116e-06\n",
      "\n",
      "Start of epoch 395\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.5581e-06 - val_loss: 4.2905e-06\n",
      "\n",
      "Start of epoch 396\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.5400e-06 - val_loss: 4.2695e-06\n",
      "\n",
      "Start of epoch 397\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.5208e-06 - val_loss: 4.2478e-06\n",
      "\n",
      "Start of epoch 398\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.5010e-06 - val_loss: 4.2252e-06\n",
      "\n",
      "Start of epoch 399\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.4812e-06 - val_loss: 4.2024e-06\n",
      "\n",
      "Start of epoch 400\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.4617e-06 - val_loss: 4.1805e-06\n",
      "\n",
      "Start of epoch 401\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.4417e-06 - val_loss: 4.1575e-06\n",
      "\n",
      "Start of epoch 402\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.4210e-06 - val_loss: 4.1345e-06\n",
      "\n",
      "Start of epoch 403\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.4005e-06 - val_loss: 4.1118e-06\n",
      "\n",
      "Start of epoch 404\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.3808e-06 - val_loss: 4.0893e-06\n",
      "\n",
      "Start of epoch 405\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.3605e-06 - val_loss: 4.0674e-06\n",
      "\n",
      "Start of epoch 406\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.3395e-06 - val_loss: 4.0470e-06\n",
      "\n",
      "Start of epoch 407\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.3195e-06 - val_loss: 4.0270e-06\n",
      "\n",
      "Start of epoch 408\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.3001e-06 - val_loss: 4.0071e-06\n",
      "\n",
      "Start of epoch 409\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2807e-06 - val_loss: 3.9876e-06\n",
      "\n",
      "Start of epoch 410\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.2616e-06 - val_loss: 3.9682e-06\n",
      "\n",
      "Start of epoch 411\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.2429e-06 - val_loss: 3.9497e-06\n",
      "\n",
      "Start of epoch 412\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.2233e-06 - val_loss: 3.9312e-06\n",
      "\n",
      "Start of epoch 413\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2013e-06 - val_loss: 3.9127e-06\n",
      "\n",
      "Start of epoch 414\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.1793e-06 - val_loss: 3.8929e-06\n",
      "\n",
      "Start of epoch 415\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.1572e-06 - val_loss: 3.8738e-06\n",
      "\n",
      "Start of epoch 416\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.1348e-06 - val_loss: 3.8532e-06\n",
      "\n",
      "Start of epoch 417\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.1123e-06 - val_loss: 3.8333e-06\n",
      "\n",
      "Start of epoch 418\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0897e-06 - val_loss: 3.8144e-06\n",
      "\n",
      "Start of epoch 419\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0672e-06 - val_loss: 3.7953e-06\n",
      "\n",
      "Start of epoch 420\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.0453e-06 - val_loss: 3.7764e-06\n",
      "\n",
      "Start of epoch 421\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0233e-06 - val_loss: 3.7575e-06\n",
      "\n",
      "Start of epoch 422\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.0004e-06 - val_loss: 3.7366e-06\n",
      "\n",
      "Start of epoch 423\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9783e-06 - val_loss: 3.7154e-06\n",
      "\n",
      "Start of epoch 424\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9576e-06 - val_loss: 3.6939e-06\n",
      "\n",
      "Start of epoch 425\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.9373e-06 - val_loss: 3.6733e-06\n",
      "\n",
      "Start of epoch 426\n",
      "1984/1984 [==============================] - 0s 40us/step - train_loss: 3.9173e-06 - val_loss: 3.6536e-06\n",
      "\n",
      "Start of epoch 427\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 3.8981e-06 - val_loss: 3.6339e-06\n",
      "\n",
      "Start of epoch 428\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.8788e-06 - val_loss: 3.6141e-06\n",
      "\n",
      "Start of epoch 429\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.8587e-06 - val_loss: 3.5962e-06\n",
      "\n",
      "Start of epoch 430\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.8389e-06 - val_loss: 3.5788e-06\n",
      "\n",
      "Start of epoch 431\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.8188e-06 - val_loss: 3.5626e-06\n",
      "\n",
      "Start of epoch 432\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.7984e-06 - val_loss: 3.5436e-06\n",
      "\n",
      "Start of epoch 433\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.7771e-06 - val_loss: 3.5267e-06\n",
      "\n",
      "Start of epoch 434\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.7574e-06 - val_loss: 3.5094e-06\n",
      "\n",
      "Start of epoch 435\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.7383e-06 - val_loss: 3.4928e-06\n",
      "\n",
      "Start of epoch 436\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.7201e-06 - val_loss: 3.4771e-06\n",
      "\n",
      "Start of epoch 437\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.7026e-06 - val_loss: 3.4612e-06\n",
      "\n",
      "Start of epoch 438\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.6844e-06 - val_loss: 3.4447e-06\n",
      "\n",
      "Start of epoch 439\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.6643e-06 - val_loss: 3.4294e-06\n",
      "\n",
      "Start of epoch 440\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.6451e-06 - val_loss: 3.4138e-06\n",
      "\n",
      "Start of epoch 441\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.6259e-06 - val_loss: 3.3988e-06\n",
      "\n",
      "Start of epoch 442\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.6074e-06 - val_loss: 3.3829e-06\n",
      "\n",
      "Start of epoch 443\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.5880e-06 - val_loss: 3.3635e-06\n",
      "\n",
      "Start of epoch 444\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5682e-06 - val_loss: 3.3439e-06\n",
      "\n",
      "Start of epoch 445\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5497e-06 - val_loss: 3.3234e-06\n",
      "\n",
      "Start of epoch 446\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5322e-06 - val_loss: 3.3037e-06\n",
      "\n",
      "Start of epoch 447\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5141e-06 - val_loss: 3.2851e-06\n",
      "\n",
      "Start of epoch 448\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.4955e-06 - val_loss: 3.2665e-06\n",
      "\n",
      "Start of epoch 449\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.4768e-06 - val_loss: 3.2474e-06\n",
      "\n",
      "Start of epoch 450\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.4573e-06 - val_loss: 3.2287e-06\n",
      "\n",
      "Start of epoch 451\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.4380e-06 - val_loss: 3.2104e-06\n",
      "\n",
      "Start of epoch 452\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.4184e-06 - val_loss: 3.1924e-06\n",
      "\n",
      "Start of epoch 453\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.3978e-06 - val_loss: 3.1750e-06\n",
      "\n",
      "Start of epoch 454\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.3778e-06 - val_loss: 3.1582e-06\n",
      "\n",
      "Start of epoch 455\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.3589e-06 - val_loss: 3.1420e-06\n",
      "\n",
      "Start of epoch 456\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.3403e-06 - val_loss: 3.1264e-06\n",
      "\n",
      "Start of epoch 457\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.3227e-06 - val_loss: 3.1112e-06\n",
      "\n",
      "Start of epoch 458\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.3061e-06 - val_loss: 3.0972e-06\n",
      "\n",
      "Start of epoch 459\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2892e-06 - val_loss: 3.0817e-06\n",
      "\n",
      "Start of epoch 460\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2701e-06 - val_loss: 3.0648e-06\n",
      "\n",
      "Start of epoch 461\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.2518e-06 - val_loss: 3.0481e-06\n",
      "\n",
      "Start of epoch 462\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2338e-06 - val_loss: 3.0307e-06\n",
      "\n",
      "Start of epoch 463\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2159e-06 - val_loss: 3.0138e-06\n",
      "\n",
      "Start of epoch 464\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.1981e-06 - val_loss: 2.9980e-06\n",
      "\n",
      "Start of epoch 465\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.1816e-06 - val_loss: 2.9823e-06\n",
      "\n",
      "Start of epoch 466\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.1657e-06 - val_loss: 2.9679e-06\n",
      "\n",
      "Start of epoch 467\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.1502e-06 - val_loss: 2.9526e-06\n",
      "\n",
      "Start of epoch 468\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.1349e-06 - val_loss: 2.9375e-06\n",
      "\n",
      "Start of epoch 469\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.1191e-06 - val_loss: 2.9237e-06\n",
      "\n",
      "Start of epoch 470\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.1042e-06 - val_loss: 2.9105e-06\n",
      "\n",
      "Start of epoch 471\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0897e-06 - val_loss: 2.8984e-06\n",
      "\n",
      "Start of epoch 472\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.0734e-06 - val_loss: 2.8881e-06\n",
      "\n",
      "Start of epoch 473\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.0568e-06 - val_loss: 2.8751e-06\n",
      "\n",
      "Start of epoch 474\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.0395e-06 - val_loss: 2.8644e-06\n",
      "\n",
      "Start of epoch 475\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.0234e-06 - val_loss: 2.8525e-06\n",
      "\n",
      "Start of epoch 476\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0064e-06 - val_loss: 2.8389e-06\n",
      "\n",
      "Start of epoch 477\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.9881e-06 - val_loss: 2.8255e-06\n",
      "\n",
      "Start of epoch 478\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.9692e-06 - val_loss: 2.8113e-06\n",
      "\n",
      "Start of epoch 479\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.9512e-06 - val_loss: 2.7968e-06\n",
      "\n",
      "Start of epoch 480\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.9350e-06 - val_loss: 2.7825e-06\n",
      "\n",
      "Start of epoch 481\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.9200e-06 - val_loss: 2.7683e-06\n",
      "\n",
      "Start of epoch 482\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.9061e-06 - val_loss: 2.7539e-06\n",
      "\n",
      "Start of epoch 483\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.8926e-06 - val_loss: 2.7401e-06\n",
      "\n",
      "Start of epoch 484\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8794e-06 - val_loss: 2.7265e-06\n",
      "\n",
      "Start of epoch 485\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.8660e-06 - val_loss: 2.7118e-06\n",
      "\n",
      "Start of epoch 486\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8530e-06 - val_loss: 2.6969e-06\n",
      "\n",
      "Start of epoch 487\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.8404e-06 - val_loss: 2.6827e-06\n",
      "\n",
      "Start of epoch 488\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8271e-06 - val_loss: 2.6680e-06\n",
      "\n",
      "Start of epoch 489\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8138e-06 - val_loss: 2.6537e-06\n",
      "\n",
      "Start of epoch 490\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8012e-06 - val_loss: 2.6394e-06\n",
      "\n",
      "Start of epoch 491\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.7877e-06 - val_loss: 2.6248e-06\n",
      "\n",
      "Start of epoch 492\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7731e-06 - val_loss: 2.6119e-06\n",
      "\n",
      "Start of epoch 493\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7597e-06 - val_loss: 2.5975e-06\n",
      "\n",
      "Start of epoch 494\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.7470e-06 - val_loss: 2.5846e-06\n",
      "\n",
      "Start of epoch 495\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.7352e-06 - val_loss: 2.5712e-06\n",
      "\n",
      "Start of epoch 496\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7229e-06 - val_loss: 2.5580e-06\n",
      "\n",
      "Start of epoch 497\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7083e-06 - val_loss: 2.5444e-06\n",
      "\n",
      "Start of epoch 498\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.6941e-06 - val_loss: 2.5297e-06\n",
      "\n",
      "Start of epoch 499\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.6802e-06 - val_loss: 2.5182e-06\n",
      "\n",
      "Start of epoch 500\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.6658e-06 - val_loss: 2.5055e-06\n",
      "\n",
      "Start of epoch 501\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.6501e-06 - val_loss: 2.4929e-06\n",
      "\n",
      "Start of epoch 502\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.6357e-06 - val_loss: 2.4797e-06\n",
      "\n",
      "Start of epoch 503\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.6218e-06 - val_loss: 2.4666e-06\n",
      "\n",
      "Start of epoch 504\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.6085e-06 - val_loss: 2.4519e-06\n",
      "\n",
      "Start of epoch 505\n",
      "1984/1984 [==============================] - 0s 42us/step - train_loss: 2.5954e-06 - val_loss: 2.4383e-06\n",
      "\n",
      "Start of epoch 506\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.5825e-06 - val_loss: 2.4227e-06\n",
      "\n",
      "Start of epoch 507\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.5689e-06 - val_loss: 2.4090e-06\n",
      "\n",
      "Start of epoch 508\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.5553e-06 - val_loss: 2.3920e-06\n",
      "\n",
      "Start of epoch 509\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.5416e-06 - val_loss: 2.3753e-06\n",
      "\n",
      "Start of epoch 510\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.5290e-06 - val_loss: 2.3594e-06\n",
      "\n",
      "Start of epoch 511\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.5167e-06 - val_loss: 2.3443e-06\n",
      "\n",
      "Start of epoch 512\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.5036e-06 - val_loss: 2.3291e-06\n",
      "\n",
      "Start of epoch 513\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.4888e-06 - val_loss: 2.3135e-06\n",
      "\n",
      "Start of epoch 514\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.4739e-06 - val_loss: 2.2992e-06\n",
      "\n",
      "Start of epoch 515\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.4599e-06 - val_loss: 2.2856e-06\n",
      "\n",
      "Start of epoch 516\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4466e-06 - val_loss: 2.2719e-06\n",
      "\n",
      "Start of epoch 517\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.4332e-06 - val_loss: 2.2562e-06\n",
      "\n",
      "Start of epoch 518\n",
      "1984/1984 [==============================] - 0s 44us/step - train_loss: 2.4190e-06 - val_loss: 2.2438e-06\n",
      "\n",
      "Start of epoch 519\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 2.4041e-06 - val_loss: 2.2306e-06\n",
      "\n",
      "Start of epoch 520\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.3893e-06 - val_loss: 2.2180e-06\n",
      "\n",
      "Start of epoch 521\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 2.3753e-06 - val_loss: 2.2058e-06\n",
      "\n",
      "Start of epoch 522\n",
      "1984/1984 [==============================] - 0s 42us/step - train_loss: 2.3609e-06 - val_loss: 2.1933e-06\n",
      "\n",
      "Start of epoch 523\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 2.3470e-06 - val_loss: 2.1824e-06\n",
      "\n",
      "Start of epoch 524\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.3326e-06 - val_loss: 2.1707e-06\n",
      "\n",
      "Start of epoch 525\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 2.3183e-06 - val_loss: 2.1593e-06\n",
      "\n",
      "Start of epoch 526\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 2.3047e-06 - val_loss: 2.1485e-06\n",
      "\n",
      "Start of epoch 527\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 2.2923e-06 - val_loss: 2.1374e-06\n",
      "\n",
      "Start of epoch 528\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.2796e-06 - val_loss: 2.1273e-06\n",
      "\n",
      "Start of epoch 529\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.2675e-06 - val_loss: 2.1176e-06\n",
      "\n",
      "Start of epoch 530\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.2560e-06 - val_loss: 2.1073e-06\n",
      "\n",
      "Start of epoch 531\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.2444e-06 - val_loss: 2.0977e-06\n",
      "\n",
      "Start of epoch 532\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.2331e-06 - val_loss: 2.0888e-06\n",
      "\n",
      "Start of epoch 533\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.2225e-06 - val_loss: 2.0795e-06\n",
      "\n",
      "Start of epoch 534\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.2123e-06 - val_loss: 2.0708e-06\n",
      "\n",
      "Start of epoch 535\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.2023e-06 - val_loss: 2.0625e-06\n",
      "\n",
      "Start of epoch 536\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.1925e-06 - val_loss: 2.0526e-06\n",
      "\n",
      "Start of epoch 537\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 2.1821e-06 - val_loss: 2.0428e-06\n",
      "\n",
      "Start of epoch 538\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.1720e-06 - val_loss: 2.0336e-06\n",
      "\n",
      "Start of epoch 539\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.1619e-06 - val_loss: 2.0251e-06\n",
      "\n",
      "Start of epoch 540\n",
      "1984/1984 [==============================] - 0s 42us/step - train_loss: 2.1516e-06 - val_loss: 2.0172e-06\n",
      "\n",
      "Start of epoch 541\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.1417e-06 - val_loss: 2.0096e-06\n",
      "\n",
      "Start of epoch 542\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.1324e-06 - val_loss: 2.0027e-06\n",
      "\n",
      "Start of epoch 543\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.1235e-06 - val_loss: 1.9970e-06\n",
      "\n",
      "Start of epoch 544\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1145e-06 - val_loss: 1.9898e-06\n",
      "\n",
      "Start of epoch 545\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.1039e-06 - val_loss: 1.9831e-06\n",
      "\n",
      "Start of epoch 546\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0925e-06 - val_loss: 1.9768e-06\n",
      "\n",
      "Start of epoch 547\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.0819e-06 - val_loss: 1.9707e-06\n",
      "\n",
      "Start of epoch 548\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.0720e-06 - val_loss: 1.9649e-06\n",
      "\n",
      "Start of epoch 549\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 2.0622e-06 - val_loss: 1.9597e-06\n",
      "\n",
      "Start of epoch 550\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.0529e-06 - val_loss: 1.9546e-06\n",
      "\n",
      "Start of epoch 551\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.0440e-06 - val_loss: 1.9482e-06\n",
      "\n",
      "Start of epoch 552\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.0347e-06 - val_loss: 1.9419e-06\n",
      "\n",
      "Start of epoch 553\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.0257e-06 - val_loss: 1.9350e-06\n",
      "\n",
      "Start of epoch 554\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.0174e-06 - val_loss: 1.9290e-06\n",
      "\n",
      "Start of epoch 555\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.0094e-06 - val_loss: 1.9239e-06\n",
      "\n",
      "Start of epoch 556\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.0009e-06 - val_loss: 1.9183e-06\n",
      "\n",
      "Start of epoch 557\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.9930e-06 - val_loss: 1.9130e-06\n",
      "\n",
      "Start of epoch 558\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.9856e-06 - val_loss: 1.9078e-06\n",
      "\n",
      "Start of epoch 559\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.9786e-06 - val_loss: 1.9028e-06\n",
      "\n",
      "Start of epoch 560\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.9711e-06 - val_loss: 1.8973e-06\n",
      "\n",
      "Start of epoch 561\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.9629e-06 - val_loss: 1.8911e-06\n",
      "\n",
      "Start of epoch 562\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.9553e-06 - val_loss: 1.8850e-06\n",
      "\n",
      "Start of epoch 563\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.9474e-06 - val_loss: 1.8800e-06\n",
      "\n",
      "Start of epoch 564\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.9383e-06 - val_loss: 1.8750e-06\n",
      "\n",
      "Start of epoch 565\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.9297e-06 - val_loss: 1.8711e-06\n",
      "\n",
      "Start of epoch 566\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.9205e-06 - val_loss: 1.8665e-06\n",
      "\n",
      "Start of epoch 567\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.9101e-06 - val_loss: 1.8605e-06\n",
      "\n",
      "Start of epoch 568\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.8980e-06 - val_loss: 1.8523e-06\n",
      "\n",
      "Start of epoch 569\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.8865e-06 - val_loss: 1.8457e-06\n",
      "\n",
      "Start of epoch 570\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8764e-06 - val_loss: 1.8397e-06\n",
      "\n",
      "Start of epoch 571\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8670e-06 - val_loss: 1.8340e-06\n",
      "\n",
      "Start of epoch 572\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.8584e-06 - val_loss: 1.8279e-06\n",
      "\n",
      "Start of epoch 573\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.8503e-06 - val_loss: 1.8223e-06\n",
      "\n",
      "Start of epoch 574\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.8425e-06 - val_loss: 1.8172e-06\n",
      "\n",
      "Start of epoch 575\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.8337e-06 - val_loss: 1.8121e-06\n",
      "\n",
      "Start of epoch 576\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.8257e-06 - val_loss: 1.8070e-06\n",
      "\n",
      "Start of epoch 577\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.8180e-06 - val_loss: 1.8030e-06\n",
      "\n",
      "Start of epoch 578\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.8099e-06 - val_loss: 1.7980e-06\n",
      "\n",
      "Start of epoch 579\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.8020e-06 - val_loss: 1.7932e-06\n",
      "\n",
      "Start of epoch 580\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 1.7940e-06 - val_loss: 1.7893e-06\n",
      "\n",
      "Start of epoch 581\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.7862e-06 - val_loss: 1.7849e-06\n",
      "\n",
      "Start of epoch 582\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7786e-06 - val_loss: 1.7795e-06\n",
      "\n",
      "Start of epoch 583\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.7708e-06 - val_loss: 1.7737e-06\n",
      "\n",
      "Start of epoch 584\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.7627e-06 - val_loss: 1.7692e-06\n",
      "\n",
      "Start of epoch 585\n",
      "1984/1984 [==============================] - 0s 43us/step - train_loss: 1.7546e-06 - val_loss: 1.7646e-06\n",
      "\n",
      "Start of epoch 586\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.7467e-06 - val_loss: 1.7595e-06\n",
      "\n",
      "Start of epoch 587\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7390e-06 - val_loss: 1.7546e-06\n",
      "\n",
      "Start of epoch 588\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7312e-06 - val_loss: 1.7497e-06\n",
      "\n",
      "Start of epoch 589\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.7236e-06 - val_loss: 1.7449e-06\n",
      "\n",
      "Start of epoch 590\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.7165e-06 - val_loss: 1.7405e-06\n",
      "\n",
      "Start of epoch 591\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.7096e-06 - val_loss: 1.7363e-06\n",
      "\n",
      "Start of epoch 592\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.7030e-06 - val_loss: 1.7321e-06\n",
      "\n",
      "Start of epoch 593\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6964e-06 - val_loss: 1.7282e-06\n",
      "\n",
      "Start of epoch 594\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6893e-06 - val_loss: 1.7246e-06\n",
      "\n",
      "Start of epoch 595\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6818e-06 - val_loss: 1.7207e-06\n",
      "\n",
      "Start of epoch 596\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6751e-06 - val_loss: 1.7170e-06\n",
      "\n",
      "Start of epoch 597\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.6687e-06 - val_loss: 1.7136e-06\n",
      "\n",
      "Start of epoch 598\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.6623e-06 - val_loss: 1.7097e-06\n",
      "\n",
      "Start of epoch 599\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.6561e-06 - val_loss: 1.7055e-06\n",
      "\n",
      "Start of epoch 600\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.6496e-06 - val_loss: 1.7009e-06\n",
      "\n",
      "Start of epoch 601\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6431e-06 - val_loss: 1.6977e-06\n",
      "\n",
      "Start of epoch 602\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.6369e-06 - val_loss: 1.6934e-06\n",
      "\n",
      "Start of epoch 603\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.6310e-06 - val_loss: 1.6885e-06\n",
      "\n",
      "Start of epoch 604\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6252e-06 - val_loss: 1.6835e-06\n",
      "\n",
      "Start of epoch 605\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6194e-06 - val_loss: 1.6792e-06\n",
      "\n",
      "Start of epoch 606\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6137e-06 - val_loss: 1.6745e-06\n",
      "\n",
      "Start of epoch 607\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6078e-06 - val_loss: 1.6702e-06\n",
      "\n",
      "Start of epoch 608\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6017e-06 - val_loss: 1.6664e-06\n",
      "\n",
      "Start of epoch 609\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.5950e-06 - val_loss: 1.6627e-06\n",
      "\n",
      "Start of epoch 610\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.5882e-06 - val_loss: 1.6567e-06\n",
      "\n",
      "Start of epoch 611\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.5815e-06 - val_loss: 1.6510e-06\n",
      "\n",
      "Start of epoch 612\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5743e-06 - val_loss: 1.6455e-06\n",
      "\n",
      "Start of epoch 613\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5675e-06 - val_loss: 1.6405e-06\n",
      "\n",
      "Start of epoch 614\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.5614e-06 - val_loss: 1.6359e-06\n",
      "\n",
      "Start of epoch 615\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 1.5556e-06 - val_loss: 1.6320e-06\n",
      "\n",
      "Start of epoch 616\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 1.5497e-06 - val_loss: 1.6275e-06\n",
      "\n",
      "Start of epoch 617\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.5437e-06 - val_loss: 1.6235e-06\n",
      "\n",
      "Start of epoch 618\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5374e-06 - val_loss: 1.6192e-06\n",
      "\n",
      "Start of epoch 619\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.5308e-06 - val_loss: 1.6169e-06\n",
      "\n",
      "Start of epoch 620\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.5245e-06 - val_loss: 1.6135e-06\n",
      "\n",
      "Start of epoch 621\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.5181e-06 - val_loss: 1.6106e-06\n",
      "\n",
      "Start of epoch 622\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.5116e-06 - val_loss: 1.6064e-06\n",
      "\n",
      "Start of epoch 623\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5053e-06 - val_loss: 1.6020e-06\n",
      "\n",
      "Start of epoch 624\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4994e-06 - val_loss: 1.5978e-06\n",
      "\n",
      "Start of epoch 625\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4936e-06 - val_loss: 1.5937e-06\n",
      "\n",
      "Start of epoch 626\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4881e-06 - val_loss: 1.5895e-06\n",
      "\n",
      "Start of epoch 627\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4826e-06 - val_loss: 1.5853e-06\n",
      "\n",
      "Start of epoch 628\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4773e-06 - val_loss: 1.5814e-06\n",
      "\n",
      "Start of epoch 629\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4721e-06 - val_loss: 1.5773e-06\n",
      "\n",
      "Start of epoch 630\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4671e-06 - val_loss: 1.5735e-06\n",
      "\n",
      "Start of epoch 631\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4616e-06 - val_loss: 1.5695e-06\n",
      "\n",
      "Start of epoch 632\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4561e-06 - val_loss: 1.5684e-06\n",
      "\n",
      "Start of epoch 633\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.4507e-06 - val_loss: 1.5652e-06\n",
      "\n",
      "Start of epoch 634\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.4453e-06 - val_loss: 1.5620e-06\n",
      "\n",
      "Start of epoch 635\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4395e-06 - val_loss: 1.5575e-06\n",
      "\n",
      "Start of epoch 636\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.4339e-06 - val_loss: 1.5533e-06\n",
      "\n",
      "Start of epoch 637\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4284e-06 - val_loss: 1.5486e-06\n",
      "\n",
      "Start of epoch 638\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.4229e-06 - val_loss: 1.5439e-06\n",
      "\n",
      "Start of epoch 639\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4176e-06 - val_loss: 1.5393e-06\n",
      "\n",
      "Start of epoch 640\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4124e-06 - val_loss: 1.5348e-06\n",
      "\n",
      "Start of epoch 641\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4073e-06 - val_loss: 1.5303e-06\n",
      "\n",
      "Start of epoch 642\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.4022e-06 - val_loss: 1.5259e-06\n",
      "\n",
      "Start of epoch 643\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 1.3972e-06 - val_loss: 1.5215e-06\n",
      "\n",
      "Start of epoch 644\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3920e-06 - val_loss: 1.5171e-06\n",
      "\n",
      "Start of epoch 645\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3867e-06 - val_loss: 1.5136e-06\n",
      "\n",
      "Start of epoch 646\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3815e-06 - val_loss: 1.5095e-06\n",
      "\n",
      "Start of epoch 647\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3762e-06 - val_loss: 1.5056e-06\n",
      "\n",
      "Start of epoch 648\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3710e-06 - val_loss: 1.5014e-06\n",
      "\n",
      "Start of epoch 649\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3659e-06 - val_loss: 1.4972e-06\n",
      "\n",
      "Start of epoch 650\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3607e-06 - val_loss: 1.4929e-06\n",
      "\n",
      "Start of epoch 651\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3556e-06 - val_loss: 1.4886e-06\n",
      "\n",
      "Start of epoch 652\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3504e-06 - val_loss: 1.4845e-06\n",
      "\n",
      "Start of epoch 653\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3449e-06 - val_loss: 1.4797e-06\n",
      "\n",
      "Start of epoch 654\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3395e-06 - val_loss: 1.4753e-06\n",
      "\n",
      "Start of epoch 655\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3342e-06 - val_loss: 1.4712e-06\n",
      "\n",
      "Start of epoch 656\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3289e-06 - val_loss: 1.4669e-06\n",
      "\n",
      "Start of epoch 657\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3237e-06 - val_loss: 1.4625e-06\n",
      "\n",
      "Start of epoch 658\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3185e-06 - val_loss: 1.4582e-06\n",
      "\n",
      "Start of epoch 659\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3134e-06 - val_loss: 1.4537e-06\n",
      "\n",
      "Start of epoch 660\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3082e-06 - val_loss: 1.4492e-06\n",
      "\n",
      "Start of epoch 661\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3031e-06 - val_loss: 1.4446e-06\n",
      "\n",
      "Start of epoch 662\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2979e-06 - val_loss: 1.4397e-06\n",
      "\n",
      "Start of epoch 663\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.2928e-06 - val_loss: 1.4346e-06\n",
      "\n",
      "Start of epoch 664\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.2876e-06 - val_loss: 1.4296e-06\n",
      "\n",
      "Start of epoch 665\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.2825e-06 - val_loss: 1.4245e-06\n",
      "\n",
      "Start of epoch 666\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2773e-06 - val_loss: 1.4194e-06\n",
      "\n",
      "Start of epoch 667\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2721e-06 - val_loss: 1.4143e-06\n",
      "\n",
      "Start of epoch 668\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2670e-06 - val_loss: 1.4092e-06\n",
      "\n",
      "Start of epoch 669\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2617e-06 - val_loss: 1.4041e-06\n",
      "\n",
      "Start of epoch 670\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2565e-06 - val_loss: 1.3988e-06\n",
      "\n",
      "Start of epoch 671\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2511e-06 - val_loss: 1.3935e-06\n",
      "\n",
      "Start of epoch 672\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2458e-06 - val_loss: 1.3879e-06\n",
      "\n",
      "Start of epoch 673\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 1.2404e-06 - val_loss: 1.3824e-06\n",
      "\n",
      "Start of epoch 674\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2350e-06 - val_loss: 1.3767e-06\n",
      "\n",
      "Start of epoch 675\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2297e-06 - val_loss: 1.3710e-06\n",
      "\n",
      "Start of epoch 676\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2240e-06 - val_loss: 1.3654e-06\n",
      "\n",
      "Start of epoch 677\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2186e-06 - val_loss: 1.3597e-06\n",
      "\n",
      "Start of epoch 678\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.2129e-06 - val_loss: 1.3537e-06\n",
      "\n",
      "Start of epoch 679\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2071e-06 - val_loss: 1.3477e-06\n",
      "\n",
      "Start of epoch 680\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 1.2012e-06 - val_loss: 1.3417e-06\n",
      "\n",
      "Start of epoch 681\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1953e-06 - val_loss: 1.3357e-06\n",
      "\n",
      "Start of epoch 682\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1893e-06 - val_loss: 1.3295e-06\n",
      "\n",
      "Start of epoch 683\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1833e-06 - val_loss: 1.3232e-06\n",
      "\n",
      "Start of epoch 684\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1772e-06 - val_loss: 1.3166e-06\n",
      "\n",
      "Start of epoch 685\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1713e-06 - val_loss: 1.3094e-06\n",
      "\n",
      "Start of epoch 686\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1651e-06 - val_loss: 1.3025e-06\n",
      "\n",
      "Start of epoch 687\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1588e-06 - val_loss: 1.2956e-06\n",
      "\n",
      "Start of epoch 688\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1526e-06 - val_loss: 1.2888e-06\n",
      "\n",
      "Start of epoch 689\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1461e-06 - val_loss: 1.2808e-06\n",
      "\n",
      "Start of epoch 690\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1388e-06 - val_loss: 1.2718e-06\n",
      "\n",
      "Start of epoch 691\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1318e-06 - val_loss: 1.2616e-06\n",
      "\n",
      "Start of epoch 692\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.1242e-06 - val_loss: 1.2523e-06\n",
      "\n",
      "Start of epoch 693\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1172e-06 - val_loss: 1.2436e-06\n",
      "\n",
      "Start of epoch 694\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.1103e-06 - val_loss: 1.2350e-06\n",
      "\n",
      "Start of epoch 695\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1036e-06 - val_loss: 1.2268e-06\n",
      "\n",
      "Start of epoch 696\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0970e-06 - val_loss: 1.2187e-06\n",
      "\n",
      "Start of epoch 697\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0906e-06 - val_loss: 1.2107e-06\n",
      "\n",
      "Start of epoch 698\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.0842e-06 - val_loss: 1.2040e-06\n",
      "\n",
      "Start of epoch 699\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.0777e-06 - val_loss: 1.1960e-06\n",
      "\n",
      "Start of epoch 700\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.0710e-06 - val_loss: 1.1888e-06\n",
      "\n",
      "Start of epoch 701\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.0643e-06 - val_loss: 1.1812e-06\n",
      "\n",
      "Start of epoch 702\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0578e-06 - val_loss: 1.1739e-06\n",
      "\n",
      "Start of epoch 703\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0516e-06 - val_loss: 1.1671e-06\n",
      "\n",
      "Start of epoch 704\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.0456e-06 - val_loss: 1.1603e-06\n",
      "\n",
      "Start of epoch 705\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.0396e-06 - val_loss: 1.1537e-06\n",
      "\n",
      "Start of epoch 706\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.0338e-06 - val_loss: 1.1469e-06\n",
      "\n",
      "Start of epoch 707\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.0281e-06 - val_loss: 1.1403e-06\n",
      "\n",
      "Start of epoch 708\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.0224e-06 - val_loss: 1.1334e-06\n",
      "\n",
      "Start of epoch 709\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.0169e-06 - val_loss: 1.1267e-06\n",
      "\n",
      "Start of epoch 710\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.0115e-06 - val_loss: 1.1201e-06\n",
      "\n",
      "Start of epoch 711\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.0060e-06 - val_loss: 1.1137e-06\n",
      "\n",
      "Start of epoch 712\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0006e-06 - val_loss: 1.1083e-06\n",
      "\n",
      "Start of epoch 713\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.9533e-07 - val_loss: 1.1019e-06\n",
      "\n",
      "Start of epoch 714\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.9010e-07 - val_loss: 1.0955e-06\n",
      "\n",
      "Start of epoch 715\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.8486e-07 - val_loss: 1.0891e-06\n",
      "\n",
      "Start of epoch 716\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.7989e-07 - val_loss: 1.0828e-06\n",
      "\n",
      "Start of epoch 717\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.7457e-07 - val_loss: 1.0766e-06\n",
      "\n",
      "Start of epoch 718\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.6936e-07 - val_loss: 1.0704e-06\n",
      "\n",
      "Start of epoch 719\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.6414e-07 - val_loss: 1.0640e-06\n",
      "\n",
      "Start of epoch 720\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 9.5907e-07 - val_loss: 1.0578e-06\n",
      "\n",
      "Start of epoch 721\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.5410e-07 - val_loss: 1.0517e-06\n",
      "\n",
      "Start of epoch 722\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.4920e-07 - val_loss: 1.0457e-06\n",
      "\n",
      "Start of epoch 723\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.4434e-07 - val_loss: 1.0410e-06\n",
      "\n",
      "Start of epoch 724\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.3976e-07 - val_loss: 1.0344e-06\n",
      "\n",
      "Start of epoch 725\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.3463e-07 - val_loss: 1.0285e-06\n",
      "\n",
      "Start of epoch 726\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.2964e-07 - val_loss: 1.0226e-06\n",
      "\n",
      "Start of epoch 727\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.2473e-07 - val_loss: 1.0169e-06\n",
      "\n",
      "Start of epoch 728\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.1889e-07 - val_loss: 1.0091e-06\n",
      "\n",
      "Start of epoch 729\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.1258e-07 - val_loss: 1.0023e-06\n",
      "\n",
      "Start of epoch 730\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.0692e-07 - val_loss: 9.9666e-07\n",
      "\n",
      "Start of epoch 731\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.0097e-07 - val_loss: 9.8925e-07\n",
      "\n",
      "Start of epoch 732\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.9440e-07 - val_loss: 9.8087e-07\n",
      "\n",
      "Start of epoch 733\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.8817e-07 - val_loss: 9.7267e-07\n",
      "\n",
      "Start of epoch 734\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.8246e-07 - val_loss: 9.6605e-07\n",
      "\n",
      "Start of epoch 735\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.7662e-07 - val_loss: 9.5718e-07\n",
      "\n",
      "Start of epoch 736\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.7066e-07 - val_loss: 9.4892e-07\n",
      "\n",
      "Start of epoch 737\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.6501e-07 - val_loss: 9.4133e-07\n",
      "\n",
      "Start of epoch 738\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.5975e-07 - val_loss: 9.3420e-07\n",
      "\n",
      "Start of epoch 739\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 8.5457e-07 - val_loss: 9.2840e-07\n",
      "\n",
      "Start of epoch 740\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 8.4961e-07 - val_loss: 9.2181e-07\n",
      "\n",
      "Start of epoch 741\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.4477e-07 - val_loss: 9.1577e-07\n",
      "\n",
      "Start of epoch 742\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.4020e-07 - val_loss: 9.1002e-07\n",
      "\n",
      "Start of epoch 743\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.3569e-07 - val_loss: 9.0353e-07\n",
      "\n",
      "Start of epoch 744\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.3095e-07 - val_loss: 8.9724e-07\n",
      "\n",
      "Start of epoch 745\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.2553e-07 - val_loss: 8.9003e-07\n",
      "\n",
      "Start of epoch 746\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.1997e-07 - val_loss: 8.8364e-07\n",
      "\n",
      "Start of epoch 747\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.1498e-07 - val_loss: 8.7761e-07\n",
      "\n",
      "Start of epoch 748\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.0981e-07 - val_loss: 8.7119e-07\n",
      "\n",
      "Start of epoch 749\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.0385e-07 - val_loss: 8.6454e-07\n",
      "\n",
      "Start of epoch 750\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.9820e-07 - val_loss: 8.5978e-07\n",
      "\n",
      "Start of epoch 751\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.9295e-07 - val_loss: 8.5435e-07\n",
      "\n",
      "Start of epoch 752\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.8766e-07 - val_loss: 8.4910e-07\n",
      "\n",
      "Start of epoch 753\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.8171e-07 - val_loss: 8.4400e-07\n",
      "\n",
      "Start of epoch 754\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.7603e-07 - val_loss: 8.3831e-07\n",
      "\n",
      "Start of epoch 755\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.7026e-07 - val_loss: 8.3313e-07\n",
      "\n",
      "Start of epoch 756\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.6508e-07 - val_loss: 8.2793e-07\n",
      "\n",
      "Start of epoch 757\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.5974e-07 - val_loss: 8.2273e-07\n",
      "\n",
      "Start of epoch 758\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 7.5524e-07 - val_loss: 8.1756e-07\n",
      "\n",
      "Start of epoch 759\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.5035e-07 - val_loss: 8.1224e-07\n",
      "\n",
      "Start of epoch 760\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 7.4524e-07 - val_loss: 8.0462e-07\n",
      "\n",
      "Start of epoch 761\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.3957e-07 - val_loss: 7.9891e-07\n",
      "\n",
      "Start of epoch 762\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.3413e-07 - val_loss: 7.9344e-07\n",
      "\n",
      "Start of epoch 763\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 7.2870e-07 - val_loss: 7.8827e-07\n",
      "\n",
      "Start of epoch 764\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.2381e-07 - val_loss: 7.8349e-07\n",
      "\n",
      "Start of epoch 765\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.1936e-07 - val_loss: 7.7811e-07\n",
      "\n",
      "Start of epoch 766\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.1445e-07 - val_loss: 7.7277e-07\n",
      "\n",
      "Start of epoch 767\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.0988e-07 - val_loss: 7.6783e-07\n",
      "\n",
      "Start of epoch 768\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.0563e-07 - val_loss: 7.6282e-07\n",
      "\n",
      "Start of epoch 769\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 7.0153e-07 - val_loss: 7.5798e-07\n",
      "\n",
      "Start of epoch 770\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.9746e-07 - val_loss: 7.5319e-07\n",
      "\n",
      "Start of epoch 771\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.9337e-07 - val_loss: 7.4830e-07\n",
      "\n",
      "Start of epoch 772\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.8938e-07 - val_loss: 7.4369e-07\n",
      "\n",
      "Start of epoch 773\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.8560e-07 - val_loss: 7.3913e-07\n",
      "\n",
      "Start of epoch 774\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.8182e-07 - val_loss: 7.3467e-07\n",
      "\n",
      "Start of epoch 775\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.7817e-07 - val_loss: 7.3035e-07\n",
      "\n",
      "Start of epoch 776\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.7469e-07 - val_loss: 7.2603e-07\n",
      "\n",
      "Start of epoch 777\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.7132e-07 - val_loss: 7.2223e-07\n",
      "\n",
      "Start of epoch 778\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 6.6784e-07 - val_loss: 7.1801e-07\n",
      "\n",
      "Start of epoch 779\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.6452e-07 - val_loss: 7.1409e-07\n",
      "\n",
      "Start of epoch 780\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.6108e-07 - val_loss: 7.1034e-07\n",
      "\n",
      "Start of epoch 781\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.5722e-07 - val_loss: 7.0649e-07\n",
      "\n",
      "Start of epoch 782\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.5370e-07 - val_loss: 7.0308e-07\n",
      "\n",
      "Start of epoch 783\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.4995e-07 - val_loss: 6.9981e-07\n",
      "\n",
      "Start of epoch 784\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.4636e-07 - val_loss: 6.9638e-07\n",
      "\n",
      "Start of epoch 785\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.4291e-07 - val_loss: 6.9278e-07\n",
      "\n",
      "Start of epoch 786\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.3979e-07 - val_loss: 6.8936e-07\n",
      "\n",
      "Start of epoch 787\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.3678e-07 - val_loss: 6.8574e-07\n",
      "\n",
      "Start of epoch 788\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.3383e-07 - val_loss: 6.8248e-07\n",
      "\n",
      "Start of epoch 789\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.3066e-07 - val_loss: 6.7937e-07\n",
      "\n",
      "Start of epoch 790\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.2759e-07 - val_loss: 6.7608e-07\n",
      "\n",
      "Start of epoch 791\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.2469e-07 - val_loss: 6.7277e-07\n",
      "\n",
      "Start of epoch 792\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.2188e-07 - val_loss: 6.6973e-07\n",
      "\n",
      "Start of epoch 793\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.1917e-07 - val_loss: 6.6671e-07\n",
      "\n",
      "Start of epoch 794\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.1654e-07 - val_loss: 6.6399e-07\n",
      "\n",
      "Start of epoch 795\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.1376e-07 - val_loss: 6.6092e-07\n",
      "\n",
      "Start of epoch 796\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 6.1095e-07 - val_loss: 6.5791e-07\n",
      "\n",
      "Start of epoch 797\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 6.0813e-07 - val_loss: 6.5504e-07\n",
      "\n",
      "Start of epoch 798\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 6.0529e-07 - val_loss: 6.5253e-07\n",
      "\n",
      "Start of epoch 799\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 6.0243e-07 - val_loss: 6.4924e-07\n",
      "\n",
      "Start of epoch 800\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 5.9961e-07 - val_loss: 6.4637e-07\n",
      "\n",
      "Start of epoch 801\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.9675e-07 - val_loss: 6.4398e-07\n",
      "\n",
      "Start of epoch 802\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.9403e-07 - val_loss: 6.4137e-07\n",
      "\n",
      "Start of epoch 803\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9150e-07 - val_loss: 6.3890e-07\n",
      "\n",
      "Start of epoch 804\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.8906e-07 - val_loss: 6.3624e-07\n",
      "\n",
      "Start of epoch 805\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.8669e-07 - val_loss: 6.3342e-07\n",
      "\n",
      "Start of epoch 806\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8424e-07 - val_loss: 6.3062e-07\n",
      "\n",
      "Start of epoch 807\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8199e-07 - val_loss: 6.2804e-07\n",
      "\n",
      "Start of epoch 808\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.7961e-07 - val_loss: 6.2543e-07\n",
      "\n",
      "Start of epoch 809\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7736e-07 - val_loss: 6.2271e-07\n",
      "\n",
      "Start of epoch 810\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.7514e-07 - val_loss: 6.2004e-07\n",
      "\n",
      "Start of epoch 811\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.7291e-07 - val_loss: 6.1744e-07\n",
      "\n",
      "Start of epoch 812\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 5.7077e-07 - val_loss: 6.1496e-07\n",
      "\n",
      "Start of epoch 813\n",
      "1984/1984 [==============================] - 0s 49us/step - train_loss: 5.6872e-07 - val_loss: 6.1246e-07\n",
      "\n",
      "Start of epoch 814\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 5.6653e-07 - val_loss: 6.0972e-07\n",
      "\n",
      "Start of epoch 815\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.6438e-07 - val_loss: 6.0732e-07\n",
      "\n",
      "Start of epoch 816\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.6231e-07 - val_loss: 6.0488e-07\n",
      "\n",
      "Start of epoch 817\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.6023e-07 - val_loss: 6.0250e-07\n",
      "\n",
      "Start of epoch 818\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.5820e-07 - val_loss: 6.0007e-07\n",
      "\n",
      "Start of epoch 819\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.5612e-07 - val_loss: 5.9785e-07\n",
      "\n",
      "Start of epoch 820\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 5.5408e-07 - val_loss: 5.9542e-07\n",
      "\n",
      "Start of epoch 821\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 5.5206e-07 - val_loss: 5.9284e-07\n",
      "\n",
      "Start of epoch 822\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 5.5008e-07 - val_loss: 5.9047e-07\n",
      "\n",
      "Start of epoch 823\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 5.4812e-07 - val_loss: 5.8819e-07\n",
      "\n",
      "Start of epoch 824\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.4607e-07 - val_loss: 5.8578e-07\n",
      "\n",
      "Start of epoch 825\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.4412e-07 - val_loss: 5.8371e-07\n",
      "\n",
      "Start of epoch 826\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 5.4224e-07 - val_loss: 5.8166e-07\n",
      "\n",
      "Start of epoch 827\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.4049e-07 - val_loss: 5.7908e-07\n",
      "\n",
      "Start of epoch 828\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.3859e-07 - val_loss: 5.7704e-07\n",
      "\n",
      "Start of epoch 829\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3657e-07 - val_loss: 5.7487e-07\n",
      "\n",
      "Start of epoch 830\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.3484e-07 - val_loss: 5.7277e-07\n",
      "\n",
      "Start of epoch 831\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 5.3288e-07 - val_loss: 5.7006e-07\n",
      "\n",
      "Start of epoch 832\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 5.3076e-07 - val_loss: 5.6781e-07\n",
      "\n",
      "Start of epoch 833\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.2872e-07 - val_loss: 5.6561e-07\n",
      "\n",
      "Start of epoch 834\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2666e-07 - val_loss: 5.6331e-07\n",
      "\n",
      "Start of epoch 835\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2463e-07 - val_loss: 5.6095e-07\n",
      "\n",
      "Start of epoch 836\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.2261e-07 - val_loss: 5.5867e-07\n",
      "\n",
      "Start of epoch 837\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2082e-07 - val_loss: 5.5639e-07\n",
      "\n",
      "Start of epoch 838\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.1878e-07 - val_loss: 5.5413e-07\n",
      "\n",
      "Start of epoch 839\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 5.1688e-07 - val_loss: 5.5210e-07\n",
      "\n",
      "Start of epoch 840\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 5.1511e-07 - val_loss: 5.5012e-07\n",
      "\n",
      "Start of epoch 841\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 5.1340e-07 - val_loss: 5.4817e-07\n",
      "\n",
      "Start of epoch 842\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 5.1166e-07 - val_loss: 5.4629e-07\n",
      "\n",
      "Start of epoch 843\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.0993e-07 - val_loss: 5.4417e-07\n",
      "\n",
      "Start of epoch 844\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.0830e-07 - val_loss: 5.4237e-07\n",
      "\n",
      "Start of epoch 845\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 5.0690e-07 - val_loss: 5.4055e-07\n",
      "\n",
      "Start of epoch 846\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.0523e-07 - val_loss: 5.3840e-07\n",
      "\n",
      "Start of epoch 847\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 5.0376e-07 - val_loss: 5.3648e-07\n",
      "\n",
      "Start of epoch 848\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 5.0214e-07 - val_loss: 5.3464e-07\n",
      "\n",
      "Start of epoch 849\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 5.0057e-07 - val_loss: 5.3317e-07\n",
      "\n",
      "Start of epoch 850\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 4.9907e-07 - val_loss: 5.3130e-07\n",
      "\n",
      "Start of epoch 851\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.9754e-07 - val_loss: 5.2951e-07\n",
      "\n",
      "Start of epoch 852\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 4.9598e-07 - val_loss: 5.2782e-07\n",
      "\n",
      "Start of epoch 853\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.9450e-07 - val_loss: 5.2598e-07\n",
      "\n",
      "Start of epoch 854\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.9306e-07 - val_loss: 5.2441e-07\n",
      "\n",
      "Start of epoch 855\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.9164e-07 - val_loss: 5.2261e-07\n",
      "\n",
      "Start of epoch 856\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 4.9021e-07 - val_loss: 5.2083e-07\n",
      "\n",
      "Start of epoch 857\n",
      "1984/1984 [==============================] - 0s 45us/step - train_loss: 4.8863e-07 - val_loss: 5.1902e-07\n",
      "\n",
      "Start of epoch 858\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 4.8706e-07 - val_loss: 5.1749e-07\n",
      "\n",
      "Start of epoch 859\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 4.8551e-07 - val_loss: 5.1558e-07\n",
      "\n",
      "Start of epoch 860\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 4.8396e-07 - val_loss: 5.1401e-07\n",
      "\n",
      "Start of epoch 861\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 4.8244e-07 - val_loss: 5.1241e-07\n",
      "\n",
      "Start of epoch 862\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 4.8084e-07 - val_loss: 5.1084e-07\n",
      "\n",
      "Start of epoch 863\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 4.7932e-07 - val_loss: 5.0924e-07\n",
      "\n",
      "Start of epoch 864\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 4.7783e-07 - val_loss: 5.0761e-07\n",
      "\n",
      "Start of epoch 865\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 4.7628e-07 - val_loss: 5.0605e-07\n",
      "\n",
      "Start of epoch 866\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.7499e-07 - val_loss: 5.0449e-07\n",
      "\n",
      "Start of epoch 867\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.7345e-07 - val_loss: 5.0289e-07\n",
      "\n",
      "Start of epoch 868\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.7219e-07 - val_loss: 5.0142e-07\n",
      "\n",
      "Start of epoch 869\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.7084e-07 - val_loss: 4.9985e-07\n",
      "\n",
      "Start of epoch 870\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 4.6946e-07 - val_loss: 4.9847e-07\n",
      "\n",
      "Start of epoch 871\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.6814e-07 - val_loss: 4.9705e-07\n",
      "\n",
      "Start of epoch 872\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.6698e-07 - val_loss: 4.9566e-07\n",
      "\n",
      "Start of epoch 873\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.6576e-07 - val_loss: 4.9421e-07\n",
      "\n",
      "Start of epoch 874\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.6454e-07 - val_loss: 4.9285e-07\n",
      "\n",
      "Start of epoch 875\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.6330e-07 - val_loss: 4.9155e-07\n",
      "\n",
      "Start of epoch 876\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.6212e-07 - val_loss: 4.9034e-07\n",
      "\n",
      "Start of epoch 877\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.6092e-07 - val_loss: 4.8881e-07\n",
      "\n",
      "Start of epoch 878\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.5975e-07 - val_loss: 4.8754e-07\n",
      "\n",
      "Start of epoch 879\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.5864e-07 - val_loss: 4.8627e-07\n",
      "\n",
      "Start of epoch 880\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.5747e-07 - val_loss: 4.8503e-07\n",
      "\n",
      "Start of epoch 881\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.5633e-07 - val_loss: 4.8374e-07\n",
      "\n",
      "Start of epoch 882\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.5502e-07 - val_loss: 4.8256e-07\n",
      "\n",
      "Start of epoch 883\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.5395e-07 - val_loss: 4.8136e-07\n",
      "\n",
      "Start of epoch 884\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.5280e-07 - val_loss: 4.8013e-07\n",
      "\n",
      "Start of epoch 885\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.5156e-07 - val_loss: 4.7898e-07\n",
      "\n",
      "Start of epoch 886\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.5056e-07 - val_loss: 4.7778e-07\n",
      "\n",
      "Start of epoch 887\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.4939e-07 - val_loss: 4.7652e-07\n",
      "\n",
      "Start of epoch 888\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 4.4837e-07 - val_loss: 4.7539e-07\n",
      "\n",
      "Start of epoch 889\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.4734e-07 - val_loss: 4.7422e-07\n",
      "\n",
      "Start of epoch 890\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.4629e-07 - val_loss: 4.7306e-07\n",
      "\n",
      "Start of epoch 891\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.4523e-07 - val_loss: 4.7191e-07\n",
      "\n",
      "Start of epoch 892\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.4421e-07 - val_loss: 4.7082e-07\n",
      "\n",
      "Start of epoch 893\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.4314e-07 - val_loss: 4.6968e-07\n",
      "\n",
      "Start of epoch 894\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.4215e-07 - val_loss: 4.6860e-07\n",
      "\n",
      "Start of epoch 895\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.4114e-07 - val_loss: 4.6751e-07\n",
      "\n",
      "Start of epoch 896\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 4.4012e-07 - val_loss: 4.6628e-07\n",
      "\n",
      "Start of epoch 897\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.3911e-07 - val_loss: 4.6529e-07\n",
      "\n",
      "Start of epoch 898\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.3806e-07 - val_loss: 4.6412e-07\n",
      "\n",
      "Start of epoch 899\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.3705e-07 - val_loss: 4.6310e-07\n",
      "\n",
      "Start of epoch 900\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.3600e-07 - val_loss: 4.6209e-07\n",
      "\n",
      "Start of epoch 901\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.3500e-07 - val_loss: 4.6105e-07\n",
      "\n",
      "Start of epoch 902\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.3403e-07 - val_loss: 4.5997e-07\n",
      "\n",
      "Start of epoch 903\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.3310e-07 - val_loss: 4.5891e-07\n",
      "\n",
      "Start of epoch 904\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.3210e-07 - val_loss: 4.5784e-07\n",
      "\n",
      "Start of epoch 905\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 4.3121e-07 - val_loss: 4.5679e-07\n",
      "\n",
      "Start of epoch 906\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.3020e-07 - val_loss: 4.5579e-07\n",
      "\n",
      "Start of epoch 907\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.2928e-07 - val_loss: 4.5479e-07\n",
      "\n",
      "Start of epoch 908\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.2836e-07 - val_loss: 4.5379e-07\n",
      "\n",
      "Start of epoch 909\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.2737e-07 - val_loss: 4.5277e-07\n",
      "\n",
      "Start of epoch 910\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.2655e-07 - val_loss: 4.5178e-07\n",
      "\n",
      "Start of epoch 911\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.2565e-07 - val_loss: 4.5078e-07\n",
      "\n",
      "Start of epoch 912\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.2472e-07 - val_loss: 4.4977e-07\n",
      "\n",
      "Start of epoch 913\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.2384e-07 - val_loss: 4.4887e-07\n",
      "\n",
      "Start of epoch 914\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2295e-07 - val_loss: 4.4788e-07\n",
      "\n",
      "Start of epoch 915\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.2208e-07 - val_loss: 4.4693e-07\n",
      "\n",
      "Start of epoch 916\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2119e-07 - val_loss: 4.4602e-07\n",
      "\n",
      "Start of epoch 917\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2036e-07 - val_loss: 4.4507e-07\n",
      "\n",
      "Start of epoch 918\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.1947e-07 - val_loss: 4.4416e-07\n",
      "\n",
      "Start of epoch 919\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.1870e-07 - val_loss: 4.4319e-07\n",
      "\n",
      "Start of epoch 920\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.1775e-07 - val_loss: 4.4227e-07\n",
      "\n",
      "Start of epoch 921\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.1692e-07 - val_loss: 4.4137e-07\n",
      "\n",
      "Start of epoch 922\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.1604e-07 - val_loss: 4.4039e-07\n",
      "\n",
      "Start of epoch 923\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 4.1525e-07 - val_loss: 4.3948e-07\n",
      "\n",
      "Start of epoch 924\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.1443e-07 - val_loss: 4.3856e-07\n",
      "\n",
      "Start of epoch 925\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.1363e-07 - val_loss: 4.3763e-07\n",
      "\n",
      "Start of epoch 926\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.1278e-07 - val_loss: 4.3681e-07\n",
      "\n",
      "Start of epoch 927\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.1196e-07 - val_loss: 4.3600e-07\n",
      "\n",
      "Start of epoch 928\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.1121e-07 - val_loss: 4.3509e-07\n",
      "\n",
      "Start of epoch 929\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 4.1041e-07 - val_loss: 4.3421e-07\n",
      "\n",
      "Start of epoch 930\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.0953e-07 - val_loss: 4.3331e-07\n",
      "\n",
      "Start of epoch 931\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.0871e-07 - val_loss: 4.3245e-07\n",
      "\n",
      "Start of epoch 932\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0803e-07 - val_loss: 4.3138e-07\n",
      "\n",
      "Start of epoch 933\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 4.0717e-07 - val_loss: 4.3064e-07\n",
      "\n",
      "Start of epoch 934\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.0644e-07 - val_loss: 4.2977e-07\n",
      "\n",
      "Start of epoch 935\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0563e-07 - val_loss: 4.2885e-07\n",
      "\n",
      "Start of epoch 936\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.0487e-07 - val_loss: 4.2805e-07\n",
      "\n",
      "Start of epoch 937\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0409e-07 - val_loss: 4.2706e-07\n",
      "\n",
      "Start of epoch 938\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.0328e-07 - val_loss: 4.2623e-07\n",
      "\n",
      "Start of epoch 939\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0250e-07 - val_loss: 4.2539e-07\n",
      "\n",
      "Start of epoch 940\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 4.0173e-07 - val_loss: 4.2449e-07\n",
      "\n",
      "Start of epoch 941\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.0093e-07 - val_loss: 4.2349e-07\n",
      "\n",
      "Start of epoch 942\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0014e-07 - val_loss: 4.2262e-07\n",
      "\n",
      "Start of epoch 943\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.9939e-07 - val_loss: 4.2180e-07\n",
      "\n",
      "Start of epoch 944\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9846e-07 - val_loss: 4.2075e-07\n",
      "\n",
      "Start of epoch 945\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9780e-07 - val_loss: 4.1983e-07\n",
      "\n",
      "Start of epoch 946\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.9695e-07 - val_loss: 4.1902e-07\n",
      "\n",
      "Start of epoch 947\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9629e-07 - val_loss: 4.1826e-07\n",
      "\n",
      "Start of epoch 948\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9545e-07 - val_loss: 4.1718e-07\n",
      "\n",
      "Start of epoch 949\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9484e-07 - val_loss: 4.1625e-07\n",
      "\n",
      "Start of epoch 950\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9402e-07 - val_loss: 4.1551e-07\n",
      "\n",
      "Start of epoch 951\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9322e-07 - val_loss: 4.1470e-07\n",
      "\n",
      "Start of epoch 952\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9247e-07 - val_loss: 4.1384e-07\n",
      "\n",
      "Start of epoch 953\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.9173e-07 - val_loss: 4.1298e-07\n",
      "\n",
      "Start of epoch 954\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9104e-07 - val_loss: 4.1219e-07\n",
      "\n",
      "Start of epoch 955\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.9036e-07 - val_loss: 4.1137e-07\n",
      "\n",
      "Start of epoch 956\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.8960e-07 - val_loss: 4.1054e-07\n",
      "\n",
      "Start of epoch 957\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.8882e-07 - val_loss: 4.0977e-07\n",
      "\n",
      "Start of epoch 958\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.8816e-07 - val_loss: 4.0889e-07\n",
      "\n",
      "Start of epoch 959\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.8732e-07 - val_loss: 4.0810e-07\n",
      "\n",
      "Start of epoch 960\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.8658e-07 - val_loss: 4.0729e-07\n",
      "\n",
      "Start of epoch 961\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.8594e-07 - val_loss: 4.0650e-07\n",
      "\n",
      "Start of epoch 962\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.8514e-07 - val_loss: 4.0564e-07\n",
      "\n",
      "Start of epoch 963\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.8444e-07 - val_loss: 4.0482e-07\n",
      "\n",
      "Start of epoch 964\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.8371e-07 - val_loss: 4.0402e-07\n",
      "\n",
      "Start of epoch 965\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.8300e-07 - val_loss: 4.0326e-07\n",
      "\n",
      "Start of epoch 966\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.8236e-07 - val_loss: 4.0243e-07\n",
      "\n",
      "Start of epoch 967\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.8160e-07 - val_loss: 4.0169e-07\n",
      "\n",
      "Start of epoch 968\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.8093e-07 - val_loss: 4.0093e-07\n",
      "\n",
      "Start of epoch 969\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.8027e-07 - val_loss: 4.0010e-07\n",
      "\n",
      "Start of epoch 970\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.7953e-07 - val_loss: 3.9938e-07\n",
      "\n",
      "Start of epoch 971\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.7889e-07 - val_loss: 3.9858e-07\n",
      "\n",
      "Start of epoch 972\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.7815e-07 - val_loss: 3.9779e-07\n",
      "\n",
      "Start of epoch 973\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.7744e-07 - val_loss: 3.9703e-07\n",
      "\n",
      "Start of epoch 974\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.7679e-07 - val_loss: 3.9622e-07\n",
      "\n",
      "Start of epoch 975\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 3.7605e-07 - val_loss: 3.9551e-07\n",
      "\n",
      "Start of epoch 976\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.7542e-07 - val_loss: 3.9463e-07\n",
      "\n",
      "Start of epoch 977\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.7471e-07 - val_loss: 3.9390e-07\n",
      "\n",
      "Start of epoch 978\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.7411e-07 - val_loss: 3.9303e-07\n",
      "\n",
      "Start of epoch 979\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.7339e-07 - val_loss: 3.9234e-07\n",
      "\n",
      "Start of epoch 980\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.7266e-07 - val_loss: 3.9158e-07\n",
      "\n",
      "Start of epoch 981\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.7200e-07 - val_loss: 3.9083e-07\n",
      "\n",
      "Start of epoch 982\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.7139e-07 - val_loss: 3.9004e-07\n",
      "\n",
      "Start of epoch 983\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.7063e-07 - val_loss: 3.8934e-07\n",
      "\n",
      "Start of epoch 984\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.7003e-07 - val_loss: 3.8854e-07\n",
      "\n",
      "Start of epoch 985\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.6927e-07 - val_loss: 3.8785e-07\n",
      "\n",
      "Start of epoch 986\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.6865e-07 - val_loss: 3.8710e-07\n",
      "\n",
      "Start of epoch 987\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.6799e-07 - val_loss: 3.8630e-07\n",
      "\n",
      "Start of epoch 988\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.6734e-07 - val_loss: 3.8558e-07\n",
      "\n",
      "Start of epoch 989\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.6669e-07 - val_loss: 3.8479e-07\n",
      "\n",
      "Start of epoch 990\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 3.6593e-07 - val_loss: 3.8407e-07\n",
      "\n",
      "Start of epoch 991\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.6533e-07 - val_loss: 3.8328e-07\n",
      "\n",
      "Start of epoch 992\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.6465e-07 - val_loss: 3.8253e-07\n",
      "\n",
      "Start of epoch 993\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.6407e-07 - val_loss: 3.8149e-07\n",
      "\n",
      "Start of epoch 994\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.6328e-07 - val_loss: 3.8083e-07\n",
      "\n",
      "Start of epoch 995\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.6256e-07 - val_loss: 3.8010e-07\n",
      "\n",
      "Start of epoch 996\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.6198e-07 - val_loss: 3.7931e-07\n",
      "\n",
      "Start of epoch 997\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.6127e-07 - val_loss: 3.7859e-07\n",
      "\n",
      "Start of epoch 998\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.6057e-07 - val_loss: 3.7788e-07\n",
      "\n",
      "Start of epoch 999\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.5990e-07 - val_loss: 3.7709e-07\n",
      "\n",
      "Start of epoch 1000\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5924e-07 - val_loss: 3.7638e-07\n",
      "\n",
      "Start of epoch 1001\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5860e-07 - val_loss: 3.7563e-07\n",
      "\n",
      "Start of epoch 1002\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5784e-07 - val_loss: 3.7494e-07\n",
      "\n",
      "Start of epoch 1003\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5721e-07 - val_loss: 3.7429e-07\n",
      "\n",
      "Start of epoch 1004\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5660e-07 - val_loss: 3.7351e-07\n",
      "\n",
      "Start of epoch 1005\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 3.5586e-07 - val_loss: 3.7282e-07\n",
      "\n",
      "Start of epoch 1006\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5518e-07 - val_loss: 3.7203e-07\n",
      "\n",
      "Start of epoch 1007\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5456e-07 - val_loss: 3.7135e-07\n",
      "\n",
      "Start of epoch 1008\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.5394e-07 - val_loss: 3.7061e-07\n",
      "\n",
      "Start of epoch 1009\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.5325e-07 - val_loss: 3.6988e-07\n",
      "\n",
      "Start of epoch 1010\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5259e-07 - val_loss: 3.6915e-07\n",
      "\n",
      "Start of epoch 1011\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.5194e-07 - val_loss: 3.6839e-07\n",
      "\n",
      "Start of epoch 1012\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5128e-07 - val_loss: 3.6775e-07\n",
      "\n",
      "Start of epoch 1013\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5061e-07 - val_loss: 3.6700e-07\n",
      "\n",
      "Start of epoch 1014\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.4996e-07 - val_loss: 3.6629e-07\n",
      "\n",
      "Start of epoch 1015\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.4922e-07 - val_loss: 3.6552e-07\n",
      "\n",
      "Start of epoch 1016\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.4857e-07 - val_loss: 3.6492e-07\n",
      "\n",
      "Start of epoch 1017\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.4798e-07 - val_loss: 3.6412e-07\n",
      "\n",
      "Start of epoch 1018\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.4728e-07 - val_loss: 3.6347e-07\n",
      "\n",
      "Start of epoch 1019\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.4664e-07 - val_loss: 3.6274e-07\n",
      "\n",
      "Start of epoch 1020\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 3.4597e-07 - val_loss: 3.6202e-07\n",
      "\n",
      "Start of epoch 1021\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.4529e-07 - val_loss: 3.6131e-07\n",
      "\n",
      "Start of epoch 1022\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.4466e-07 - val_loss: 3.6057e-07\n",
      "\n",
      "Start of epoch 1023\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.4400e-07 - val_loss: 3.5989e-07\n",
      "\n",
      "Start of epoch 1024\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.4333e-07 - val_loss: 3.5921e-07\n",
      "\n",
      "Start of epoch 1025\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.4266e-07 - val_loss: 3.5849e-07\n",
      "\n",
      "Start of epoch 1026\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.4203e-07 - val_loss: 3.5783e-07\n",
      "\n",
      "Start of epoch 1027\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.4139e-07 - val_loss: 3.5712e-07\n",
      "\n",
      "Start of epoch 1028\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.4073e-07 - val_loss: 3.5644e-07\n",
      "\n",
      "Start of epoch 1029\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.4008e-07 - val_loss: 3.5572e-07\n",
      "\n",
      "Start of epoch 1030\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.3946e-07 - val_loss: 3.5505e-07\n",
      "\n",
      "Start of epoch 1031\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.3878e-07 - val_loss: 3.5433e-07\n",
      "\n",
      "Start of epoch 1032\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.3817e-07 - val_loss: 3.5366e-07\n",
      "\n",
      "Start of epoch 1033\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.3752e-07 - val_loss: 3.5295e-07\n",
      "\n",
      "Start of epoch 1034\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.3689e-07 - val_loss: 3.5233e-07\n",
      "\n",
      "Start of epoch 1035\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.3622e-07 - val_loss: 3.5158e-07\n",
      "\n",
      "Start of epoch 1036\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.3560e-07 - val_loss: 3.5086e-07\n",
      "\n",
      "Start of epoch 1037\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.3493e-07 - val_loss: 3.5017e-07\n",
      "\n",
      "Start of epoch 1038\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.3432e-07 - val_loss: 3.4949e-07\n",
      "\n",
      "Start of epoch 1039\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.3365e-07 - val_loss: 3.4877e-07\n",
      "\n",
      "Start of epoch 1040\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.3304e-07 - val_loss: 3.4810e-07\n",
      "\n",
      "Start of epoch 1041\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.3231e-07 - val_loss: 3.4739e-07\n",
      "\n",
      "Start of epoch 1042\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.3170e-07 - val_loss: 3.4672e-07\n",
      "\n",
      "Start of epoch 1043\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.3106e-07 - val_loss: 3.4601e-07\n",
      "\n",
      "Start of epoch 1044\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.3047e-07 - val_loss: 3.4533e-07\n",
      "\n",
      "Start of epoch 1045\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.2975e-07 - val_loss: 3.4465e-07\n",
      "\n",
      "Start of epoch 1046\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.2916e-07 - val_loss: 3.4397e-07\n",
      "\n",
      "Start of epoch 1047\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2853e-07 - val_loss: 3.4326e-07\n",
      "\n",
      "Start of epoch 1048\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2791e-07 - val_loss: 3.4264e-07\n",
      "\n",
      "Start of epoch 1049\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.2726e-07 - val_loss: 3.4189e-07\n",
      "\n",
      "Start of epoch 1050\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.2658e-07 - val_loss: 3.4120e-07\n",
      "\n",
      "Start of epoch 1051\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.2598e-07 - val_loss: 3.4051e-07\n",
      "\n",
      "Start of epoch 1052\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2536e-07 - val_loss: 3.3981e-07\n",
      "\n",
      "Start of epoch 1053\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2462e-07 - val_loss: 3.3912e-07\n",
      "\n",
      "Start of epoch 1054\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2404e-07 - val_loss: 3.3844e-07\n",
      "\n",
      "Start of epoch 1055\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.2336e-07 - val_loss: 3.3774e-07\n",
      "\n",
      "Start of epoch 1056\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.2274e-07 - val_loss: 3.3706e-07\n",
      "\n",
      "Start of epoch 1057\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.2212e-07 - val_loss: 3.3635e-07\n",
      "\n",
      "Start of epoch 1058\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.2145e-07 - val_loss: 3.3569e-07\n",
      "\n",
      "Start of epoch 1059\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.2088e-07 - val_loss: 3.3499e-07\n",
      "\n",
      "Start of epoch 1060\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.2015e-07 - val_loss: 3.3431e-07\n",
      "\n",
      "Start of epoch 1061\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.1955e-07 - val_loss: 3.3364e-07\n",
      "\n",
      "Start of epoch 1062\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.1896e-07 - val_loss: 3.3294e-07\n",
      "\n",
      "Start of epoch 1063\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.1834e-07 - val_loss: 3.3239e-07\n",
      "\n",
      "Start of epoch 1064\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.1762e-07 - val_loss: 3.3160e-07\n",
      "\n",
      "Start of epoch 1065\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.1702e-07 - val_loss: 3.3091e-07\n",
      "\n",
      "Start of epoch 1066\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.1642e-07 - val_loss: 3.3021e-07\n",
      "\n",
      "Start of epoch 1067\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 3.1579e-07 - val_loss: 3.2953e-07\n",
      "\n",
      "Start of epoch 1068\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.1516e-07 - val_loss: 3.2884e-07\n",
      "\n",
      "Start of epoch 1069\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.1452e-07 - val_loss: 3.2815e-07\n",
      "\n",
      "Start of epoch 1070\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.1384e-07 - val_loss: 3.2748e-07\n",
      "\n",
      "Start of epoch 1071\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.1322e-07 - val_loss: 3.2681e-07\n",
      "\n",
      "Start of epoch 1072\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.1251e-07 - val_loss: 3.2613e-07\n",
      "\n",
      "Start of epoch 1073\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.1193e-07 - val_loss: 3.2546e-07\n",
      "\n",
      "Start of epoch 1074\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.1130e-07 - val_loss: 3.2473e-07\n",
      "\n",
      "Start of epoch 1075\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.1066e-07 - val_loss: 3.2409e-07\n",
      "\n",
      "Start of epoch 1076\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.1001e-07 - val_loss: 3.2339e-07\n",
      "\n",
      "Start of epoch 1077\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0940e-07 - val_loss: 3.2273e-07\n",
      "\n",
      "Start of epoch 1078\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0872e-07 - val_loss: 3.2203e-07\n",
      "\n",
      "Start of epoch 1079\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.0810e-07 - val_loss: 3.2137e-07\n",
      "\n",
      "Start of epoch 1080\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0746e-07 - val_loss: 3.2065e-07\n",
      "\n",
      "Start of epoch 1081\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.0686e-07 - val_loss: 3.1999e-07\n",
      "\n",
      "Start of epoch 1082\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.0618e-07 - val_loss: 3.1929e-07\n",
      "\n",
      "Start of epoch 1083\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0557e-07 - val_loss: 3.1863e-07\n",
      "\n",
      "Start of epoch 1084\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.0491e-07 - val_loss: 3.1795e-07\n",
      "\n",
      "Start of epoch 1085\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 3.0426e-07 - val_loss: 3.1735e-07\n",
      "\n",
      "Start of epoch 1086\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0369e-07 - val_loss: 3.1658e-07\n",
      "\n",
      "Start of epoch 1087\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0305e-07 - val_loss: 3.1595e-07\n",
      "\n",
      "Start of epoch 1088\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0243e-07 - val_loss: 3.1530e-07\n",
      "\n",
      "Start of epoch 1089\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0173e-07 - val_loss: 3.1465e-07\n",
      "\n",
      "Start of epoch 1090\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0113e-07 - val_loss: 3.1399e-07\n",
      "\n",
      "Start of epoch 1091\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.0049e-07 - val_loss: 3.1328e-07\n",
      "\n",
      "Start of epoch 1092\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.9980e-07 - val_loss: 3.1265e-07\n",
      "\n",
      "Start of epoch 1093\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.9923e-07 - val_loss: 3.1206e-07\n",
      "\n",
      "Start of epoch 1094\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.9857e-07 - val_loss: 3.1133e-07\n",
      "\n",
      "Start of epoch 1095\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.9798e-07 - val_loss: 3.1066e-07\n",
      "\n",
      "Start of epoch 1096\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.9734e-07 - val_loss: 3.0995e-07\n",
      "\n",
      "Start of epoch 1097\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.9667e-07 - val_loss: 3.0928e-07\n",
      "\n",
      "Start of epoch 1098\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.9602e-07 - val_loss: 3.0863e-07\n",
      "\n",
      "Start of epoch 1099\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.9536e-07 - val_loss: 3.0797e-07\n",
      "\n",
      "Start of epoch 1100\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.9480e-07 - val_loss: 3.0736e-07\n",
      "\n",
      "Start of epoch 1101\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.9413e-07 - val_loss: 3.0661e-07\n",
      "\n",
      "Start of epoch 1102\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.9355e-07 - val_loss: 3.0593e-07\n",
      "\n",
      "Start of epoch 1103\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.9286e-07 - val_loss: 3.0524e-07\n",
      "\n",
      "Start of epoch 1104\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.9219e-07 - val_loss: 3.0458e-07\n",
      "\n",
      "Start of epoch 1105\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.9162e-07 - val_loss: 3.0392e-07\n",
      "\n",
      "Start of epoch 1106\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.9096e-07 - val_loss: 3.0324e-07\n",
      "\n",
      "Start of epoch 1107\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.9038e-07 - val_loss: 3.0257e-07\n",
      "\n",
      "Start of epoch 1108\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8969e-07 - val_loss: 3.0188e-07\n",
      "\n",
      "Start of epoch 1109\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.8909e-07 - val_loss: 3.0125e-07\n",
      "\n",
      "Start of epoch 1110\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.8846e-07 - val_loss: 3.0055e-07\n",
      "\n",
      "Start of epoch 1111\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.8780e-07 - val_loss: 2.9987e-07\n",
      "\n",
      "Start of epoch 1112\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.8728e-07 - val_loss: 2.9920e-07\n",
      "\n",
      "Start of epoch 1113\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.8654e-07 - val_loss: 2.9846e-07\n",
      "\n",
      "Start of epoch 1114\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.8595e-07 - val_loss: 2.9784e-07\n",
      "\n",
      "Start of epoch 1115\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.8531e-07 - val_loss: 2.9713e-07\n",
      "\n",
      "Start of epoch 1116\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8468e-07 - val_loss: 2.9648e-07\n",
      "\n",
      "Start of epoch 1117\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.8401e-07 - val_loss: 2.9581e-07\n",
      "\n",
      "Start of epoch 1118\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.8341e-07 - val_loss: 2.9514e-07\n",
      "\n",
      "Start of epoch 1119\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8278e-07 - val_loss: 2.9442e-07\n",
      "\n",
      "Start of epoch 1120\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8208e-07 - val_loss: 2.9378e-07\n",
      "\n",
      "Start of epoch 1121\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8154e-07 - val_loss: 2.9310e-07\n",
      "\n",
      "Start of epoch 1122\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.8088e-07 - val_loss: 2.9240e-07\n",
      "\n",
      "Start of epoch 1123\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.8027e-07 - val_loss: 2.9177e-07\n",
      "\n",
      "Start of epoch 1124\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.7959e-07 - val_loss: 2.9113e-07\n",
      "\n",
      "Start of epoch 1125\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.7909e-07 - val_loss: 2.9044e-07\n",
      "\n",
      "Start of epoch 1126\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7836e-07 - val_loss: 2.8978e-07\n",
      "\n",
      "Start of epoch 1127\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7776e-07 - val_loss: 2.8914e-07\n",
      "\n",
      "Start of epoch 1128\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.7715e-07 - val_loss: 2.8835e-07\n",
      "\n",
      "Start of epoch 1129\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.7648e-07 - val_loss: 2.8773e-07\n",
      "\n",
      "Start of epoch 1130\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7584e-07 - val_loss: 2.8705e-07\n",
      "\n",
      "Start of epoch 1131\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.7524e-07 - val_loss: 2.8641e-07\n",
      "\n",
      "Start of epoch 1132\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.7458e-07 - val_loss: 2.8572e-07\n",
      "\n",
      "Start of epoch 1133\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 2.7399e-07 - val_loss: 2.8508e-07\n",
      "\n",
      "Start of epoch 1134\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.7332e-07 - val_loss: 2.8438e-07\n",
      "\n",
      "Start of epoch 1135\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.7274e-07 - val_loss: 2.8374e-07\n",
      "\n",
      "Start of epoch 1136\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7207e-07 - val_loss: 2.8304e-07\n",
      "\n",
      "Start of epoch 1137\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.7148e-07 - val_loss: 2.8237e-07\n",
      "\n",
      "Start of epoch 1138\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.7081e-07 - val_loss: 2.8170e-07\n",
      "\n",
      "Start of epoch 1139\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7022e-07 - val_loss: 2.8104e-07\n",
      "\n",
      "Start of epoch 1140\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.6956e-07 - val_loss: 2.8037e-07\n",
      "\n",
      "Start of epoch 1141\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.6898e-07 - val_loss: 2.7971e-07\n",
      "\n",
      "Start of epoch 1142\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.6834e-07 - val_loss: 2.7911e-07\n",
      "\n",
      "Start of epoch 1143\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.6768e-07 - val_loss: 2.7843e-07\n",
      "\n",
      "Start of epoch 1144\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.6712e-07 - val_loss: 2.7773e-07\n",
      "\n",
      "Start of epoch 1145\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.6644e-07 - val_loss: 2.7706e-07\n",
      "\n",
      "Start of epoch 1146\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.6578e-07 - val_loss: 2.7641e-07\n",
      "\n",
      "Start of epoch 1147\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.6522e-07 - val_loss: 2.7572e-07\n",
      "\n",
      "Start of epoch 1148\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.6456e-07 - val_loss: 2.7506e-07\n",
      "\n",
      "Start of epoch 1149\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.6390e-07 - val_loss: 2.7442e-07\n",
      "\n",
      "Start of epoch 1150\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.6336e-07 - val_loss: 2.7373e-07\n",
      "\n",
      "Start of epoch 1151\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.6267e-07 - val_loss: 2.7307e-07\n",
      "\n",
      "Start of epoch 1152\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.6210e-07 - val_loss: 2.7239e-07\n",
      "\n",
      "Start of epoch 1153\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 2.6143e-07 - val_loss: 2.7175e-07\n",
      "\n",
      "Start of epoch 1154\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.6085e-07 - val_loss: 2.7108e-07\n",
      "\n",
      "Start of epoch 1155\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.6019e-07 - val_loss: 2.7043e-07\n",
      "\n",
      "Start of epoch 1156\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.5961e-07 - val_loss: 2.6976e-07\n",
      "\n",
      "Start of epoch 1157\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.5896e-07 - val_loss: 2.6914e-07\n",
      "\n",
      "Start of epoch 1158\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5829e-07 - val_loss: 2.6848e-07\n",
      "\n",
      "Start of epoch 1159\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.5773e-07 - val_loss: 2.6782e-07\n",
      "\n",
      "Start of epoch 1160\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5708e-07 - val_loss: 2.6716e-07\n",
      "\n",
      "Start of epoch 1161\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5649e-07 - val_loss: 2.6651e-07\n",
      "\n",
      "Start of epoch 1162\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5583e-07 - val_loss: 2.6585e-07\n",
      "\n",
      "Start of epoch 1163\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.5531e-07 - val_loss: 2.6519e-07\n",
      "\n",
      "Start of epoch 1164\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5466e-07 - val_loss: 2.6444e-07\n",
      "\n",
      "Start of epoch 1165\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5402e-07 - val_loss: 2.6388e-07\n",
      "\n",
      "Start of epoch 1166\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5334e-07 - val_loss: 2.6326e-07\n",
      "\n",
      "Start of epoch 1167\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5279e-07 - val_loss: 2.6257e-07\n",
      "\n",
      "Start of epoch 1168\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 2.5217e-07 - val_loss: 2.6183e-07\n",
      "\n",
      "Start of epoch 1169\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.5158e-07 - val_loss: 2.6121e-07\n",
      "\n",
      "Start of epoch 1170\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.5091e-07 - val_loss: 2.6057e-07\n",
      "\n",
      "Start of epoch 1171\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.5035e-07 - val_loss: 2.5991e-07\n",
      "\n",
      "Start of epoch 1172\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4968e-07 - val_loss: 2.5923e-07\n",
      "\n",
      "Start of epoch 1173\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4909e-07 - val_loss: 2.5863e-07\n",
      "\n",
      "Start of epoch 1174\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4841e-07 - val_loss: 2.5804e-07\n",
      "\n",
      "Start of epoch 1175\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.4785e-07 - val_loss: 2.5732e-07\n",
      "\n",
      "Start of epoch 1176\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4725e-07 - val_loss: 2.5662e-07\n",
      "\n",
      "Start of epoch 1177\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4665e-07 - val_loss: 2.5625e-07\n",
      "\n",
      "Start of epoch 1178\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.4606e-07 - val_loss: 2.5536e-07\n",
      "\n",
      "Start of epoch 1179\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4537e-07 - val_loss: 2.5473e-07\n",
      "\n",
      "Start of epoch 1180\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.4480e-07 - val_loss: 2.5410e-07\n",
      "\n",
      "Start of epoch 1181\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4416e-07 - val_loss: 2.5340e-07\n",
      "\n",
      "Start of epoch 1182\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.4356e-07 - val_loss: 2.5282e-07\n",
      "\n",
      "Start of epoch 1183\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.4296e-07 - val_loss: 2.5215e-07\n",
      "\n",
      "Start of epoch 1184\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.4229e-07 - val_loss: 2.5151e-07\n",
      "\n",
      "Start of epoch 1185\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.4179e-07 - val_loss: 2.5080e-07\n",
      "\n",
      "Start of epoch 1186\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.4110e-07 - val_loss: 2.5021e-07\n",
      "\n",
      "Start of epoch 1187\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4048e-07 - val_loss: 2.4958e-07\n",
      "\n",
      "Start of epoch 1188\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.3991e-07 - val_loss: 2.4889e-07\n",
      "\n",
      "Start of epoch 1189\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3926e-07 - val_loss: 2.4824e-07\n",
      "\n",
      "Start of epoch 1190\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3863e-07 - val_loss: 2.4761e-07\n",
      "\n",
      "Start of epoch 1191\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3802e-07 - val_loss: 2.4697e-07\n",
      "\n",
      "Start of epoch 1192\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.3743e-07 - val_loss: 2.4631e-07\n",
      "\n",
      "Start of epoch 1193\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.3684e-07 - val_loss: 2.4569e-07\n",
      "\n",
      "Start of epoch 1194\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3625e-07 - val_loss: 2.4506e-07\n",
      "\n",
      "Start of epoch 1195\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3563e-07 - val_loss: 2.4441e-07\n",
      "\n",
      "Start of epoch 1196\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.3508e-07 - val_loss: 2.4380e-07\n",
      "\n",
      "Start of epoch 1197\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.3442e-07 - val_loss: 2.4313e-07\n",
      "\n",
      "Start of epoch 1198\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3390e-07 - val_loss: 2.4247e-07\n",
      "\n",
      "Start of epoch 1199\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.3325e-07 - val_loss: 2.4182e-07\n",
      "\n",
      "Start of epoch 1200\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.3265e-07 - val_loss: 2.4127e-07\n",
      "\n",
      "Start of epoch 1201\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3202e-07 - val_loss: 2.4058e-07\n",
      "\n",
      "Start of epoch 1202\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3146e-07 - val_loss: 2.3997e-07\n",
      "\n",
      "Start of epoch 1203\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3083e-07 - val_loss: 2.3930e-07\n",
      "\n",
      "Start of epoch 1204\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.3024e-07 - val_loss: 2.3871e-07\n",
      "\n",
      "Start of epoch 1205\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2962e-07 - val_loss: 2.3805e-07\n",
      "\n",
      "Start of epoch 1206\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.2903e-07 - val_loss: 2.3749e-07\n",
      "\n",
      "Start of epoch 1207\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.2842e-07 - val_loss: 2.3679e-07\n",
      "\n",
      "Start of epoch 1208\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2784e-07 - val_loss: 2.3616e-07\n",
      "\n",
      "Start of epoch 1209\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2726e-07 - val_loss: 2.3565e-07\n",
      "\n",
      "Start of epoch 1210\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.2672e-07 - val_loss: 2.3496e-07\n",
      "\n",
      "Start of epoch 1211\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.2604e-07 - val_loss: 2.3427e-07\n",
      "\n",
      "Start of epoch 1212\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2547e-07 - val_loss: 2.3367e-07\n",
      "\n",
      "Start of epoch 1213\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.2482e-07 - val_loss: 2.3311e-07\n",
      "\n",
      "Start of epoch 1214\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2425e-07 - val_loss: 2.3239e-07\n",
      "\n",
      "Start of epoch 1215\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.2365e-07 - val_loss: 2.3173e-07\n",
      "\n",
      "Start of epoch 1216\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.2301e-07 - val_loss: 2.3114e-07\n",
      "\n",
      "Start of epoch 1217\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2247e-07 - val_loss: 2.3055e-07\n",
      "\n",
      "Start of epoch 1218\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2186e-07 - val_loss: 2.2990e-07\n",
      "\n",
      "Start of epoch 1219\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2134e-07 - val_loss: 2.2927e-07\n",
      "\n",
      "Start of epoch 1220\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2072e-07 - val_loss: 2.2873e-07\n",
      "\n",
      "Start of epoch 1221\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2014e-07 - val_loss: 2.2820e-07\n",
      "\n",
      "Start of epoch 1222\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1961e-07 - val_loss: 2.2756e-07\n",
      "\n",
      "Start of epoch 1223\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1896e-07 - val_loss: 2.2690e-07\n",
      "\n",
      "Start of epoch 1224\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1837e-07 - val_loss: 2.2631e-07\n",
      "\n",
      "Start of epoch 1225\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.1774e-07 - val_loss: 2.2573e-07\n",
      "\n",
      "Start of epoch 1226\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.1717e-07 - val_loss: 2.2506e-07\n",
      "\n",
      "Start of epoch 1227\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1658e-07 - val_loss: 2.2442e-07\n",
      "\n",
      "Start of epoch 1228\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1598e-07 - val_loss: 2.2381e-07\n",
      "\n",
      "Start of epoch 1229\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1540e-07 - val_loss: 2.2318e-07\n",
      "\n",
      "Start of epoch 1230\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1483e-07 - val_loss: 2.2255e-07\n",
      "\n",
      "Start of epoch 1231\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1423e-07 - val_loss: 2.2196e-07\n",
      "\n",
      "Start of epoch 1232\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1368e-07 - val_loss: 2.2137e-07\n",
      "\n",
      "Start of epoch 1233\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1306e-07 - val_loss: 2.2071e-07\n",
      "\n",
      "Start of epoch 1234\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.1255e-07 - val_loss: 2.2007e-07\n",
      "\n",
      "Start of epoch 1235\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.1198e-07 - val_loss: 2.1949e-07\n",
      "\n",
      "Start of epoch 1236\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1138e-07 - val_loss: 2.1893e-07\n",
      "\n",
      "Start of epoch 1237\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.1073e-07 - val_loss: 2.1830e-07\n",
      "\n",
      "Start of epoch 1238\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.1018e-07 - val_loss: 2.1767e-07\n",
      "\n",
      "Start of epoch 1239\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0958e-07 - val_loss: 2.1703e-07\n",
      "\n",
      "Start of epoch 1240\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0904e-07 - val_loss: 2.1647e-07\n",
      "\n",
      "Start of epoch 1241\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.0842e-07 - val_loss: 2.1584e-07\n",
      "\n",
      "Start of epoch 1242\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.0791e-07 - val_loss: 2.1520e-07\n",
      "\n",
      "Start of epoch 1243\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.0730e-07 - val_loss: 2.1460e-07\n",
      "\n",
      "Start of epoch 1244\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0672e-07 - val_loss: 2.1406e-07\n",
      "\n",
      "Start of epoch 1245\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0611e-07 - val_loss: 2.1343e-07\n",
      "\n",
      "Start of epoch 1246\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.0561e-07 - val_loss: 2.1280e-07\n",
      "\n",
      "Start of epoch 1247\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0501e-07 - val_loss: 2.1220e-07\n",
      "\n",
      "Start of epoch 1248\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0443e-07 - val_loss: 2.1162e-07\n",
      "\n",
      "Start of epoch 1249\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0384e-07 - val_loss: 2.1103e-07\n",
      "\n",
      "Start of epoch 1250\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.0326e-07 - val_loss: 2.1043e-07\n",
      "\n",
      "Start of epoch 1251\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0269e-07 - val_loss: 2.0985e-07\n",
      "\n",
      "Start of epoch 1252\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.0218e-07 - val_loss: 2.0921e-07\n",
      "\n",
      "Start of epoch 1253\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0156e-07 - val_loss: 2.0863e-07\n",
      "\n",
      "Start of epoch 1254\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0103e-07 - val_loss: 2.0804e-07\n",
      "\n",
      "Start of epoch 1255\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0044e-07 - val_loss: 2.0747e-07\n",
      "\n",
      "Start of epoch 1256\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.9984e-07 - val_loss: 2.0690e-07\n",
      "\n",
      "Start of epoch 1257\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.9935e-07 - val_loss: 2.0627e-07\n",
      "\n",
      "Start of epoch 1258\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.9875e-07 - val_loss: 2.0570e-07\n",
      "\n",
      "Start of epoch 1259\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.9816e-07 - val_loss: 2.0514e-07\n",
      "\n",
      "Start of epoch 1260\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.9771e-07 - val_loss: 2.0475e-07\n",
      "\n",
      "Start of epoch 1261\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.9715e-07 - val_loss: 2.0400e-07\n",
      "\n",
      "Start of epoch 1262\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.9649e-07 - val_loss: 2.0340e-07\n",
      "\n",
      "Start of epoch 1263\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.9599e-07 - val_loss: 2.0276e-07\n",
      "\n",
      "Start of epoch 1264\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.9540e-07 - val_loss: 2.0220e-07\n",
      "\n",
      "Start of epoch 1265\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.9481e-07 - val_loss: 2.0164e-07\n",
      "\n",
      "Start of epoch 1266\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.9432e-07 - val_loss: 2.0101e-07\n",
      "\n",
      "Start of epoch 1267\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.9373e-07 - val_loss: 2.0043e-07\n",
      "\n",
      "Start of epoch 1268\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.9317e-07 - val_loss: 1.9988e-07\n",
      "\n",
      "Start of epoch 1269\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.9261e-07 - val_loss: 1.9929e-07\n",
      "\n",
      "Start of epoch 1270\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.9211e-07 - val_loss: 1.9868e-07\n",
      "\n",
      "Start of epoch 1271\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.9153e-07 - val_loss: 1.9813e-07\n",
      "\n",
      "Start of epoch 1272\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.9096e-07 - val_loss: 1.9758e-07\n",
      "\n",
      "Start of epoch 1273\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.9047e-07 - val_loss: 1.9698e-07\n",
      "\n",
      "Start of epoch 1274\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.8988e-07 - val_loss: 1.9641e-07\n",
      "\n",
      "Start of epoch 1275\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.8932e-07 - val_loss: 1.9587e-07\n",
      "\n",
      "Start of epoch 1276\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8878e-07 - val_loss: 1.9528e-07\n",
      "\n",
      "Start of epoch 1277\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8822e-07 - val_loss: 1.9471e-07\n",
      "\n",
      "Start of epoch 1278\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.8774e-07 - val_loss: 1.9410e-07\n",
      "\n",
      "Start of epoch 1279\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8715e-07 - val_loss: 1.9355e-07\n",
      "\n",
      "Start of epoch 1280\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8662e-07 - val_loss: 1.9299e-07\n",
      "\n",
      "Start of epoch 1281\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.8609e-07 - val_loss: 1.9240e-07\n",
      "\n",
      "Start of epoch 1282\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.8550e-07 - val_loss: 1.9189e-07\n",
      "\n",
      "Start of epoch 1283\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.8504e-07 - val_loss: 1.9127e-07\n",
      "\n",
      "Start of epoch 1284\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8446e-07 - val_loss: 1.9074e-07\n",
      "\n",
      "Start of epoch 1285\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8390e-07 - val_loss: 1.9018e-07\n",
      "\n",
      "Start of epoch 1286\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8338e-07 - val_loss: 1.8962e-07\n",
      "\n",
      "Start of epoch 1287\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8282e-07 - val_loss: 1.8906e-07\n",
      "\n",
      "Start of epoch 1288\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.8232e-07 - val_loss: 1.8848e-07\n",
      "\n",
      "Start of epoch 1289\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.8178e-07 - val_loss: 1.8792e-07\n",
      "\n",
      "Start of epoch 1290\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8124e-07 - val_loss: 1.8738e-07\n",
      "\n",
      "Start of epoch 1291\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.8071e-07 - val_loss: 1.8684e-07\n",
      "\n",
      "Start of epoch 1292\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.8024e-07 - val_loss: 1.8643e-07\n",
      "\n",
      "Start of epoch 1293\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7969e-07 - val_loss: 1.8579e-07\n",
      "\n",
      "Start of epoch 1294\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7914e-07 - val_loss: 1.8518e-07\n",
      "\n",
      "Start of epoch 1295\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7860e-07 - val_loss: 1.8462e-07\n",
      "\n",
      "Start of epoch 1296\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7804e-07 - val_loss: 1.8407e-07\n",
      "\n",
      "Start of epoch 1297\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7752e-07 - val_loss: 1.8354e-07\n",
      "\n",
      "Start of epoch 1298\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7701e-07 - val_loss: 1.8300e-07\n",
      "\n",
      "Start of epoch 1299\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7654e-07 - val_loss: 1.8244e-07\n",
      "\n",
      "Start of epoch 1300\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.7598e-07 - val_loss: 1.8190e-07\n",
      "\n",
      "Start of epoch 1301\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.7545e-07 - val_loss: 1.8135e-07\n",
      "\n",
      "Start of epoch 1302\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.7492e-07 - val_loss: 1.8082e-07\n",
      "\n",
      "Start of epoch 1303\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.7441e-07 - val_loss: 1.8029e-07\n",
      "\n",
      "Start of epoch 1304\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.7389e-07 - val_loss: 1.7975e-07\n",
      "\n",
      "Start of epoch 1305\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7337e-07 - val_loss: 1.7922e-07\n",
      "\n",
      "Start of epoch 1306\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7286e-07 - val_loss: 1.7868e-07\n",
      "\n",
      "Start of epoch 1307\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7234e-07 - val_loss: 1.7815e-07\n",
      "\n",
      "Start of epoch 1308\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.7183e-07 - val_loss: 1.7761e-07\n",
      "\n",
      "Start of epoch 1309\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7131e-07 - val_loss: 1.7708e-07\n",
      "\n",
      "Start of epoch 1310\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.7080e-07 - val_loss: 1.7655e-07\n",
      "\n",
      "Start of epoch 1311\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7029e-07 - val_loss: 1.7602e-07\n",
      "\n",
      "Start of epoch 1312\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6978e-07 - val_loss: 1.7547e-07\n",
      "\n",
      "Start of epoch 1313\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.6925e-07 - val_loss: 1.7495e-07\n",
      "\n",
      "Start of epoch 1314\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6878e-07 - val_loss: 1.7441e-07\n",
      "\n",
      "Start of epoch 1315\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6826e-07 - val_loss: 1.7389e-07\n",
      "\n",
      "Start of epoch 1316\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6775e-07 - val_loss: 1.7338e-07\n",
      "\n",
      "Start of epoch 1317\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.6723e-07 - val_loss: 1.7286e-07\n",
      "\n",
      "Start of epoch 1318\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6676e-07 - val_loss: 1.7233e-07\n",
      "\n",
      "Start of epoch 1319\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6625e-07 - val_loss: 1.7186e-07\n",
      "\n",
      "Start of epoch 1320\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6574e-07 - val_loss: 1.7133e-07\n",
      "\n",
      "Start of epoch 1321\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6523e-07 - val_loss: 1.7080e-07\n",
      "\n",
      "Start of epoch 1322\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6474e-07 - val_loss: 1.7028e-07\n",
      "\n",
      "Start of epoch 1323\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6424e-07 - val_loss: 1.6979e-07\n",
      "\n",
      "Start of epoch 1324\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6373e-07 - val_loss: 1.6929e-07\n",
      "\n",
      "Start of epoch 1325\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.6325e-07 - val_loss: 1.6870e-07\n",
      "\n",
      "Start of epoch 1326\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6274e-07 - val_loss: 1.6820e-07\n",
      "\n",
      "Start of epoch 1327\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.6223e-07 - val_loss: 1.6770e-07\n",
      "\n",
      "Start of epoch 1328\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6177e-07 - val_loss: 1.6718e-07\n",
      "\n",
      "Start of epoch 1329\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.6128e-07 - val_loss: 1.6677e-07\n",
      "\n",
      "Start of epoch 1330\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6076e-07 - val_loss: 1.6619e-07\n",
      "\n",
      "Start of epoch 1331\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6027e-07 - val_loss: 1.6569e-07\n",
      "\n",
      "Start of epoch 1332\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.5978e-07 - val_loss: 1.6516e-07\n",
      "\n",
      "Start of epoch 1333\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5929e-07 - val_loss: 1.6467e-07\n",
      "\n",
      "Start of epoch 1334\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.5880e-07 - val_loss: 1.6415e-07\n",
      "\n",
      "Start of epoch 1335\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5832e-07 - val_loss: 1.6366e-07\n",
      "\n",
      "Start of epoch 1336\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5783e-07 - val_loss: 1.6316e-07\n",
      "\n",
      "Start of epoch 1337\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5737e-07 - val_loss: 1.6272e-07\n",
      "\n",
      "Start of epoch 1338\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.5688e-07 - val_loss: 1.6215e-07\n",
      "\n",
      "Start of epoch 1339\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5639e-07 - val_loss: 1.6166e-07\n",
      "\n",
      "Start of epoch 1340\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.5591e-07 - val_loss: 1.6115e-07\n",
      "\n",
      "Start of epoch 1341\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5543e-07 - val_loss: 1.6067e-07\n",
      "\n",
      "Start of epoch 1342\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5494e-07 - val_loss: 1.6016e-07\n",
      "\n",
      "Start of epoch 1343\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5446e-07 - val_loss: 1.5968e-07\n",
      "\n",
      "Start of epoch 1344\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5399e-07 - val_loss: 1.5919e-07\n",
      "\n",
      "Start of epoch 1345\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.5353e-07 - val_loss: 1.5871e-07\n",
      "\n",
      "Start of epoch 1346\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.5305e-07 - val_loss: 1.5821e-07\n",
      "\n",
      "Start of epoch 1347\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.5259e-07 - val_loss: 1.5771e-07\n",
      "\n",
      "Start of epoch 1348\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.5211e-07 - val_loss: 1.5723e-07\n",
      "\n",
      "Start of epoch 1349\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5164e-07 - val_loss: 1.5672e-07\n",
      "\n",
      "Start of epoch 1350\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.5118e-07 - val_loss: 1.5625e-07\n",
      "\n",
      "Start of epoch 1351\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5070e-07 - val_loss: 1.5579e-07\n",
      "\n",
      "Start of epoch 1352\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5022e-07 - val_loss: 1.5530e-07\n",
      "\n",
      "Start of epoch 1353\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4978e-07 - val_loss: 1.5497e-07\n",
      "\n",
      "Start of epoch 1354\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4932e-07 - val_loss: 1.5437e-07\n",
      "\n",
      "Start of epoch 1355\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4883e-07 - val_loss: 1.5382e-07\n",
      "\n",
      "Start of epoch 1356\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4838e-07 - val_loss: 1.5337e-07\n",
      "\n",
      "Start of epoch 1357\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4791e-07 - val_loss: 1.5287e-07\n",
      "\n",
      "Start of epoch 1358\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4746e-07 - val_loss: 1.5242e-07\n",
      "\n",
      "Start of epoch 1359\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4699e-07 - val_loss: 1.5190e-07\n",
      "\n",
      "Start of epoch 1360\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4652e-07 - val_loss: 1.5145e-07\n",
      "\n",
      "Start of epoch 1361\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4606e-07 - val_loss: 1.5101e-07\n",
      "\n",
      "Start of epoch 1362\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4561e-07 - val_loss: 1.5054e-07\n",
      "\n",
      "Start of epoch 1363\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4517e-07 - val_loss: 1.5006e-07\n",
      "\n",
      "Start of epoch 1364\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.4471e-07 - val_loss: 1.4959e-07\n",
      "\n",
      "Start of epoch 1365\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4424e-07 - val_loss: 1.4913e-07\n",
      "\n",
      "Start of epoch 1366\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4381e-07 - val_loss: 1.4867e-07\n",
      "\n",
      "Start of epoch 1367\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4335e-07 - val_loss: 1.4823e-07\n",
      "\n",
      "Start of epoch 1368\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.4290e-07 - val_loss: 1.4776e-07\n",
      "\n",
      "Start of epoch 1369\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4246e-07 - val_loss: 1.4731e-07\n",
      "\n",
      "Start of epoch 1370\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4201e-07 - val_loss: 1.4684e-07\n",
      "\n",
      "Start of epoch 1371\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4158e-07 - val_loss: 1.4635e-07\n",
      "\n",
      "Start of epoch 1372\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4115e-07 - val_loss: 1.4605e-07\n",
      "\n",
      "Start of epoch 1373\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4074e-07 - val_loss: 1.4548e-07\n",
      "\n",
      "Start of epoch 1374\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4024e-07 - val_loss: 1.4500e-07\n",
      "\n",
      "Start of epoch 1375\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3978e-07 - val_loss: 1.4454e-07\n",
      "\n",
      "Start of epoch 1376\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3934e-07 - val_loss: 1.4408e-07\n",
      "\n",
      "Start of epoch 1377\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3890e-07 - val_loss: 1.4363e-07\n",
      "\n",
      "Start of epoch 1378\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3846e-07 - val_loss: 1.4318e-07\n",
      "\n",
      "Start of epoch 1379\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3804e-07 - val_loss: 1.4274e-07\n",
      "\n",
      "Start of epoch 1380\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3760e-07 - val_loss: 1.4230e-07\n",
      "\n",
      "Start of epoch 1381\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3717e-07 - val_loss: 1.4185e-07\n",
      "\n",
      "Start of epoch 1382\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3676e-07 - val_loss: 1.4141e-07\n",
      "\n",
      "Start of epoch 1383\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3631e-07 - val_loss: 1.4094e-07\n",
      "\n",
      "Start of epoch 1384\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3589e-07 - val_loss: 1.4053e-07\n",
      "\n",
      "Start of epoch 1385\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3544e-07 - val_loss: 1.4010e-07\n",
      "\n",
      "Start of epoch 1386\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3503e-07 - val_loss: 1.3965e-07\n",
      "\n",
      "Start of epoch 1387\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3461e-07 - val_loss: 1.3918e-07\n",
      "\n",
      "Start of epoch 1388\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3420e-07 - val_loss: 1.3878e-07\n",
      "\n",
      "Start of epoch 1389\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3376e-07 - val_loss: 1.3830e-07\n",
      "\n",
      "Start of epoch 1390\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3333e-07 - val_loss: 1.3788e-07\n",
      "\n",
      "Start of epoch 1391\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3293e-07 - val_loss: 1.3747e-07\n",
      "\n",
      "Start of epoch 1392\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3249e-07 - val_loss: 1.3702e-07\n",
      "\n",
      "Start of epoch 1393\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3209e-07 - val_loss: 1.3661e-07\n",
      "\n",
      "Start of epoch 1394\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3166e-07 - val_loss: 1.3616e-07\n",
      "\n",
      "Start of epoch 1395\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3124e-07 - val_loss: 1.3572e-07\n",
      "\n",
      "Start of epoch 1396\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3083e-07 - val_loss: 1.3533e-07\n",
      "\n",
      "Start of epoch 1397\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3042e-07 - val_loss: 1.3486e-07\n",
      "\n",
      "Start of epoch 1398\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3008e-07 - val_loss: 1.3452e-07\n",
      "\n",
      "Start of epoch 1399\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2969e-07 - val_loss: 1.3408e-07\n",
      "\n",
      "Start of epoch 1400\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2921e-07 - val_loss: 1.3362e-07\n",
      "\n",
      "Start of epoch 1401\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2876e-07 - val_loss: 1.3325e-07\n",
      "\n",
      "Start of epoch 1402\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2838e-07 - val_loss: 1.3277e-07\n",
      "\n",
      "Start of epoch 1403\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2793e-07 - val_loss: 1.3238e-07\n",
      "\n",
      "Start of epoch 1404\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2751e-07 - val_loss: 1.3194e-07\n",
      "\n",
      "Start of epoch 1405\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.2712e-07 - val_loss: 1.3154e-07\n",
      "\n",
      "Start of epoch 1406\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2672e-07 - val_loss: 1.3113e-07\n",
      "\n",
      "Start of epoch 1407\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.2630e-07 - val_loss: 1.3070e-07\n",
      "\n",
      "Start of epoch 1408\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2592e-07 - val_loss: 1.3029e-07\n",
      "\n",
      "Start of epoch 1409\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2551e-07 - val_loss: 1.2989e-07\n",
      "\n",
      "Start of epoch 1410\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2511e-07 - val_loss: 1.2948e-07\n",
      "\n",
      "Start of epoch 1411\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2472e-07 - val_loss: 1.2906e-07\n",
      "\n",
      "Start of epoch 1412\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2432e-07 - val_loss: 1.2869e-07\n",
      "\n",
      "Start of epoch 1413\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2397e-07 - val_loss: 1.2825e-07\n",
      "\n",
      "Start of epoch 1414\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2354e-07 - val_loss: 1.2786e-07\n",
      "\n",
      "Start of epoch 1415\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2314e-07 - val_loss: 1.2744e-07\n",
      "\n",
      "Start of epoch 1416\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2275e-07 - val_loss: 1.2706e-07\n",
      "\n",
      "Start of epoch 1417\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2238e-07 - val_loss: 1.2664e-07\n",
      "\n",
      "Start of epoch 1418\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2199e-07 - val_loss: 1.2625e-07\n",
      "\n",
      "Start of epoch 1419\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.2158e-07 - val_loss: 1.2559e-07\n",
      "\n",
      "Start of epoch 1420\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.2117e-07 - val_loss: 1.2539e-07\n",
      "\n",
      "Start of epoch 1421\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2080e-07 - val_loss: 1.2476e-07\n",
      "\n",
      "Start of epoch 1422\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2034e-07 - val_loss: 1.2463e-07\n",
      "\n",
      "Start of epoch 1423\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2004e-07 - val_loss: 1.2397e-07\n",
      "\n",
      "Start of epoch 1424\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1961e-07 - val_loss: 1.2383e-07\n",
      "\n",
      "Start of epoch 1425\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1929e-07 - val_loss: 1.2323e-07\n",
      "\n",
      "Start of epoch 1426\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1886e-07 - val_loss: 1.2311e-07\n",
      "\n",
      "Start of epoch 1427\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1858e-07 - val_loss: 1.2244e-07\n",
      "\n",
      "Start of epoch 1428\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1809e-07 - val_loss: 1.2203e-07\n",
      "\n",
      "Start of epoch 1429\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1773e-07 - val_loss: 1.2192e-07\n",
      "\n",
      "Start of epoch 1430\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1741e-07 - val_loss: 1.2134e-07\n",
      "\n",
      "Start of epoch 1431\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.1702e-07 - val_loss: 1.2117e-07\n",
      "\n",
      "Start of epoch 1432\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1667e-07 - val_loss: 1.2059e-07\n",
      "\n",
      "Start of epoch 1433\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1628e-07 - val_loss: 1.2042e-07\n",
      "\n",
      "Start of epoch 1434\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1597e-07 - val_loss: 1.2005e-07\n",
      "\n",
      "Start of epoch 1435\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1559e-07 - val_loss: 1.1939e-07\n",
      "\n",
      "Start of epoch 1436\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1512e-07 - val_loss: 1.1902e-07\n",
      "\n",
      "Start of epoch 1437\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1478e-07 - val_loss: 1.1860e-07\n",
      "\n",
      "Start of epoch 1438\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1439e-07 - val_loss: 1.1828e-07\n",
      "\n",
      "Start of epoch 1439\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1406e-07 - val_loss: 1.1787e-07\n",
      "\n",
      "Start of epoch 1440\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1367e-07 - val_loss: 1.1755e-07\n",
      "\n",
      "Start of epoch 1441\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1335e-07 - val_loss: 1.1714e-07\n",
      "\n",
      "Start of epoch 1442\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.1295e-07 - val_loss: 1.1682e-07\n",
      "\n",
      "Start of epoch 1443\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1263e-07 - val_loss: 1.1641e-07\n",
      "\n",
      "Start of epoch 1444\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1224e-07 - val_loss: 1.1610e-07\n",
      "\n",
      "Start of epoch 1445\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1192e-07 - val_loss: 1.1569e-07\n",
      "\n",
      "Start of epoch 1446\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1153e-07 - val_loss: 1.1537e-07\n",
      "\n",
      "Start of epoch 1447\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1122e-07 - val_loss: 1.1497e-07\n",
      "\n",
      "Start of epoch 1448\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1083e-07 - val_loss: 1.1465e-07\n",
      "\n",
      "Start of epoch 1449\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1052e-07 - val_loss: 1.1425e-07\n",
      "\n",
      "Start of epoch 1450\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1013e-07 - val_loss: 1.1394e-07\n",
      "\n",
      "Start of epoch 1451\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0982e-07 - val_loss: 1.1354e-07\n",
      "\n",
      "Start of epoch 1452\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0944e-07 - val_loss: 1.1322e-07\n",
      "\n",
      "Start of epoch 1453\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0912e-07 - val_loss: 1.1287e-07\n",
      "\n",
      "Start of epoch 1454\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.0890e-07 - val_loss: 1.1301e-07\n",
      "\n",
      "Start of epoch 1455\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0860e-07 - val_loss: 1.1224e-07\n",
      "\n",
      "Start of epoch 1456\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0815e-07 - val_loss: 1.1186e-07\n",
      "\n",
      "Start of epoch 1457\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0777e-07 - val_loss: 1.1150e-07\n",
      "\n",
      "Start of epoch 1458\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0744e-07 - val_loss: 1.1114e-07\n",
      "\n",
      "Start of epoch 1459\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0709e-07 - val_loss: 1.1081e-07\n",
      "\n",
      "Start of epoch 1460\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0676e-07 - val_loss: 1.1045e-07\n",
      "\n",
      "Start of epoch 1461\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.0642e-07 - val_loss: 1.1012e-07\n",
      "\n",
      "Start of epoch 1462\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0609e-07 - val_loss: 1.0976e-07\n",
      "\n",
      "Start of epoch 1463\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0575e-07 - val_loss: 1.0944e-07\n",
      "\n",
      "Start of epoch 1464\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0542e-07 - val_loss: 1.0909e-07\n",
      "\n",
      "Start of epoch 1465\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0509e-07 - val_loss: 1.0876e-07\n",
      "\n",
      "Start of epoch 1466\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.0477e-07 - val_loss: 1.0844e-07\n",
      "\n",
      "Start of epoch 1467\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0445e-07 - val_loss: 1.0811e-07\n",
      "\n",
      "Start of epoch 1468\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.0412e-07 - val_loss: 1.0773e-07\n",
      "\n",
      "Start of epoch 1469\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0376e-07 - val_loss: 1.0738e-07\n",
      "\n",
      "Start of epoch 1470\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.0343e-07 - val_loss: 1.0704e-07\n",
      "\n",
      "Start of epoch 1471\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0310e-07 - val_loss: 1.0676e-07\n",
      "\n",
      "Start of epoch 1472\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.0281e-07 - val_loss: 1.0638e-07\n",
      "\n",
      "Start of epoch 1473\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0246e-07 - val_loss: 1.0610e-07\n",
      "\n",
      "Start of epoch 1474\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0217e-07 - val_loss: 1.0573e-07\n",
      "\n",
      "Start of epoch 1475\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0182e-07 - val_loss: 1.0545e-07\n",
      "\n",
      "Start of epoch 1476\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.0153e-07 - val_loss: 1.0508e-07\n",
      "\n",
      "Start of epoch 1477\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.0119e-07 - val_loss: 1.0481e-07\n",
      "\n",
      "Start of epoch 1478\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0091e-07 - val_loss: 1.0448e-07\n",
      "\n",
      "Start of epoch 1479\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0059e-07 - val_loss: 1.0415e-07\n",
      "\n",
      "Start of epoch 1480\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0028e-07 - val_loss: 1.0383e-07\n",
      "\n",
      "Start of epoch 1481\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.9964e-08 - val_loss: 1.0352e-07\n",
      "\n",
      "Start of epoch 1482\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.9656e-08 - val_loss: 1.0320e-07\n",
      "\n",
      "Start of epoch 1483\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.9350e-08 - val_loss: 1.0289e-07\n",
      "\n",
      "Start of epoch 1484\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.9043e-08 - val_loss: 1.0256e-07\n",
      "\n",
      "Start of epoch 1485\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.8729e-08 - val_loss: 1.0225e-07\n",
      "\n",
      "Start of epoch 1486\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.8423e-08 - val_loss: 1.0194e-07\n",
      "\n",
      "Start of epoch 1487\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 9.8120e-08 - val_loss: 1.0164e-07\n",
      "\n",
      "Start of epoch 1488\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.7821e-08 - val_loss: 1.0137e-07\n",
      "\n",
      "Start of epoch 1489\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.7545e-08 - val_loss: 1.0105e-07\n",
      "\n",
      "Start of epoch 1490\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.7240e-08 - val_loss: 1.0073e-07\n",
      "\n",
      "Start of epoch 1491\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.6938e-08 - val_loss: 1.0043e-07\n",
      "\n",
      "Start of epoch 1492\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.6641e-08 - val_loss: 1.0013e-07\n",
      "\n",
      "Start of epoch 1493\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.6347e-08 - val_loss: 9.9833e-08\n",
      "\n",
      "Start of epoch 1494\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.6054e-08 - val_loss: 9.9520e-08\n",
      "\n",
      "Start of epoch 1495\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.5754e-08 - val_loss: 9.9221e-08\n",
      "\n",
      "Start of epoch 1496\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.5460e-08 - val_loss: 9.8926e-08\n",
      "\n",
      "Start of epoch 1497\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.5171e-08 - val_loss: 9.8632e-08\n",
      "\n",
      "Start of epoch 1498\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.4883e-08 - val_loss: 9.8342e-08\n",
      "\n",
      "Start of epoch 1499\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.4695e-08 - val_loss: 9.8158e-08\n",
      "\n",
      "Start of epoch 1500\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 9.4369e-08 - val_loss: 9.7772e-08\n",
      "\n",
      "Start of epoch 1501\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.4038e-08 - val_loss: 9.7485e-08\n",
      "\n",
      "Start of epoch 1502\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.3760e-08 - val_loss: 9.7181e-08\n",
      "\n",
      "Start of epoch 1503\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.3465e-08 - val_loss: 9.6894e-08\n",
      "\n",
      "Start of epoch 1504\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.3186e-08 - val_loss: 9.6609e-08\n",
      "\n",
      "Start of epoch 1505\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.2902e-08 - val_loss: 9.6326e-08\n",
      "\n",
      "Start of epoch 1506\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.2630e-08 - val_loss: 9.6080e-08\n",
      "\n",
      "Start of epoch 1507\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.2371e-08 - val_loss: 9.5798e-08\n",
      "\n",
      "Start of epoch 1508\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.2099e-08 - val_loss: 9.5485e-08\n",
      "\n",
      "Start of epoch 1509\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.1801e-08 - val_loss: 9.5201e-08\n",
      "\n",
      "Start of epoch 1510\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.1532e-08 - val_loss: 9.4964e-08\n",
      "\n",
      "Start of epoch 1511\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.1278e-08 - val_loss: 9.4686e-08\n",
      "\n",
      "Start of epoch 1512\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.1011e-08 - val_loss: 9.4377e-08\n",
      "\n",
      "Start of epoch 1513\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.0715e-08 - val_loss: 9.4075e-08\n",
      "\n",
      "Start of epoch 1514\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.0432e-08 - val_loss: 9.3841e-08\n",
      "\n",
      "Start of epoch 1515\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.0191e-08 - val_loss: 9.3557e-08\n",
      "\n",
      "Start of epoch 1516\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.9915e-08 - val_loss: 9.3263e-08\n",
      "\n",
      "Start of epoch 1517\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.9639e-08 - val_loss: 9.3034e-08\n",
      "\n",
      "Start of epoch 1518\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.9401e-08 - val_loss: 9.2754e-08\n",
      "\n",
      "Start of epoch 1519\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.9129e-08 - val_loss: 9.2493e-08\n",
      "\n",
      "Start of epoch 1520\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.8870e-08 - val_loss: 9.2229e-08\n",
      "\n",
      "Start of epoch 1521\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.8619e-08 - val_loss: 9.1960e-08\n",
      "\n",
      "Start of epoch 1522\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.8352e-08 - val_loss: 9.1705e-08\n",
      "\n",
      "Start of epoch 1523\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.8098e-08 - val_loss: 9.1444e-08\n",
      "\n",
      "Start of epoch 1524\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.7851e-08 - val_loss: 9.1178e-08\n",
      "\n",
      "Start of epoch 1525\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.7587e-08 - val_loss: 9.0926e-08\n",
      "\n",
      "Start of epoch 1526\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.7337e-08 - val_loss: 9.0671e-08\n",
      "\n",
      "Start of epoch 1527\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.7114e-08 - val_loss: 9.0419e-08\n",
      "\n",
      "Start of epoch 1528\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.6873e-08 - val_loss: 9.0180e-08\n",
      "\n",
      "Start of epoch 1529\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.6624e-08 - val_loss: 8.9944e-08\n",
      "\n",
      "Start of epoch 1530\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.6350e-08 - val_loss: 8.9658e-08\n",
      "\n",
      "Start of epoch 1531\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.6095e-08 - val_loss: 8.9403e-08\n",
      "\n",
      "Start of epoch 1532\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.5855e-08 - val_loss: 8.9143e-08\n",
      "\n",
      "Start of epoch 1533\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.5599e-08 - val_loss: 8.8875e-08\n",
      "\n",
      "Start of epoch 1534\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.5351e-08 - val_loss: 8.8650e-08\n",
      "\n",
      "Start of epoch 1535\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.5114e-08 - val_loss: 8.8413e-08\n",
      "\n",
      "Start of epoch 1536\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.4881e-08 - val_loss: 8.8167e-08\n",
      "\n",
      "Start of epoch 1537\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.4645e-08 - val_loss: 8.7924e-08\n",
      "\n",
      "Start of epoch 1538\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.4405e-08 - val_loss: 8.7680e-08\n",
      "\n",
      "Start of epoch 1539\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.4170e-08 - val_loss: 8.7440e-08\n",
      "\n",
      "Start of epoch 1540\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.3934e-08 - val_loss: 8.7198e-08\n",
      "\n",
      "Start of epoch 1541\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.3701e-08 - val_loss: 8.6961e-08\n",
      "\n",
      "Start of epoch 1542\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.3464e-08 - val_loss: 8.6729e-08\n",
      "\n",
      "Start of epoch 1543\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.3236e-08 - val_loss: 8.6495e-08\n",
      "\n",
      "Start of epoch 1544\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.3014e-08 - val_loss: 8.6248e-08\n",
      "\n",
      "Start of epoch 1545\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.2774e-08 - val_loss: 8.6021e-08\n",
      "\n",
      "Start of epoch 1546\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.2548e-08 - val_loss: 8.5787e-08\n",
      "\n",
      "Start of epoch 1547\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.2323e-08 - val_loss: 8.5556e-08\n",
      "\n",
      "Start of epoch 1548\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.2093e-08 - val_loss: 8.5330e-08\n",
      "\n",
      "Start of epoch 1549\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.1870e-08 - val_loss: 8.5101e-08\n",
      "\n",
      "Start of epoch 1550\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.1687e-08 - val_loss: 8.5124e-08\n",
      "\n",
      "Start of epoch 1551\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.1483e-08 - val_loss: 8.4694e-08\n",
      "\n",
      "Start of epoch 1552\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.1217e-08 - val_loss: 8.4427e-08\n",
      "\n",
      "Start of epoch 1553\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.0986e-08 - val_loss: 8.4197e-08\n",
      "\n",
      "Start of epoch 1554\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.0766e-08 - val_loss: 8.3976e-08\n",
      "\n",
      "Start of epoch 1555\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.0551e-08 - val_loss: 8.3753e-08\n",
      "\n",
      "Start of epoch 1556\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.0333e-08 - val_loss: 8.3534e-08\n",
      "\n",
      "Start of epoch 1557\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.0121e-08 - val_loss: 8.3305e-08\n",
      "\n",
      "Start of epoch 1558\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.9904e-08 - val_loss: 8.3104e-08\n",
      "\n",
      "Start of epoch 1559\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 7.9699e-08 - val_loss: 8.2886e-08\n",
      "\n",
      "Start of epoch 1560\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.9489e-08 - val_loss: 8.2668e-08\n",
      "\n",
      "Start of epoch 1561\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.9277e-08 - val_loss: 8.2452e-08\n",
      "\n",
      "Start of epoch 1562\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.9068e-08 - val_loss: 8.2241e-08\n",
      "\n",
      "Start of epoch 1563\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.8863e-08 - val_loss: 8.2029e-08\n",
      "\n",
      "Start of epoch 1564\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 7.8655e-08 - val_loss: 8.1821e-08\n",
      "\n",
      "Start of epoch 1565\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 7.8449e-08 - val_loss: 8.1606e-08\n",
      "\n",
      "Start of epoch 1566\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.8243e-08 - val_loss: 8.1398e-08\n",
      "\n",
      "Start of epoch 1567\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.8044e-08 - val_loss: 8.1194e-08\n",
      "\n",
      "Start of epoch 1568\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.7843e-08 - val_loss: 8.0985e-08\n",
      "\n",
      "Start of epoch 1569\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.7641e-08 - val_loss: 8.0783e-08\n",
      "\n",
      "Start of epoch 1570\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 7.7442e-08 - val_loss: 8.0580e-08\n",
      "\n",
      "Start of epoch 1571\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.7245e-08 - val_loss: 8.0378e-08\n",
      "\n",
      "Start of epoch 1572\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.7048e-08 - val_loss: 8.0177e-08\n",
      "\n",
      "Start of epoch 1573\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.6927e-08 - val_loss: 8.0084e-08\n",
      "\n",
      "Start of epoch 1574\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.6764e-08 - val_loss: 7.9798e-08\n",
      "\n",
      "Start of epoch 1575\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.6484e-08 - val_loss: 7.9581e-08\n",
      "\n",
      "Start of epoch 1576\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 7.6272e-08 - val_loss: 7.9384e-08\n",
      "\n",
      "Start of epoch 1577\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.6079e-08 - val_loss: 7.9189e-08\n",
      "\n",
      "Start of epoch 1578\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.5889e-08 - val_loss: 7.8996e-08\n",
      "\n",
      "Start of epoch 1579\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.5700e-08 - val_loss: 7.8803e-08\n",
      "\n",
      "Start of epoch 1580\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.5512e-08 - val_loss: 7.8611e-08\n",
      "\n",
      "Start of epoch 1581\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.5325e-08 - val_loss: 7.8420e-08\n",
      "\n",
      "Start of epoch 1582\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.5139e-08 - val_loss: 7.8230e-08\n",
      "\n",
      "Start of epoch 1583\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.4954e-08 - val_loss: 7.8042e-08\n",
      "\n",
      "Start of epoch 1584\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.4770e-08 - val_loss: 7.7854e-08\n",
      "\n",
      "Start of epoch 1585\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.4588e-08 - val_loss: 7.7672e-08\n",
      "\n",
      "Start of epoch 1586\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.4408e-08 - val_loss: 7.7484e-08\n",
      "\n",
      "Start of epoch 1587\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 7.4228e-08 - val_loss: 7.7301e-08\n",
      "\n",
      "Start of epoch 1588\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.4048e-08 - val_loss: 7.7114e-08\n",
      "\n",
      "Start of epoch 1589\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.3866e-08 - val_loss: 7.6934e-08\n",
      "\n",
      "Start of epoch 1590\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.3694e-08 - val_loss: 7.6758e-08\n",
      "\n",
      "Start of epoch 1591\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.3519e-08 - val_loss: 7.6575e-08\n",
      "\n",
      "Start of epoch 1592\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.3344e-08 - val_loss: 7.6397e-08\n",
      "\n",
      "Start of epoch 1593\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.3170e-08 - val_loss: 7.6220e-08\n",
      "\n",
      "Start of epoch 1594\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.2993e-08 - val_loss: 7.6042e-08\n",
      "\n",
      "Start of epoch 1595\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.2827e-08 - val_loss: 7.5871e-08\n",
      "\n",
      "Start of epoch 1596\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.2656e-08 - val_loss: 7.5691e-08\n",
      "\n",
      "Start of epoch 1597\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 7.2482e-08 - val_loss: 7.5521e-08\n",
      "\n",
      "Start of epoch 1598\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 7.2322e-08 - val_loss: 7.5347e-08\n",
      "\n",
      "Start of epoch 1599\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.2149e-08 - val_loss: 7.5177e-08\n",
      "\n",
      "Start of epoch 1600\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.1988e-08 - val_loss: 7.5007e-08\n",
      "\n",
      "Start of epoch 1601\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.1818e-08 - val_loss: 7.4836e-08\n",
      "\n",
      "Start of epoch 1602\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 7.1656e-08 - val_loss: 7.4671e-08\n",
      "\n",
      "Start of epoch 1603\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 7.1493e-08 - val_loss: 7.4498e-08\n",
      "\n",
      "Start of epoch 1604\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 7.1329e-08 - val_loss: 7.4338e-08\n",
      "\n",
      "Start of epoch 1605\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 7.1174e-08 - val_loss: 7.4174e-08\n",
      "\n",
      "Start of epoch 1606\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.1010e-08 - val_loss: 7.4007e-08\n",
      "\n",
      "Start of epoch 1607\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 7.0852e-08 - val_loss: 7.3846e-08\n",
      "\n",
      "Start of epoch 1608\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.0696e-08 - val_loss: 7.3684e-08\n",
      "\n",
      "Start of epoch 1609\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.0537e-08 - val_loss: 7.3531e-08\n",
      "\n",
      "Start of epoch 1610\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 7.0475e-08 - val_loss: 7.3485e-08\n",
      "\n",
      "Start of epoch 1611\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.0270e-08 - val_loss: 7.3204e-08\n",
      "\n",
      "Start of epoch 1612\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 7.0075e-08 - val_loss: 7.3046e-08\n",
      "\n",
      "Start of epoch 1613\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.9921e-08 - val_loss: 7.2883e-08\n",
      "\n",
      "Start of epoch 1614\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.9767e-08 - val_loss: 7.2732e-08\n",
      "\n",
      "Start of epoch 1615\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.9623e-08 - val_loss: 7.2578e-08\n",
      "\n",
      "Start of epoch 1616\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.9470e-08 - val_loss: 7.2423e-08\n",
      "\n",
      "Start of epoch 1617\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.9319e-08 - val_loss: 7.2265e-08\n",
      "\n",
      "Start of epoch 1618\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.9171e-08 - val_loss: 7.2117e-08\n",
      "\n",
      "Start of epoch 1619\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.9031e-08 - val_loss: 7.1966e-08\n",
      "\n",
      "Start of epoch 1620\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.8882e-08 - val_loss: 7.1816e-08\n",
      "\n",
      "Start of epoch 1621\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.8734e-08 - val_loss: 7.1669e-08\n",
      "\n",
      "Start of epoch 1622\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.8593e-08 - val_loss: 7.1521e-08\n",
      "\n",
      "Start of epoch 1623\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.8451e-08 - val_loss: 7.1369e-08\n",
      "\n",
      "Start of epoch 1624\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.8307e-08 - val_loss: 7.1225e-08\n",
      "\n",
      "Start of epoch 1625\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.8172e-08 - val_loss: 7.1083e-08\n",
      "\n",
      "Start of epoch 1626\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.8030e-08 - val_loss: 7.0939e-08\n",
      "\n",
      "Start of epoch 1627\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.7888e-08 - val_loss: 7.0796e-08\n",
      "\n",
      "Start of epoch 1628\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.7752e-08 - val_loss: 7.0653e-08\n",
      "\n",
      "Start of epoch 1629\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.7615e-08 - val_loss: 7.0512e-08\n",
      "\n",
      "Start of epoch 1630\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.7479e-08 - val_loss: 7.0371e-08\n",
      "\n",
      "Start of epoch 1631\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.7348e-08 - val_loss: 7.0230e-08\n",
      "\n",
      "Start of epoch 1632\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.7210e-08 - val_loss: 7.0094e-08\n",
      "\n",
      "Start of epoch 1633\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.7078e-08 - val_loss: 6.9954e-08\n",
      "\n",
      "Start of epoch 1634\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.6943e-08 - val_loss: 6.9817e-08\n",
      "\n",
      "Start of epoch 1635\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.6816e-08 - val_loss: 6.9689e-08\n",
      "\n",
      "Start of epoch 1636\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.6687e-08 - val_loss: 6.9545e-08\n",
      "\n",
      "Start of epoch 1637\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.6623e-08 - val_loss: 6.9531e-08\n",
      "\n",
      "Start of epoch 1638\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.6516e-08 - val_loss: 6.9346e-08\n",
      "\n",
      "Start of epoch 1639\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.6325e-08 - val_loss: 6.9153e-08\n",
      "\n",
      "Start of epoch 1640\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.6172e-08 - val_loss: 6.9019e-08\n",
      "\n",
      "Start of epoch 1641\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.6050e-08 - val_loss: 6.8889e-08\n",
      "\n",
      "Start of epoch 1642\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 6.5924e-08 - val_loss: 6.8758e-08\n",
      "\n",
      "Start of epoch 1643\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.5797e-08 - val_loss: 6.8632e-08\n",
      "\n",
      "Start of epoch 1644\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.5674e-08 - val_loss: 6.8500e-08\n",
      "\n",
      "Start of epoch 1645\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.5550e-08 - val_loss: 6.8375e-08\n",
      "\n",
      "Start of epoch 1646\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.5430e-08 - val_loss: 6.8250e-08\n",
      "\n",
      "Start of epoch 1647\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.5310e-08 - val_loss: 6.8129e-08\n",
      "\n",
      "Start of epoch 1648\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.5192e-08 - val_loss: 6.7998e-08\n",
      "\n",
      "Start of epoch 1649\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.5071e-08 - val_loss: 6.7877e-08\n",
      "\n",
      "Start of epoch 1650\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.4953e-08 - val_loss: 6.7751e-08\n",
      "\n",
      "Start of epoch 1651\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.4840e-08 - val_loss: 6.7636e-08\n",
      "\n",
      "Start of epoch 1652\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 6.4725e-08 - val_loss: 6.7508e-08\n",
      "\n",
      "Start of epoch 1653\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.4602e-08 - val_loss: 6.7394e-08\n",
      "\n",
      "Start of epoch 1654\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.4491e-08 - val_loss: 6.7276e-08\n",
      "\n",
      "Start of epoch 1655\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.4379e-08 - val_loss: 6.7147e-08\n",
      "\n",
      "Start of epoch 1656\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.4265e-08 - val_loss: 6.7034e-08\n",
      "\n",
      "Start of epoch 1657\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.4153e-08 - val_loss: 6.6927e-08\n",
      "\n",
      "Start of epoch 1658\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.4043e-08 - val_loss: 6.6797e-08\n",
      "\n",
      "Start of epoch 1659\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.3933e-08 - val_loss: 6.6688e-08\n",
      "\n",
      "Start of epoch 1660\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.3824e-08 - val_loss: 6.6573e-08\n",
      "\n",
      "Start of epoch 1661\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.3713e-08 - val_loss: 6.6457e-08\n",
      "\n",
      "Start of epoch 1662\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 6.3609e-08 - val_loss: 6.6353e-08\n",
      "\n",
      "Start of epoch 1663\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.3502e-08 - val_loss: 6.6236e-08\n",
      "\n",
      "Start of epoch 1664\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.3390e-08 - val_loss: 6.6121e-08\n",
      "\n",
      "Start of epoch 1665\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.3287e-08 - val_loss: 6.6018e-08\n",
      "\n",
      "Start of epoch 1666\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.3186e-08 - val_loss: 6.5903e-08\n",
      "\n",
      "Start of epoch 1667\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.3078e-08 - val_loss: 6.5792e-08\n",
      "\n",
      "Start of epoch 1668\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.2980e-08 - val_loss: 6.5692e-08\n",
      "\n",
      "Start of epoch 1669\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.2877e-08 - val_loss: 6.5587e-08\n",
      "\n",
      "Start of epoch 1670\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.2772e-08 - val_loss: 6.5471e-08\n",
      "\n",
      "Start of epoch 1671\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.2674e-08 - val_loss: 6.5375e-08\n",
      "\n",
      "Start of epoch 1672\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.2603e-08 - val_loss: 6.5339e-08\n",
      "\n",
      "Start of epoch 1673\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.2505e-08 - val_loss: 6.5180e-08\n",
      "\n",
      "Start of epoch 1674\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.2399e-08 - val_loss: 6.5126e-08\n",
      "\n",
      "Start of epoch 1675\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.2290e-08 - val_loss: 6.4964e-08\n",
      "\n",
      "Start of epoch 1676\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.2184e-08 - val_loss: 6.4855e-08\n",
      "\n",
      "Start of epoch 1677\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.2086e-08 - val_loss: 6.4756e-08\n",
      "\n",
      "Start of epoch 1678\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.1991e-08 - val_loss: 6.4653e-08\n",
      "\n",
      "Start of epoch 1679\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.1896e-08 - val_loss: 6.4553e-08\n",
      "\n",
      "Start of epoch 1680\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.1802e-08 - val_loss: 6.4453e-08\n",
      "\n",
      "Start of epoch 1681\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.1710e-08 - val_loss: 6.4360e-08\n",
      "\n",
      "Start of epoch 1682\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.1614e-08 - val_loss: 6.4251e-08\n",
      "\n",
      "Start of epoch 1683\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.1525e-08 - val_loss: 6.4161e-08\n",
      "\n",
      "Start of epoch 1684\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.1434e-08 - val_loss: 6.4070e-08\n",
      "\n",
      "Start of epoch 1685\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.1342e-08 - val_loss: 6.4092e-08\n",
      "\n",
      "Start of epoch 1686\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 6.1272e-08 - val_loss: 6.4074e-08\n",
      "\n",
      "Start of epoch 1687\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 6.1173e-08 - val_loss: 6.4026e-08\n",
      "\n",
      "Start of epoch 1688\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.1080e-08 - val_loss: 6.3947e-08\n",
      "\n",
      "Start of epoch 1689\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.0972e-08 - val_loss: 6.3862e-08\n",
      "\n",
      "Start of epoch 1690\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.0883e-08 - val_loss: 6.3786e-08\n",
      "\n",
      "Start of epoch 1691\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.0821e-08 - val_loss: 6.3746e-08\n",
      "\n",
      "Start of epoch 1692\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.0752e-08 - val_loss: 6.3661e-08\n",
      "\n",
      "Start of epoch 1693\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.0698e-08 - val_loss: 6.3711e-08\n",
      "\n",
      "Start of epoch 1694\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.0633e-08 - val_loss: 6.3487e-08\n",
      "\n",
      "Start of epoch 1695\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 6.0485e-08 - val_loss: 6.3372e-08\n",
      "\n",
      "Start of epoch 1696\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.0364e-08 - val_loss: 6.3282e-08\n",
      "\n",
      "Start of epoch 1697\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.0275e-08 - val_loss: 6.3195e-08\n",
      "\n",
      "Start of epoch 1698\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 6.0191e-08 - val_loss: 6.3105e-08\n",
      "\n",
      "Start of epoch 1699\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 6.0109e-08 - val_loss: 6.3017e-08\n",
      "\n",
      "Start of epoch 1700\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 6.0027e-08 - val_loss: 6.2931e-08\n",
      "\n",
      "Start of epoch 1701\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9989e-08 - val_loss: 6.3156e-08\n",
      "\n",
      "Start of epoch 1702\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9963e-08 - val_loss: 6.3126e-08\n",
      "\n",
      "Start of epoch 1703\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.9896e-08 - val_loss: 6.3024e-08\n",
      "\n",
      "Start of epoch 1704\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.9812e-08 - val_loss: 6.2931e-08\n",
      "\n",
      "Start of epoch 1705\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9729e-08 - val_loss: 6.2837e-08\n",
      "\n",
      "Start of epoch 1706\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9647e-08 - val_loss: 6.2743e-08\n",
      "\n",
      "Start of epoch 1707\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9565e-08 - val_loss: 6.2655e-08\n",
      "\n",
      "Start of epoch 1708\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9486e-08 - val_loss: 6.2565e-08\n",
      "\n",
      "Start of epoch 1709\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.9408e-08 - val_loss: 6.2475e-08\n",
      "\n",
      "Start of epoch 1710\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9328e-08 - val_loss: 6.2390e-08\n",
      "\n",
      "Start of epoch 1711\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9252e-08 - val_loss: 6.2304e-08\n",
      "\n",
      "Start of epoch 1712\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9177e-08 - val_loss: 6.2220e-08\n",
      "\n",
      "Start of epoch 1713\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9101e-08 - val_loss: 6.2134e-08\n",
      "\n",
      "Start of epoch 1714\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.9025e-08 - val_loss: 6.2054e-08\n",
      "\n",
      "Start of epoch 1715\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8951e-08 - val_loss: 6.1974e-08\n",
      "\n",
      "Start of epoch 1716\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8880e-08 - val_loss: 6.1893e-08\n",
      "\n",
      "Start of epoch 1717\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8808e-08 - val_loss: 6.1813e-08\n",
      "\n",
      "Start of epoch 1718\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.8737e-08 - val_loss: 6.1736e-08\n",
      "\n",
      "Start of epoch 1719\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.8677e-08 - val_loss: 6.1659e-08\n",
      "\n",
      "Start of epoch 1720\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8641e-08 - val_loss: 6.1643e-08\n",
      "\n",
      "Start of epoch 1721\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.8573e-08 - val_loss: 6.1514e-08\n",
      "\n",
      "Start of epoch 1722\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.8466e-08 - val_loss: 6.1433e-08\n",
      "\n",
      "Start of epoch 1723\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8390e-08 - val_loss: 6.1358e-08\n",
      "\n",
      "Start of epoch 1724\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8323e-08 - val_loss: 6.1284e-08\n",
      "\n",
      "Start of epoch 1725\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8256e-08 - val_loss: 6.1212e-08\n",
      "\n",
      "Start of epoch 1726\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8190e-08 - val_loss: 6.1141e-08\n",
      "\n",
      "Start of epoch 1727\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.8125e-08 - val_loss: 6.1070e-08\n",
      "\n",
      "Start of epoch 1728\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.8061e-08 - val_loss: 6.1001e-08\n",
      "\n",
      "Start of epoch 1729\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.7997e-08 - val_loss: 6.0932e-08\n",
      "\n",
      "Start of epoch 1730\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7933e-08 - val_loss: 6.0862e-08\n",
      "\n",
      "Start of epoch 1731\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7869e-08 - val_loss: 6.0799e-08\n",
      "\n",
      "Start of epoch 1732\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.7808e-08 - val_loss: 6.0727e-08\n",
      "\n",
      "Start of epoch 1733\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7745e-08 - val_loss: 6.0664e-08\n",
      "\n",
      "Start of epoch 1734\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7685e-08 - val_loss: 6.0595e-08\n",
      "\n",
      "Start of epoch 1735\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7624e-08 - val_loss: 6.0533e-08\n",
      "\n",
      "Start of epoch 1736\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7564e-08 - val_loss: 6.0465e-08\n",
      "\n",
      "Start of epoch 1737\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7505e-08 - val_loss: 6.0407e-08\n",
      "\n",
      "Start of epoch 1738\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7447e-08 - val_loss: 6.0339e-08\n",
      "\n",
      "Start of epoch 1739\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7388e-08 - val_loss: 6.0280e-08\n",
      "\n",
      "Start of epoch 1740\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7331e-08 - val_loss: 6.0213e-08\n",
      "\n",
      "Start of epoch 1741\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.7272e-08 - val_loss: 6.0157e-08\n",
      "\n",
      "Start of epoch 1742\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.7217e-08 - val_loss: 6.0091e-08\n",
      "\n",
      "Start of epoch 1743\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7159e-08 - val_loss: 6.0035e-08\n",
      "\n",
      "Start of epoch 1744\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.7104e-08 - val_loss: 5.9973e-08\n",
      "\n",
      "Start of epoch 1745\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.7146e-08 - val_loss: 6.0070e-08\n",
      "\n",
      "Start of epoch 1746\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.7038e-08 - val_loss: 5.9882e-08\n",
      "\n",
      "Start of epoch 1747\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.6946e-08 - val_loss: 5.9800e-08\n",
      "\n",
      "Start of epoch 1748\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.6886e-08 - val_loss: 5.9693e-08\n",
      "\n",
      "Start of epoch 1749\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.6736e-08 - val_loss: 5.9023e-08\n",
      "\n",
      "Start of epoch 1750\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.6204e-08 - val_loss: 5.8911e-08\n",
      "\n",
      "Start of epoch 1751\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.6161e-08 - val_loss: 5.8983e-08\n",
      "\n",
      "Start of epoch 1752\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.6105e-08 - val_loss: 5.8928e-08\n",
      "\n",
      "Start of epoch 1753\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.6038e-08 - val_loss: 5.8862e-08\n",
      "\n",
      "Start of epoch 1754\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5958e-08 - val_loss: 5.8767e-08\n",
      "\n",
      "Start of epoch 1755\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5877e-08 - val_loss: 5.8708e-08\n",
      "\n",
      "Start of epoch 1756\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5794e-08 - val_loss: 5.8644e-08\n",
      "\n",
      "Start of epoch 1757\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.5746e-08 - val_loss: 5.8576e-08\n",
      "\n",
      "Start of epoch 1758\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5675e-08 - val_loss: 5.8508e-08\n",
      "\n",
      "Start of epoch 1759\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5614e-08 - val_loss: 5.8449e-08\n",
      "\n",
      "Start of epoch 1760\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5539e-08 - val_loss: 5.8404e-08\n",
      "\n",
      "Start of epoch 1761\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5487e-08 - val_loss: 5.8340e-08\n",
      "\n",
      "Start of epoch 1762\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 5.5439e-08 - val_loss: 5.8285e-08\n",
      "\n",
      "Start of epoch 1763\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5368e-08 - val_loss: 5.8241e-08\n",
      "\n",
      "Start of epoch 1764\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.5310e-08 - val_loss: 5.8188e-08\n",
      "\n",
      "Start of epoch 1765\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5280e-08 - val_loss: 5.8105e-08\n",
      "\n",
      "Start of epoch 1766\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5236e-08 - val_loss: 5.8074e-08\n",
      "\n",
      "Start of epoch 1767\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5175e-08 - val_loss: 5.8024e-08\n",
      "\n",
      "Start of epoch 1768\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5094e-08 - val_loss: 5.7965e-08\n",
      "\n",
      "Start of epoch 1769\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.5033e-08 - val_loss: 5.7916e-08\n",
      "\n",
      "Start of epoch 1770\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.5002e-08 - val_loss: 5.7870e-08\n",
      "\n",
      "Start of epoch 1771\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.4947e-08 - val_loss: 5.7814e-08\n",
      "\n",
      "Start of epoch 1772\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.4912e-08 - val_loss: 5.7776e-08\n",
      "\n",
      "Start of epoch 1773\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.4842e-08 - val_loss: 5.7711e-08\n",
      "\n",
      "Start of epoch 1774\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.4819e-08 - val_loss: 5.7687e-08\n",
      "\n",
      "Start of epoch 1775\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.4757e-08 - val_loss: 5.7616e-08\n",
      "\n",
      "Start of epoch 1776\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.4768e-08 - val_loss: 5.7577e-08\n",
      "\n",
      "Start of epoch 1777\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.4724e-08 - val_loss: 5.7562e-08\n",
      "\n",
      "Start of epoch 1778\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.4615e-08 - val_loss: 5.7478e-08\n",
      "\n",
      "Start of epoch 1779\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.4577e-08 - val_loss: 5.7416e-08\n",
      "\n",
      "Start of epoch 1780\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.4540e-08 - val_loss: 5.7400e-08\n",
      "\n",
      "Start of epoch 1781\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.4469e-08 - val_loss: 5.7340e-08\n",
      "\n",
      "Start of epoch 1782\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.4420e-08 - val_loss: 5.7298e-08\n",
      "\n",
      "Start of epoch 1783\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.4396e-08 - val_loss: 5.7238e-08\n",
      "\n",
      "Start of epoch 1784\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.4358e-08 - val_loss: 5.7225e-08\n",
      "\n",
      "Start of epoch 1785\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.4289e-08 - val_loss: 5.7165e-08\n",
      "\n",
      "Start of epoch 1786\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.4242e-08 - val_loss: 5.7120e-08\n",
      "\n",
      "Start of epoch 1787\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.4198e-08 - val_loss: 5.7080e-08\n",
      "\n",
      "Start of epoch 1788\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.4155e-08 - val_loss: 5.7040e-08\n",
      "\n",
      "Start of epoch 1789\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.4123e-08 - val_loss: 5.7008e-08\n",
      "\n",
      "Start of epoch 1790\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.4077e-08 - val_loss: 5.6979e-08\n",
      "\n",
      "Start of epoch 1791\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.4037e-08 - val_loss: 5.6938e-08\n",
      "\n",
      "Start of epoch 1792\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.3996e-08 - val_loss: 5.6896e-08\n",
      "\n",
      "Start of epoch 1793\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.3953e-08 - val_loss: 5.6854e-08\n",
      "\n",
      "Start of epoch 1794\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.3910e-08 - val_loss: 5.6812e-08\n",
      "\n",
      "Start of epoch 1795\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3868e-08 - val_loss: 5.6770e-08\n",
      "\n",
      "Start of epoch 1796\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.3826e-08 - val_loss: 5.6730e-08\n",
      "\n",
      "Start of epoch 1797\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3785e-08 - val_loss: 5.6690e-08\n",
      "\n",
      "Start of epoch 1798\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3744e-08 - val_loss: 5.6650e-08\n",
      "\n",
      "Start of epoch 1799\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3704e-08 - val_loss: 5.6613e-08\n",
      "\n",
      "Start of epoch 1800\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3711e-08 - val_loss: 5.6852e-08\n",
      "\n",
      "Start of epoch 1801\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3709e-08 - val_loss: 5.6599e-08\n",
      "\n",
      "Start of epoch 1802\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3599e-08 - val_loss: 5.6485e-08\n",
      "\n",
      "Start of epoch 1803\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.3584e-08 - val_loss: 5.6445e-08\n",
      "\n",
      "Start of epoch 1804\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3524e-08 - val_loss: 5.6419e-08\n",
      "\n",
      "Start of epoch 1805\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.3475e-08 - val_loss: 5.6387e-08\n",
      "\n",
      "Start of epoch 1806\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3430e-08 - val_loss: 5.6311e-08\n",
      "\n",
      "Start of epoch 1807\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.3411e-08 - val_loss: 5.6279e-08\n",
      "\n",
      "Start of epoch 1808\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3389e-08 - val_loss: 5.6251e-08\n",
      "\n",
      "Start of epoch 1809\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3329e-08 - val_loss: 5.6232e-08\n",
      "\n",
      "Start of epoch 1810\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3284e-08 - val_loss: 5.6163e-08\n",
      "\n",
      "Start of epoch 1811\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.3252e-08 - val_loss: 5.6155e-08\n",
      "\n",
      "Start of epoch 1812\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3210e-08 - val_loss: 5.6091e-08\n",
      "\n",
      "Start of epoch 1813\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.3188e-08 - val_loss: 5.6057e-08\n",
      "\n",
      "Start of epoch 1814\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3146e-08 - val_loss: 5.6049e-08\n",
      "\n",
      "Start of epoch 1815\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.3101e-08 - val_loss: 5.5983e-08\n",
      "\n",
      "Start of epoch 1816\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.3069e-08 - val_loss: 5.5976e-08\n",
      "\n",
      "Start of epoch 1817\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.3030e-08 - val_loss: 5.5912e-08\n",
      "\n",
      "Start of epoch 1818\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2998e-08 - val_loss: 5.5905e-08\n",
      "\n",
      "Start of epoch 1819\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2958e-08 - val_loss: 5.5841e-08\n",
      "\n",
      "Start of epoch 1820\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2926e-08 - val_loss: 5.5835e-08\n",
      "\n",
      "Start of epoch 1821\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2888e-08 - val_loss: 5.5771e-08\n",
      "\n",
      "Start of epoch 1822\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2856e-08 - val_loss: 5.5765e-08\n",
      "\n",
      "Start of epoch 1823\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.2819e-08 - val_loss: 5.5702e-08\n",
      "\n",
      "Start of epoch 1824\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.2793e-08 - val_loss: 5.5665e-08\n",
      "\n",
      "Start of epoch 1825\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2766e-08 - val_loss: 5.5669e-08\n",
      "\n",
      "Start of epoch 1826\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.2723e-08 - val_loss: 5.5605e-08\n",
      "\n",
      "Start of epoch 1827\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2687e-08 - val_loss: 5.5564e-08\n",
      "\n",
      "Start of epoch 1828\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.2657e-08 - val_loss: 5.5531e-08\n",
      "\n",
      "Start of epoch 1829\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.2623e-08 - val_loss: 5.5511e-08\n",
      "\n",
      "Start of epoch 1830\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.2577e-08 - val_loss: 5.5480e-08\n",
      "\n",
      "Start of epoch 1831\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 5.2530e-08 - val_loss: 5.5492e-08\n",
      "\n",
      "Start of epoch 1832\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.2442e-08 - val_loss: 5.5328e-08\n",
      "\n",
      "Start of epoch 1833\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.2463e-08 - val_loss: 5.5499e-08\n",
      "\n",
      "Start of epoch 1834\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.2392e-08 - val_loss: 5.5240e-08\n",
      "\n",
      "Start of epoch 1835\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 5.2294e-08 - val_loss: 5.5139e-08\n",
      "\n",
      "Start of epoch 1836\n",
      "1984/1984 [==============================] - 0s 42us/step - train_loss: 5.2173e-08 - val_loss: 5.4993e-08\n",
      "\n",
      "Start of epoch 1837\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.2104e-08 - val_loss: 5.4912e-08\n",
      "\n",
      "Start of epoch 1838\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.1999e-08 - val_loss: 5.4814e-08\n",
      "\n",
      "Start of epoch 1839\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.1941e-08 - val_loss: 5.4732e-08\n",
      "\n",
      "Start of epoch 1840\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.1832e-08 - val_loss: 5.4629e-08\n",
      "\n",
      "Start of epoch 1841\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.1772e-08 - val_loss: 5.4544e-08\n",
      "\n",
      "Start of epoch 1842\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.1659e-08 - val_loss: 5.4436e-08\n",
      "\n",
      "Start of epoch 1843\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.1595e-08 - val_loss: 5.4347e-08\n",
      "\n",
      "Start of epoch 1844\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.1476e-08 - val_loss: 5.4231e-08\n",
      "\n",
      "Start of epoch 1845\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.1413e-08 - val_loss: 5.4148e-08\n",
      "\n",
      "Start of epoch 1846\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.1291e-08 - val_loss: 5.4025e-08\n",
      "\n",
      "Start of epoch 1847\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 5.1197e-08 - val_loss: 5.3920e-08\n",
      "\n",
      "Start of epoch 1848\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.1100e-08 - val_loss: 5.3820e-08\n",
      "\n",
      "Start of epoch 1849\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.1035e-08 - val_loss: 5.3733e-08\n",
      "\n",
      "Start of epoch 1850\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.0903e-08 - val_loss: 5.3601e-08\n",
      "\n",
      "Start of epoch 1851\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.0803e-08 - val_loss: 5.3489e-08\n",
      "\n",
      "Start of epoch 1852\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.0700e-08 - val_loss: 5.3380e-08\n",
      "\n",
      "Start of epoch 1853\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 5.0598e-08 - val_loss: 5.3269e-08\n",
      "\n",
      "Start of epoch 1854\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.0494e-08 - val_loss: 5.3157e-08\n",
      "\n",
      "Start of epoch 1855\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.0388e-08 - val_loss: 5.3042e-08\n",
      "\n",
      "Start of epoch 1856\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.0281e-08 - val_loss: 5.2927e-08\n",
      "\n",
      "Start of epoch 1857\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 5.0173e-08 - val_loss: 5.2810e-08\n",
      "\n",
      "Start of epoch 1858\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 5.0063e-08 - val_loss: 5.2692e-08\n",
      "\n",
      "Start of epoch 1859\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.9952e-08 - val_loss: 5.2573e-08\n",
      "\n",
      "Start of epoch 1860\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.9841e-08 - val_loss: 5.2454e-08\n",
      "\n",
      "Start of epoch 1861\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.9728e-08 - val_loss: 5.2333e-08\n",
      "\n",
      "Start of epoch 1862\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.9565e-08 - val_loss: 5.2291e-08\n",
      "\n",
      "Start of epoch 1863\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.9395e-08 - val_loss: 5.2061e-08\n",
      "\n",
      "Start of epoch 1864\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.9287e-08 - val_loss: 5.2028e-08\n",
      "\n",
      "Start of epoch 1865\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.9212e-08 - val_loss: 5.2098e-08\n",
      "\n",
      "Start of epoch 1866\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.9150e-08 - val_loss: 5.1822e-08\n",
      "\n",
      "Start of epoch 1867\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.8910e-08 - val_loss: 5.1556e-08\n",
      "\n",
      "Start of epoch 1868\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.8823e-08 - val_loss: 5.1529e-08\n",
      "\n",
      "Start of epoch 1869\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.8652e-08 - val_loss: 5.1289e-08\n",
      "\n",
      "Start of epoch 1870\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.8552e-08 - val_loss: 5.1246e-08\n",
      "\n",
      "Start of epoch 1871\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.8423e-08 - val_loss: 5.1053e-08\n",
      "\n",
      "Start of epoch 1872\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.8325e-08 - val_loss: 5.0977e-08\n",
      "\n",
      "Start of epoch 1873\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 4.8141e-08 - val_loss: 5.0734e-08\n",
      "\n",
      "Start of epoch 1874\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.8068e-08 - val_loss: 5.0700e-08\n",
      "\n",
      "Start of epoch 1875\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.7877e-08 - val_loss: 5.0466e-08\n",
      "\n",
      "Start of epoch 1876\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.7751e-08 - val_loss: 5.0375e-08\n",
      "\n",
      "Start of epoch 1877\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.7682e-08 - val_loss: 5.0283e-08\n",
      "\n",
      "Start of epoch 1878\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.7479e-08 - val_loss: 5.0044e-08\n",
      "\n",
      "Start of epoch 1879\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.7414e-08 - val_loss: 5.0001e-08\n",
      "\n",
      "Start of epoch 1880\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.7209e-08 - val_loss: 4.9769e-08\n",
      "\n",
      "Start of epoch 1881\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.7147e-08 - val_loss: 4.9716e-08\n",
      "\n",
      "Start of epoch 1882\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.6936e-08 - val_loss: 4.9487e-08\n",
      "\n",
      "Start of epoch 1883\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.6876e-08 - val_loss: 4.9427e-08\n",
      "\n",
      "Start of epoch 1884\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.6655e-08 - val_loss: 4.9218e-08\n",
      "\n",
      "Start of epoch 1885\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.6497e-08 - val_loss: 4.9047e-08\n",
      "\n",
      "Start of epoch 1886\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.6352e-08 - val_loss: 4.8895e-08\n",
      "\n",
      "Start of epoch 1887\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.6209e-08 - val_loss: 4.8745e-08\n",
      "\n",
      "Start of epoch 1888\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.6073e-08 - val_loss: 4.8600e-08\n",
      "\n",
      "Start of epoch 1889\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.5938e-08 - val_loss: 4.8390e-08\n",
      "\n",
      "Start of epoch 1890\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.5837e-08 - val_loss: 4.8321e-08\n",
      "\n",
      "Start of epoch 1891\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.5662e-08 - val_loss: 4.8156e-08\n",
      "\n",
      "Start of epoch 1892\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.5509e-08 - val_loss: 4.7930e-08\n",
      "\n",
      "Start of epoch 1893\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.5409e-08 - val_loss: 4.7856e-08\n",
      "\n",
      "Start of epoch 1894\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.5229e-08 - val_loss: 4.7699e-08\n",
      "\n",
      "Start of epoch 1895\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.5077e-08 - val_loss: 4.7460e-08\n",
      "\n",
      "Start of epoch 1896\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.5093e-08 - val_loss: 4.7910e-08\n",
      "\n",
      "Start of epoch 1897\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.4964e-08 - val_loss: 4.7280e-08\n",
      "\n",
      "Start of epoch 1898\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.4678e-08 - val_loss: 4.7014e-08\n",
      "\n",
      "Start of epoch 1899\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.4496e-08 - val_loss: 4.6917e-08\n",
      "\n",
      "Start of epoch 1900\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.4338e-08 - val_loss: 4.6666e-08\n",
      "\n",
      "Start of epoch 1901\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 4.4194e-08 - val_loss: 4.6592e-08\n",
      "\n",
      "Start of epoch 1902\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.4047e-08 - val_loss: 4.6351e-08\n",
      "\n",
      "Start of epoch 1903\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.3899e-08 - val_loss: 4.6272e-08\n",
      "\n",
      "Start of epoch 1904\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.3737e-08 - val_loss: 4.6018e-08\n",
      "\n",
      "Start of epoch 1905\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.3594e-08 - val_loss: 4.5949e-08\n",
      "\n",
      "Start of epoch 1906\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.3446e-08 - val_loss: 4.5702e-08\n",
      "\n",
      "Start of epoch 1907\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.3295e-08 - val_loss: 4.5623e-08\n",
      "\n",
      "Start of epoch 1908\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.3132e-08 - val_loss: 4.5364e-08\n",
      "\n",
      "Start of epoch 1909\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2987e-08 - val_loss: 4.5298e-08\n",
      "\n",
      "Start of epoch 1910\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2839e-08 - val_loss: 4.5048e-08\n",
      "\n",
      "Start of epoch 1911\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2686e-08 - val_loss: 4.4970e-08\n",
      "\n",
      "Start of epoch 1912\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2523e-08 - val_loss: 4.4703e-08\n",
      "\n",
      "Start of epoch 1913\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2375e-08 - val_loss: 4.4642e-08\n",
      "\n",
      "Start of epoch 1914\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2218e-08 - val_loss: 4.4371e-08\n",
      "\n",
      "Start of epoch 1915\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.2071e-08 - val_loss: 4.4307e-08\n",
      "\n",
      "Start of epoch 1916\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.1921e-08 - val_loss: 4.4052e-08\n",
      "\n",
      "Start of epoch 1917\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 4.1768e-08 - val_loss: 4.3975e-08\n",
      "\n",
      "Start of epoch 1918\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 4.1601e-08 - val_loss: 4.3704e-08\n",
      "\n",
      "Start of epoch 1919\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 4.1454e-08 - val_loss: 4.3644e-08\n",
      "\n",
      "Start of epoch 1920\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 4.1294e-08 - val_loss: 4.3369e-08\n",
      "\n",
      "Start of epoch 1921\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 4.1147e-08 - val_loss: 4.3307e-08\n",
      "\n",
      "Start of epoch 1922\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 4.0996e-08 - val_loss: 4.3049e-08\n",
      "\n",
      "Start of epoch 1923\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.0842e-08 - val_loss: 4.2971e-08\n",
      "\n",
      "Start of epoch 1924\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0674e-08 - val_loss: 4.2696e-08\n",
      "\n",
      "Start of epoch 1925\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 4.0528e-08 - val_loss: 4.2639e-08\n",
      "\n",
      "Start of epoch 1926\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0366e-08 - val_loss: 4.2359e-08\n",
      "\n",
      "Start of epoch 1927\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0221e-08 - val_loss: 4.2300e-08\n",
      "\n",
      "Start of epoch 1928\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 4.0069e-08 - val_loss: 4.2042e-08\n",
      "\n",
      "Start of epoch 1929\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9915e-08 - val_loss: 4.1961e-08\n",
      "\n",
      "Start of epoch 1930\n",
      "1984/1984 [==============================] - 0s 43us/step - train_loss: 3.9746e-08 - val_loss: 4.1685e-08\n",
      "\n",
      "Start of epoch 1931\n",
      "1984/1984 [==============================] - 0s 45us/step - train_loss: 3.9601e-08 - val_loss: 4.1629e-08\n",
      "\n",
      "Start of epoch 1932\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.9439e-08 - val_loss: 4.1347e-08\n",
      "\n",
      "Start of epoch 1933\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.9294e-08 - val_loss: 4.1288e-08\n",
      "\n",
      "Start of epoch 1934\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 3.9143e-08 - val_loss: 4.1032e-08\n",
      "\n",
      "Start of epoch 1935\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 3.8989e-08 - val_loss: 4.0947e-08\n",
      "\n",
      "Start of epoch 1936\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 3.8820e-08 - val_loss: 4.0672e-08\n",
      "\n",
      "Start of epoch 1937\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 3.8677e-08 - val_loss: 4.0616e-08\n",
      "\n",
      "Start of epoch 1938\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 3.8515e-08 - val_loss: 4.0388e-08\n",
      "\n",
      "Start of epoch 1939\n",
      "1984/1984 [==============================] - 0s 46us/step - train_loss: 3.8352e-08 - val_loss: 4.0236e-08\n",
      "\n",
      "Start of epoch 1940\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 3.8202e-08 - val_loss: 4.0070e-08\n",
      "\n",
      "Start of epoch 1941\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 3.8052e-08 - val_loss: 3.9914e-08\n",
      "\n",
      "Start of epoch 1942\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.7911e-08 - val_loss: 3.9736e-08\n",
      "\n",
      "Start of epoch 1943\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.7808e-08 - val_loss: 3.9562e-08\n",
      "\n",
      "Start of epoch 1944\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.7649e-08 - val_loss: 3.9456e-08\n",
      "\n",
      "Start of epoch 1945\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.7448e-08 - val_loss: 3.9167e-08\n",
      "\n",
      "Start of epoch 1946\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.7300e-08 - val_loss: 3.9027e-08\n",
      "\n",
      "Start of epoch 1947\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.7150e-08 - val_loss: 3.8902e-08\n",
      "\n",
      "Start of epoch 1948\n",
      "1984/1984 [==============================] - 0s 40us/step - train_loss: 3.6984e-08 - val_loss: 3.8722e-08\n",
      "\n",
      "Start of epoch 1949\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 3.6835e-08 - val_loss: 3.8567e-08\n",
      "\n",
      "Start of epoch 1950\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 3.6699e-08 - val_loss: 3.8383e-08\n",
      "\n",
      "Start of epoch 1951\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 3.6554e-08 - val_loss: 3.8266e-08\n",
      "\n",
      "Start of epoch 1952\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 3.6387e-08 - val_loss: 3.8003e-08\n",
      "\n",
      "Start of epoch 1953\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.6247e-08 - val_loss: 3.7867e-08\n",
      "\n",
      "Start of epoch 1954\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.6095e-08 - val_loss: 3.7727e-08\n",
      "\n",
      "Start of epoch 1955\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.5938e-08 - val_loss: 3.7558e-08\n",
      "\n",
      "Start of epoch 1956\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.5807e-08 - val_loss: 3.7441e-08\n",
      "\n",
      "Start of epoch 1957\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.5645e-08 - val_loss: 3.7188e-08\n",
      "\n",
      "Start of epoch 1958\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 3.5505e-08 - val_loss: 3.7048e-08\n",
      "\n",
      "Start of epoch 1959\n",
      "1984/1984 [==============================] - 0s 42us/step - train_loss: 3.5350e-08 - val_loss: 3.6898e-08\n",
      "\n",
      "Start of epoch 1960\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 3.5202e-08 - val_loss: 3.6736e-08\n",
      "\n",
      "Start of epoch 1961\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 3.5056e-08 - val_loss: 3.6579e-08\n",
      "\n",
      "Start of epoch 1962\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 3.4929e-08 - val_loss: 3.6454e-08\n",
      "\n",
      "Start of epoch 1963\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 3.4765e-08 - val_loss: 3.6206e-08\n",
      "\n",
      "Start of epoch 1964\n",
      "1984/1984 [==============================] - 0s 42us/step - train_loss: 3.4629e-08 - val_loss: 3.6074e-08\n",
      "\n",
      "Start of epoch 1965\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 3.4470e-08 - val_loss: 3.5863e-08\n",
      "\n",
      "Start of epoch 1966\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 3.4342e-08 - val_loss: 3.5742e-08\n",
      "\n",
      "Start of epoch 1967\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 3.4209e-08 - val_loss: 3.5564e-08\n",
      "\n",
      "Start of epoch 1968\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 3.4058e-08 - val_loss: 3.5418e-08\n",
      "\n",
      "Start of epoch 1969\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 3.3898e-08 - val_loss: 3.5241e-08\n",
      "\n",
      "Start of epoch 1970\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 3.3751e-08 - val_loss: 3.5082e-08\n",
      "\n",
      "Start of epoch 1971\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 3.3628e-08 - val_loss: 3.4966e-08\n",
      "\n",
      "Start of epoch 1972\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 3.3466e-08 - val_loss: 3.4765e-08\n",
      "\n",
      "Start of epoch 1973\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 3.3367e-08 - val_loss: 3.4423e-08\n",
      "\n",
      "Start of epoch 1974\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 3.3236e-08 - val_loss: 3.4252e-08\n",
      "\n",
      "Start of epoch 1975\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 3.3094e-08 - val_loss: 3.4097e-08\n",
      "\n",
      "Start of epoch 1976\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 3.2956e-08 - val_loss: 3.3941e-08\n",
      "\n",
      "Start of epoch 1977\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 3.2815e-08 - val_loss: 3.3783e-08\n",
      "\n",
      "Start of epoch 1978\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 3.2676e-08 - val_loss: 3.3626e-08\n",
      "\n",
      "Start of epoch 1979\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 3.2537e-08 - val_loss: 3.3471e-08\n",
      "\n",
      "Start of epoch 1980\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 3.2399e-08 - val_loss: 3.3314e-08\n",
      "\n",
      "Start of epoch 1981\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.2310e-08 - val_loss: 3.3387e-08\n",
      "\n",
      "Start of epoch 1982\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 3.2148e-08 - val_loss: 3.3013e-08\n",
      "\n",
      "Start of epoch 1983\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.2011e-08 - val_loss: 3.2886e-08\n",
      "\n",
      "Start of epoch 1984\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.1850e-08 - val_loss: 3.2712e-08\n",
      "\n",
      "Start of epoch 1985\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 3.1726e-08 - val_loss: 3.2565e-08\n",
      "\n",
      "Start of epoch 1986\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.1631e-08 - val_loss: 3.2472e-08\n",
      "\n",
      "Start of epoch 1987\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 3.1494e-08 - val_loss: 3.2265e-08\n",
      "\n",
      "Start of epoch 1988\n",
      "1984/1984 [==============================] - 0s 45us/step - train_loss: 3.1321e-08 - val_loss: 3.2101e-08\n",
      "\n",
      "Start of epoch 1989\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 3.1181e-08 - val_loss: 3.1948e-08\n",
      "\n",
      "Start of epoch 1990\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 3.1048e-08 - val_loss: 3.1796e-08\n",
      "\n",
      "Start of epoch 1991\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 3.0916e-08 - val_loss: 3.1645e-08\n",
      "\n",
      "Start of epoch 1992\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 3.0784e-08 - val_loss: 3.1494e-08\n",
      "\n",
      "Start of epoch 1993\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 3.0653e-08 - val_loss: 3.1344e-08\n",
      "\n",
      "Start of epoch 1994\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 3.0523e-08 - val_loss: 3.1197e-08\n",
      "\n",
      "Start of epoch 1995\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 3.0393e-08 - val_loss: 3.1051e-08\n",
      "\n",
      "Start of epoch 1996\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 3.0264e-08 - val_loss: 3.0907e-08\n",
      "\n",
      "Start of epoch 1997\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.0136e-08 - val_loss: 3.0763e-08\n",
      "\n",
      "Start of epoch 1998\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 3.0007e-08 - val_loss: 3.0619e-08\n",
      "\n",
      "Start of epoch 1999\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.9879e-08 - val_loss: 3.0478e-08\n",
      "\n",
      "Start of epoch 2000\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.9753e-08 - val_loss: 3.0336e-08\n",
      "\n",
      "Start of epoch 2001\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.9626e-08 - val_loss: 3.0196e-08\n",
      "\n",
      "Start of epoch 2002\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.9616e-08 - val_loss: 3.0012e-08\n",
      "\n",
      "Start of epoch 2003\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.9457e-08 - val_loss: 2.9886e-08\n",
      "\n",
      "Start of epoch 2004\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.9313e-08 - val_loss: 2.9742e-08\n",
      "\n",
      "Start of epoch 2005\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.9168e-08 - val_loss: 2.9643e-08\n",
      "\n",
      "Start of epoch 2006\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 2.9042e-08 - val_loss: 2.9492e-08\n",
      "\n",
      "Start of epoch 2007\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.8939e-08 - val_loss: 2.9345e-08\n",
      "\n",
      "Start of epoch 2008\n",
      "1984/1984 [==============================] - 0s 40us/step - train_loss: 2.8822e-08 - val_loss: 2.9208e-08\n",
      "\n",
      "Start of epoch 2009\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.8717e-08 - val_loss: 2.9199e-08\n",
      "\n",
      "Start of epoch 2010\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.8593e-08 - val_loss: 2.8945e-08\n",
      "\n",
      "Start of epoch 2011\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.8461e-08 - val_loss: 2.8802e-08\n",
      "\n",
      "Start of epoch 2012\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8342e-08 - val_loss: 2.8668e-08\n",
      "\n",
      "Start of epoch 2013\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.8220e-08 - val_loss: 2.8538e-08\n",
      "\n",
      "Start of epoch 2014\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.8101e-08 - val_loss: 2.8405e-08\n",
      "\n",
      "Start of epoch 2015\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.7981e-08 - val_loss: 2.8275e-08\n",
      "\n",
      "Start of epoch 2016\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7864e-08 - val_loss: 2.8149e-08\n",
      "\n",
      "Start of epoch 2017\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7747e-08 - val_loss: 2.8020e-08\n",
      "\n",
      "Start of epoch 2018\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7633e-08 - val_loss: 2.7901e-08\n",
      "\n",
      "Start of epoch 2019\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7518e-08 - val_loss: 2.7775e-08\n",
      "\n",
      "Start of epoch 2020\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7403e-08 - val_loss: 2.7655e-08\n",
      "\n",
      "Start of epoch 2021\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7289e-08 - val_loss: 2.7535e-08\n",
      "\n",
      "Start of epoch 2022\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.7176e-08 - val_loss: 2.7419e-08\n",
      "\n",
      "Start of epoch 2023\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.7064e-08 - val_loss: 2.7303e-08\n",
      "\n",
      "Start of epoch 2024\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.6954e-08 - val_loss: 2.7189e-08\n",
      "\n",
      "Start of epoch 2025\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.6844e-08 - val_loss: 2.7076e-08\n",
      "\n",
      "Start of epoch 2026\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.6741e-08 - val_loss: 2.6983e-08\n",
      "\n",
      "Start of epoch 2027\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.6634e-08 - val_loss: 2.6870e-08\n",
      "\n",
      "Start of epoch 2028\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.6650e-08 - val_loss: 2.6734e-08\n",
      "\n",
      "Start of epoch 2029\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.6478e-08 - val_loss: 2.6604e-08\n",
      "\n",
      "Start of epoch 2030\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.6308e-08 - val_loss: 2.6503e-08\n",
      "\n",
      "Start of epoch 2031\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 2.6193e-08 - val_loss: 2.6397e-08\n",
      "\n",
      "Start of epoch 2032\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.6085e-08 - val_loss: 2.6296e-08\n",
      "\n",
      "Start of epoch 2033\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.6089e-08 - val_loss: 2.6174e-08\n",
      "\n",
      "Start of epoch 2034\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5923e-08 - val_loss: 2.6066e-08\n",
      "\n",
      "Start of epoch 2035\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5783e-08 - val_loss: 2.5973e-08\n",
      "\n",
      "Start of epoch 2036\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5669e-08 - val_loss: 2.5875e-08\n",
      "\n",
      "Start of epoch 2037\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.5674e-08 - val_loss: 2.5763e-08\n",
      "\n",
      "Start of epoch 2038\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.5510e-08 - val_loss: 2.5658e-08\n",
      "\n",
      "Start of epoch 2039\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5372e-08 - val_loss: 2.5569e-08\n",
      "\n",
      "Start of epoch 2040\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.5367e-08 - val_loss: 2.5461e-08\n",
      "\n",
      "Start of epoch 2041\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.5210e-08 - val_loss: 2.5360e-08\n",
      "\n",
      "Start of epoch 2042\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.5071e-08 - val_loss: 2.5276e-08\n",
      "\n",
      "Start of epoch 2043\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 2.5067e-08 - val_loss: 2.5172e-08\n",
      "\n",
      "Start of epoch 2044\n",
      "1984/1984 [==============================] - 0s 43us/step - train_loss: 2.5022e-08 - val_loss: 2.5077e-08\n",
      "\n",
      "Start of epoch 2045\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 2.4823e-08 - val_loss: 2.4969e-08\n",
      "\n",
      "Start of epoch 2046\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.4682e-08 - val_loss: 2.4889e-08\n",
      "\n",
      "Start of epoch 2047\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.4677e-08 - val_loss: 2.4791e-08\n",
      "\n",
      "Start of epoch 2048\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4520e-08 - val_loss: 2.4698e-08\n",
      "\n",
      "Start of epoch 2049\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.4494e-08 - val_loss: 2.4611e-08\n",
      "\n",
      "Start of epoch 2050\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.4333e-08 - val_loss: 2.4514e-08\n",
      "\n",
      "Start of epoch 2051\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.4304e-08 - val_loss: 2.4431e-08\n",
      "\n",
      "Start of epoch 2052\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.4183e-08 - val_loss: 2.4496e-08\n",
      "\n",
      "Start of epoch 2053\n",
      "1984/1984 [==============================] - 0s 45us/step - train_loss: 2.4180e-08 - val_loss: 2.4274e-08\n",
      "\n",
      "Start of epoch 2054\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 2.4061e-08 - val_loss: 2.4156e-08\n",
      "\n",
      "Start of epoch 2055\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.3868e-08 - val_loss: 2.4059e-08\n",
      "\n",
      "Start of epoch 2056\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.3728e-08 - val_loss: 2.3986e-08\n",
      "\n",
      "Start of epoch 2057\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.3697e-08 - val_loss: 2.3860e-08\n",
      "\n",
      "Start of epoch 2058\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.3574e-08 - val_loss: 2.3784e-08\n",
      "\n",
      "Start of epoch 2059\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.3450e-08 - val_loss: 2.3716e-08\n",
      "\n",
      "Start of epoch 2060\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 2.3308e-08 - val_loss: 2.3573e-08\n",
      "\n",
      "Start of epoch 2061\n",
      "1984/1984 [==============================] - 0s 42us/step - train_loss: 2.3270e-08 - val_loss: 2.3510e-08\n",
      "\n",
      "Start of epoch 2062\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.3174e-08 - val_loss: 2.3454e-08\n",
      "\n",
      "Start of epoch 2063\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 2.3068e-08 - val_loss: 2.3367e-08\n",
      "\n",
      "Start of epoch 2064\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.3001e-08 - val_loss: 2.3291e-08\n",
      "\n",
      "Start of epoch 2065\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.2863e-08 - val_loss: 2.3150e-08\n",
      "\n",
      "Start of epoch 2066\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.2823e-08 - val_loss: 2.3087e-08\n",
      "\n",
      "Start of epoch 2067\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.2726e-08 - val_loss: 2.3044e-08\n",
      "\n",
      "Start of epoch 2068\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.2589e-08 - val_loss: 2.2895e-08\n",
      "\n",
      "Start of epoch 2069\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 2.2552e-08 - val_loss: 2.2834e-08\n",
      "\n",
      "Start of epoch 2070\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.2443e-08 - val_loss: 2.2754e-08\n",
      "\n",
      "Start of epoch 2071\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.2370e-08 - val_loss: 2.2686e-08\n",
      "\n",
      "Start of epoch 2072\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.2237e-08 - val_loss: 2.2541e-08\n",
      "\n",
      "Start of epoch 2073\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.2191e-08 - val_loss: 2.2482e-08\n",
      "\n",
      "Start of epoch 2074\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.2079e-08 - val_loss: 2.2404e-08\n",
      "\n",
      "Start of epoch 2075\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.2058e-08 - val_loss: 2.2289e-08\n",
      "\n",
      "Start of epoch 2076\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.1912e-08 - val_loss: 2.2215e-08\n",
      "\n",
      "Start of epoch 2077\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1789e-08 - val_loss: 2.2102e-08\n",
      "\n",
      "Start of epoch 2078\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1772e-08 - val_loss: 2.2023e-08\n",
      "\n",
      "Start of epoch 2079\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.1618e-08 - val_loss: 2.1822e-08\n",
      "\n",
      "Start of epoch 2080\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 2.1494e-08 - val_loss: 2.1700e-08\n",
      "\n",
      "Start of epoch 2081\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 2.1387e-08 - val_loss: 2.1575e-08\n",
      "\n",
      "Start of epoch 2082\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.1297e-08 - val_loss: 2.1460e-08\n",
      "\n",
      "Start of epoch 2083\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1204e-08 - val_loss: 2.1349e-08\n",
      "\n",
      "Start of epoch 2084\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.1107e-08 - val_loss: 2.1240e-08\n",
      "\n",
      "Start of epoch 2085\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.1021e-08 - val_loss: 2.1131e-08\n",
      "\n",
      "Start of epoch 2086\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0929e-08 - val_loss: 2.1024e-08\n",
      "\n",
      "Start of epoch 2087\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0832e-08 - val_loss: 2.0917e-08\n",
      "\n",
      "Start of epoch 2088\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.0746e-08 - val_loss: 2.0810e-08\n",
      "\n",
      "Start of epoch 2089\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0649e-08 - val_loss: 2.0702e-08\n",
      "\n",
      "Start of epoch 2090\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 2.0564e-08 - val_loss: 2.0596e-08\n",
      "\n",
      "Start of epoch 2091\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.0494e-08 - val_loss: 2.0494e-08\n",
      "\n",
      "Start of epoch 2092\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0381e-08 - val_loss: 2.0490e-08\n",
      "\n",
      "Start of epoch 2093\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 2.0371e-08 - val_loss: 2.0276e-08\n",
      "\n",
      "Start of epoch 2094\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 2.0122e-08 - val_loss: 2.0090e-08\n",
      "\n",
      "Start of epoch 2095\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 2.0121e-08 - val_loss: 2.0035e-08\n",
      "\n",
      "Start of epoch 2096\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.9930e-08 - val_loss: 1.9866e-08\n",
      "\n",
      "Start of epoch 2097\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.9949e-08 - val_loss: 1.9808e-08\n",
      "\n",
      "Start of epoch 2098\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.9757e-08 - val_loss: 1.9648e-08\n",
      "\n",
      "Start of epoch 2099\n",
      "1984/1984 [==============================] - 0s 49us/step - train_loss: 1.9699e-08 - val_loss: 1.9536e-08\n",
      "\n",
      "Start of epoch 2100\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.9545e-08 - val_loss: 1.9418e-08\n",
      "\n",
      "Start of epoch 2101\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.9533e-08 - val_loss: 1.9342e-08\n",
      "\n",
      "Start of epoch 2102\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.9407e-08 - val_loss: 1.9451e-08\n",
      "\n",
      "Start of epoch 2103\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.9375e-08 - val_loss: 1.9083e-08\n",
      "\n",
      "Start of epoch 2104\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.9233e-08 - val_loss: 1.9229e-08\n",
      "\n",
      "Start of epoch 2105\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.9244e-08 - val_loss: 1.8827e-08\n",
      "\n",
      "Start of epoch 2106\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.9093e-08 - val_loss: 1.9000e-08\n",
      "\n",
      "Start of epoch 2107\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.9068e-08 - val_loss: 1.8836e-08\n",
      "\n",
      "Start of epoch 2108\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.8958e-08 - val_loss: 1.8769e-08\n",
      "\n",
      "Start of epoch 2109\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.8908e-08 - val_loss: 1.8638e-08\n",
      "\n",
      "Start of epoch 2110\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.8849e-08 - val_loss: 1.8541e-08\n",
      "\n",
      "Start of epoch 2111\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.8770e-08 - val_loss: 1.8487e-08\n",
      "\n",
      "Start of epoch 2112\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.8704e-08 - val_loss: 1.8366e-08\n",
      "\n",
      "Start of epoch 2113\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.8664e-08 - val_loss: 1.8277e-08\n",
      "\n",
      "Start of epoch 2114\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.8596e-08 - val_loss: 1.8192e-08\n",
      "\n",
      "Start of epoch 2115\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.8513e-08 - val_loss: 1.8139e-08\n",
      "\n",
      "Start of epoch 2116\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 1.8465e-08 - val_loss: 1.8036e-08\n",
      "\n",
      "Start of epoch 2117\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.8428e-08 - val_loss: 1.7963e-08\n",
      "\n",
      "Start of epoch 2118\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.8396e-08 - val_loss: 1.8033e-08\n",
      "\n",
      "Start of epoch 2119\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.8331e-08 - val_loss: 1.7802e-08\n",
      "\n",
      "Start of epoch 2120\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.8265e-08 - val_loss: 1.7732e-08\n",
      "\n",
      "Start of epoch 2121\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.8211e-08 - val_loss: 1.7667e-08\n",
      "\n",
      "Start of epoch 2122\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.8166e-08 - val_loss: 1.7594e-08\n",
      "\n",
      "Start of epoch 2123\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.8112e-08 - val_loss: 1.7533e-08\n",
      "\n",
      "Start of epoch 2124\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.8068e-08 - val_loss: 1.7464e-08\n",
      "\n",
      "Start of epoch 2125\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 1.8015e-08 - val_loss: 1.7401e-08\n",
      "\n",
      "Start of epoch 2126\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.7965e-08 - val_loss: 1.7342e-08\n",
      "\n",
      "Start of epoch 2127\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.7923e-08 - val_loss: 1.7277e-08\n",
      "\n",
      "Start of epoch 2128\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.7869e-08 - val_loss: 1.7219e-08\n",
      "\n",
      "Start of epoch 2129\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.7830e-08 - val_loss: 1.7162e-08\n",
      "\n",
      "Start of epoch 2130\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.7778e-08 - val_loss: 1.7105e-08\n",
      "\n",
      "Start of epoch 2131\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.7740e-08 - val_loss: 1.7053e-08\n",
      "\n",
      "Start of epoch 2132\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.7690e-08 - val_loss: 1.6994e-08\n",
      "\n",
      "Start of epoch 2133\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7652e-08 - val_loss: 1.6941e-08\n",
      "\n",
      "Start of epoch 2134\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7601e-08 - val_loss: 1.6888e-08\n",
      "\n",
      "Start of epoch 2135\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.7581e-08 - val_loss: 1.6903e-08\n",
      "\n",
      "Start of epoch 2136\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.7545e-08 - val_loss: 1.6860e-08\n",
      "\n",
      "Start of epoch 2137\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7503e-08 - val_loss: 1.6820e-08\n",
      "\n",
      "Start of epoch 2138\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7472e-08 - val_loss: 1.6878e-08\n",
      "\n",
      "Start of epoch 2139\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.7438e-08 - val_loss: 1.6833e-08\n",
      "\n",
      "Start of epoch 2140\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7387e-08 - val_loss: 1.6786e-08\n",
      "\n",
      "Start of epoch 2141\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7349e-08 - val_loss: 1.6775e-08\n",
      "\n",
      "Start of epoch 2142\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7328e-08 - val_loss: 1.6713e-08\n",
      "\n",
      "Start of epoch 2143\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.7269e-08 - val_loss: 1.6671e-08\n",
      "\n",
      "Start of epoch 2144\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.7197e-08 - val_loss: 1.6591e-08\n",
      "\n",
      "Start of epoch 2145\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.7149e-08 - val_loss: 1.6542e-08\n",
      "\n",
      "Start of epoch 2146\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.7106e-08 - val_loss: 1.6494e-08\n",
      "\n",
      "Start of epoch 2147\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.7064e-08 - val_loss: 1.6446e-08\n",
      "\n",
      "Start of epoch 2148\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.6943e-08 - val_loss: 1.6390e-08\n",
      "\n",
      "Start of epoch 2149\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.6978e-08 - val_loss: 1.6335e-08\n",
      "\n",
      "Start of epoch 2150\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.6941e-08 - val_loss: 1.6295e-08\n",
      "\n",
      "Start of epoch 2151\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.6893e-08 - val_loss: 1.6250e-08\n",
      "\n",
      "Start of epoch 2152\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.6785e-08 - val_loss: 1.6199e-08\n",
      "\n",
      "Start of epoch 2153\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.6811e-08 - val_loss: 1.6148e-08\n",
      "\n",
      "Start of epoch 2154\n",
      "1984/1984 [==============================] - 0s 43us/step - train_loss: 1.6702e-08 - val_loss: 1.6106e-08\n",
      "\n",
      "Start of epoch 2155\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.6730e-08 - val_loss: 1.6055e-08\n",
      "\n",
      "Start of epoch 2156\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.6600e-08 - val_loss: 1.6013e-08\n",
      "\n",
      "Start of epoch 2157\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.6564e-08 - val_loss: 1.5980e-08\n",
      "\n",
      "Start of epoch 2158\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6520e-08 - val_loss: 1.5921e-08\n",
      "\n",
      "Start of epoch 2159\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.6463e-08 - val_loss: 1.5893e-08\n",
      "\n",
      "Start of epoch 2160\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.6455e-08 - val_loss: 1.5861e-08\n",
      "\n",
      "Start of epoch 2161\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.6434e-08 - val_loss: 1.5823e-08\n",
      "\n",
      "Start of epoch 2162\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 1.6370e-08 - val_loss: 1.5767e-08\n",
      "\n",
      "Start of epoch 2163\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 1.6327e-08 - val_loss: 1.5815e-08\n",
      "\n",
      "Start of epoch 2164\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.6315e-08 - val_loss: 1.5774e-08\n",
      "\n",
      "Start of epoch 2165\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.6268e-08 - val_loss: 1.5583e-08\n",
      "\n",
      "Start of epoch 2166\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.6202e-08 - val_loss: 1.5689e-08\n",
      "\n",
      "Start of epoch 2167\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.6082e-08 - val_loss: 1.5497e-08\n",
      "\n",
      "Start of epoch 2168\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.6038e-08 - val_loss: 1.5616e-08\n",
      "\n",
      "Start of epoch 2169\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.6062e-08 - val_loss: 1.5429e-08\n",
      "\n",
      "Start of epoch 2170\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.5982e-08 - val_loss: 1.5549e-08\n",
      "\n",
      "Start of epoch 2171\n",
      "1984/1984 [==============================] - 0s 46us/step - train_loss: 1.5851e-08 - val_loss: 1.5382e-08\n",
      "\n",
      "Start of epoch 2172\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.5867e-08 - val_loss: 1.5493e-08\n",
      "\n",
      "Start of epoch 2173\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.5775e-08 - val_loss: 1.5458e-08\n",
      "\n",
      "Start of epoch 2174\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.5779e-08 - val_loss: 1.5423e-08\n",
      "\n",
      "Start of epoch 2175\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.5684e-08 - val_loss: 1.5397e-08\n",
      "\n",
      "Start of epoch 2176\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.5612e-08 - val_loss: 1.5389e-08\n",
      "\n",
      "Start of epoch 2177\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.5572e-08 - val_loss: 1.5416e-08\n",
      "\n",
      "Start of epoch 2178\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.5527e-08 - val_loss: 1.5309e-08\n",
      "\n",
      "Start of epoch 2179\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 1.5478e-08 - val_loss: 1.5270e-08\n",
      "\n",
      "Start of epoch 2180\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 1.5446e-08 - val_loss: 1.5215e-08\n",
      "\n",
      "Start of epoch 2181\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 1.5408e-08 - val_loss: 1.5170e-08\n",
      "\n",
      "Start of epoch 2182\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 1.5368e-08 - val_loss: 1.5125e-08\n",
      "\n",
      "Start of epoch 2183\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 1.5339e-08 - val_loss: 1.5078e-08\n",
      "\n",
      "Start of epoch 2184\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.5307e-08 - val_loss: 1.5034e-08\n",
      "\n",
      "Start of epoch 2185\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 1.5275e-08 - val_loss: 1.5013e-08\n",
      "\n",
      "Start of epoch 2186\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.5260e-08 - val_loss: 1.4943e-08\n",
      "\n",
      "Start of epoch 2187\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.5228e-08 - val_loss: 1.4895e-08\n",
      "\n",
      "Start of epoch 2188\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.5193e-08 - val_loss: 1.4845e-08\n",
      "\n",
      "Start of epoch 2189\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.5158e-08 - val_loss: 1.4789e-08\n",
      "\n",
      "Start of epoch 2190\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5132e-08 - val_loss: 1.4740e-08\n",
      "\n",
      "Start of epoch 2191\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5105e-08 - val_loss: 1.4691e-08\n",
      "\n",
      "Start of epoch 2192\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5076e-08 - val_loss: 1.4643e-08\n",
      "\n",
      "Start of epoch 2193\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.5056e-08 - val_loss: 1.4598e-08\n",
      "\n",
      "Start of epoch 2194\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.5034e-08 - val_loss: 1.4555e-08\n",
      "\n",
      "Start of epoch 2195\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.5011e-08 - val_loss: 1.4508e-08\n",
      "\n",
      "Start of epoch 2196\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.4986e-08 - val_loss: 1.4480e-08\n",
      "\n",
      "Start of epoch 2197\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 1.4967e-08 - val_loss: 1.4438e-08\n",
      "\n",
      "Start of epoch 2198\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.4945e-08 - val_loss: 1.4399e-08\n",
      "\n",
      "Start of epoch 2199\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.4927e-08 - val_loss: 1.4368e-08\n",
      "\n",
      "Start of epoch 2200\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.4909e-08 - val_loss: 1.4356e-08\n",
      "\n",
      "Start of epoch 2201\n",
      "1984/1984 [==============================] - 0s 42us/step - train_loss: 1.4904e-08 - val_loss: 1.4291e-08\n",
      "\n",
      "Start of epoch 2202\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 1.4885e-08 - val_loss: 1.4258e-08\n",
      "\n",
      "Start of epoch 2203\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4858e-08 - val_loss: 1.4243e-08\n",
      "\n",
      "Start of epoch 2204\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4828e-08 - val_loss: 1.4186e-08\n",
      "\n",
      "Start of epoch 2205\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4809e-08 - val_loss: 1.4154e-08\n",
      "\n",
      "Start of epoch 2206\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4788e-08 - val_loss: 1.4133e-08\n",
      "\n",
      "Start of epoch 2207\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.4768e-08 - val_loss: 1.4105e-08\n",
      "\n",
      "Start of epoch 2208\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4761e-08 - val_loss: 1.4130e-08\n",
      "\n",
      "Start of epoch 2209\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4714e-08 - val_loss: 1.4116e-08\n",
      "\n",
      "Start of epoch 2210\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4701e-08 - val_loss: 1.4021e-08\n",
      "\n",
      "Start of epoch 2211\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4665e-08 - val_loss: 1.4063e-08\n",
      "\n",
      "Start of epoch 2212\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4657e-08 - val_loss: 1.3970e-08\n",
      "\n",
      "Start of epoch 2213\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4622e-08 - val_loss: 1.4005e-08\n",
      "\n",
      "Start of epoch 2214\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.4614e-08 - val_loss: 1.3920e-08\n",
      "\n",
      "Start of epoch 2215\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 1.4582e-08 - val_loss: 1.3948e-08\n",
      "\n",
      "Start of epoch 2216\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.4567e-08 - val_loss: 1.3870e-08\n",
      "\n",
      "Start of epoch 2217\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.4540e-08 - val_loss: 1.3894e-08\n",
      "\n",
      "Start of epoch 2218\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4517e-08 - val_loss: 1.3820e-08\n",
      "\n",
      "Start of epoch 2219\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4480e-08 - val_loss: 1.3828e-08\n",
      "\n",
      "Start of epoch 2220\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4470e-08 - val_loss: 1.3798e-08\n",
      "\n",
      "Start of epoch 2221\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4525e-08 - val_loss: 1.3968e-08\n",
      "\n",
      "Start of epoch 2222\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4507e-08 - val_loss: 1.3803e-08\n",
      "\n",
      "Start of epoch 2223\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.4426e-08 - val_loss: 1.3724e-08\n",
      "\n",
      "Start of epoch 2224\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.4369e-08 - val_loss: 1.3686e-08\n",
      "\n",
      "Start of epoch 2225\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4360e-08 - val_loss: 1.3662e-08\n",
      "\n",
      "Start of epoch 2226\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4326e-08 - val_loss: 1.3616e-08\n",
      "\n",
      "Start of epoch 2227\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4312e-08 - val_loss: 1.3602e-08\n",
      "\n",
      "Start of epoch 2228\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.4290e-08 - val_loss: 1.3568e-08\n",
      "\n",
      "Start of epoch 2229\n",
      "1984/1984 [==============================] - 0s 46us/step - train_loss: 1.4264e-08 - val_loss: 1.3549e-08\n",
      "\n",
      "Start of epoch 2230\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.4258e-08 - val_loss: 1.3530e-08\n",
      "\n",
      "Start of epoch 2231\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 1.4237e-08 - val_loss: 1.3477e-08\n",
      "\n",
      "Start of epoch 2232\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.4216e-08 - val_loss: 1.3458e-08\n",
      "\n",
      "Start of epoch 2233\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.4205e-08 - val_loss: 1.3450e-08\n",
      "\n",
      "Start of epoch 2234\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4193e-08 - val_loss: 1.3390e-08\n",
      "\n",
      "Start of epoch 2235\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4172e-08 - val_loss: 1.3374e-08\n",
      "\n",
      "Start of epoch 2236\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4159e-08 - val_loss: 1.3369e-08\n",
      "\n",
      "Start of epoch 2237\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4148e-08 - val_loss: 1.3311e-08\n",
      "\n",
      "Start of epoch 2238\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.4133e-08 - val_loss: 1.3296e-08\n",
      "\n",
      "Start of epoch 2239\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4123e-08 - val_loss: 1.3295e-08\n",
      "\n",
      "Start of epoch 2240\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 1.4109e-08 - val_loss: 1.3235e-08\n",
      "\n",
      "Start of epoch 2241\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 1.4099e-08 - val_loss: 1.3231e-08\n",
      "\n",
      "Start of epoch 2242\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.4096e-08 - val_loss: 1.3223e-08\n",
      "\n",
      "Start of epoch 2243\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.4074e-08 - val_loss: 1.3214e-08\n",
      "\n",
      "Start of epoch 2244\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.4070e-08 - val_loss: 1.3166e-08\n",
      "\n",
      "Start of epoch 2245\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4053e-08 - val_loss: 1.3151e-08\n",
      "\n",
      "Start of epoch 2246\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4037e-08 - val_loss: 1.3137e-08\n",
      "\n",
      "Start of epoch 2247\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4024e-08 - val_loss: 1.3120e-08\n",
      "\n",
      "Start of epoch 2248\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4027e-08 - val_loss: 1.3181e-08\n",
      "\n",
      "Start of epoch 2249\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.4002e-08 - val_loss: 1.3188e-08\n",
      "\n",
      "Start of epoch 2250\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3952e-08 - val_loss: 1.3176e-08\n",
      "\n",
      "Start of epoch 2251\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3935e-08 - val_loss: 1.3162e-08\n",
      "\n",
      "Start of epoch 2252\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4002e-08 - val_loss: 1.3121e-08\n",
      "\n",
      "Start of epoch 2253\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4091e-08 - val_loss: 1.3063e-08\n",
      "\n",
      "Start of epoch 2254\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4132e-08 - val_loss: 1.3061e-08\n",
      "\n",
      "Start of epoch 2255\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4171e-08 - val_loss: 1.3054e-08\n",
      "\n",
      "Start of epoch 2256\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.4133e-08 - val_loss: 1.3059e-08\n",
      "\n",
      "Start of epoch 2257\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.4068e-08 - val_loss: 1.3035e-08\n",
      "\n",
      "Start of epoch 2258\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.4025e-08 - val_loss: 1.3034e-08\n",
      "\n",
      "Start of epoch 2259\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3996e-08 - val_loss: 1.3011e-08\n",
      "\n",
      "Start of epoch 2260\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3970e-08 - val_loss: 1.3015e-08\n",
      "\n",
      "Start of epoch 2261\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3944e-08 - val_loss: 1.2993e-08\n",
      "\n",
      "Start of epoch 2262\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.3925e-08 - val_loss: 1.2993e-08\n",
      "\n",
      "Start of epoch 2263\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.3905e-08 - val_loss: 1.2986e-08\n",
      "\n",
      "Start of epoch 2264\n",
      "1984/1984 [==============================] - 0s 47us/step - train_loss: 1.3878e-08 - val_loss: 1.2961e-08\n",
      "\n",
      "Start of epoch 2265\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 1.3864e-08 - val_loss: 1.2957e-08\n",
      "\n",
      "Start of epoch 2266\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.3823e-08 - val_loss: 1.2986e-08\n",
      "\n",
      "Start of epoch 2267\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.3761e-08 - val_loss: 1.2983e-08\n",
      "\n",
      "Start of epoch 2268\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.3739e-08 - val_loss: 1.2971e-08\n",
      "\n",
      "Start of epoch 2269\n",
      "1984/1984 [==============================] - 0s 40us/step - train_loss: 1.3724e-08 - val_loss: 1.2957e-08\n",
      "\n",
      "Start of epoch 2270\n",
      "1984/1984 [==============================] - 0s 45us/step - train_loss: 1.3710e-08 - val_loss: 1.2944e-08\n",
      "\n",
      "Start of epoch 2271\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 1.3690e-08 - val_loss: 1.2930e-08\n",
      "\n",
      "Start of epoch 2272\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.3683e-08 - val_loss: 1.2916e-08\n",
      "\n",
      "Start of epoch 2273\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.3662e-08 - val_loss: 1.2904e-08\n",
      "\n",
      "Start of epoch 2274\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3664e-08 - val_loss: 1.2937e-08\n",
      "\n",
      "Start of epoch 2275\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.3692e-08 - val_loss: 1.2882e-08\n",
      "\n",
      "Start of epoch 2276\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.3691e-08 - val_loss: 1.2929e-08\n",
      "\n",
      "Start of epoch 2277\n",
      "1984/1984 [==============================] - 0s 43us/step - train_loss: 1.3704e-08 - val_loss: 1.2847e-08\n",
      "\n",
      "Start of epoch 2278\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 1.3660e-08 - val_loss: 1.2835e-08\n",
      "\n",
      "Start of epoch 2279\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.3640e-08 - val_loss: 1.2859e-08\n",
      "\n",
      "Start of epoch 2280\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.3651e-08 - val_loss: 1.2809e-08\n",
      "\n",
      "Start of epoch 2281\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.3651e-08 - val_loss: 1.2954e-08\n",
      "\n",
      "Start of epoch 2282\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.3687e-08 - val_loss: 1.2950e-08\n",
      "\n",
      "Start of epoch 2283\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.3668e-08 - val_loss: 1.2955e-08\n",
      "\n",
      "Start of epoch 2284\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.3650e-08 - val_loss: 1.2840e-08\n",
      "\n",
      "Start of epoch 2285\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.3569e-08 - val_loss: 1.2904e-08\n",
      "\n",
      "Start of epoch 2286\n",
      "1984/1984 [==============================] - 0s 44us/step - train_loss: 1.3582e-08 - val_loss: 1.2809e-08\n",
      "\n",
      "Start of epoch 2287\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.3543e-08 - val_loss: 1.2884e-08\n",
      "\n",
      "Start of epoch 2288\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3563e-08 - val_loss: 1.2782e-08\n",
      "\n",
      "Start of epoch 2289\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.3523e-08 - val_loss: 1.2861e-08\n",
      "\n",
      "Start of epoch 2290\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.3539e-08 - val_loss: 1.2744e-08\n",
      "\n",
      "Start of epoch 2291\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3505e-08 - val_loss: 1.2848e-08\n",
      "\n",
      "Start of epoch 2292\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3533e-08 - val_loss: 1.2737e-08\n",
      "\n",
      "Start of epoch 2293\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.3488e-08 - val_loss: 1.2818e-08\n",
      "\n",
      "Start of epoch 2294\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.3531e-08 - val_loss: 1.2723e-08\n",
      "\n",
      "Start of epoch 2295\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 1.3477e-08 - val_loss: 1.2808e-08\n",
      "\n",
      "Start of epoch 2296\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 1.3518e-08 - val_loss: 1.2689e-08\n",
      "\n",
      "Start of epoch 2297\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.3467e-08 - val_loss: 1.2795e-08\n",
      "\n",
      "Start of epoch 2298\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.3506e-08 - val_loss: 1.2669e-08\n",
      "\n",
      "Start of epoch 2299\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3452e-08 - val_loss: 1.2778e-08\n",
      "\n",
      "Start of epoch 2300\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3490e-08 - val_loss: 1.2648e-08\n",
      "\n",
      "Start of epoch 2301\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.3438e-08 - val_loss: 1.2760e-08\n",
      "\n",
      "Start of epoch 2302\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3475e-08 - val_loss: 1.2625e-08\n",
      "\n",
      "Start of epoch 2303\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3412e-08 - val_loss: 1.2718e-08\n",
      "\n",
      "Start of epoch 2304\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3459e-08 - val_loss: 1.2606e-08\n",
      "\n",
      "Start of epoch 2305\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.3402e-08 - val_loss: 1.2700e-08\n",
      "\n",
      "Start of epoch 2306\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.3451e-08 - val_loss: 1.2591e-08\n",
      "\n",
      "Start of epoch 2307\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.3395e-08 - val_loss: 1.2689e-08\n",
      "\n",
      "Start of epoch 2308\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.3461e-08 - val_loss: 1.2679e-08\n",
      "\n",
      "Start of epoch 2309\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.3427e-08 - val_loss: 1.2629e-08\n",
      "\n",
      "Start of epoch 2310\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.3431e-08 - val_loss: 1.2679e-08\n",
      "\n",
      "Start of epoch 2311\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3445e-08 - val_loss: 1.2593e-08\n",
      "\n",
      "Start of epoch 2312\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3431e-08 - val_loss: 1.2616e-08\n",
      "\n",
      "Start of epoch 2313\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3421e-08 - val_loss: 1.2606e-08\n",
      "\n",
      "Start of epoch 2314\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3410e-08 - val_loss: 1.2594e-08\n",
      "\n",
      "Start of epoch 2315\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3402e-08 - val_loss: 1.2585e-08\n",
      "\n",
      "Start of epoch 2316\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3396e-08 - val_loss: 1.2575e-08\n",
      "\n",
      "Start of epoch 2317\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3388e-08 - val_loss: 1.2566e-08\n",
      "\n",
      "Start of epoch 2318\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.3359e-08 - val_loss: 1.2538e-08\n",
      "\n",
      "Start of epoch 2319\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3375e-08 - val_loss: 1.2558e-08\n",
      "\n",
      "Start of epoch 2320\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3355e-08 - val_loss: 1.2543e-08\n",
      "\n",
      "Start of epoch 2321\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3369e-08 - val_loss: 1.2550e-08\n",
      "\n",
      "Start of epoch 2322\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3355e-08 - val_loss: 1.2538e-08\n",
      "\n",
      "Start of epoch 2323\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3365e-08 - val_loss: 1.2538e-08\n",
      "\n",
      "Start of epoch 2324\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3344e-08 - val_loss: 1.2528e-08\n",
      "\n",
      "Start of epoch 2325\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3362e-08 - val_loss: 1.2534e-08\n",
      "\n",
      "Start of epoch 2326\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3340e-08 - val_loss: 1.2517e-08\n",
      "\n",
      "Start of epoch 2327\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3331e-08 - val_loss: 1.2502e-08\n",
      "\n",
      "Start of epoch 2328\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3321e-08 - val_loss: 1.2503e-08\n",
      "\n",
      "Start of epoch 2329\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3322e-08 - val_loss: 1.2492e-08\n",
      "\n",
      "Start of epoch 2330\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3322e-08 - val_loss: 1.2480e-08\n",
      "\n",
      "Start of epoch 2331\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.3357e-08 - val_loss: 1.2490e-08\n",
      "\n",
      "Start of epoch 2332\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3340e-08 - val_loss: 1.2491e-08\n",
      "\n",
      "Start of epoch 2333\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3301e-08 - val_loss: 1.2459e-08\n",
      "\n",
      "Start of epoch 2334\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3286e-08 - val_loss: 1.2451e-08\n",
      "\n",
      "Start of epoch 2335\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3280e-08 - val_loss: 1.2444e-08\n",
      "\n",
      "Start of epoch 2336\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3275e-08 - val_loss: 1.2436e-08\n",
      "\n",
      "Start of epoch 2337\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3269e-08 - val_loss: 1.2431e-08\n",
      "\n",
      "Start of epoch 2338\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3264e-08 - val_loss: 1.2441e-08\n",
      "\n",
      "Start of epoch 2339\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3268e-08 - val_loss: 1.2434e-08\n",
      "\n",
      "Start of epoch 2340\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3258e-08 - val_loss: 1.2413e-08\n",
      "\n",
      "Start of epoch 2341\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3243e-08 - val_loss: 1.2443e-08\n",
      "\n",
      "Start of epoch 2342\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3287e-08 - val_loss: 1.2430e-08\n",
      "\n",
      "Start of epoch 2343\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3269e-08 - val_loss: 1.2434e-08\n",
      "\n",
      "Start of epoch 2344\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3298e-08 - val_loss: 1.2435e-08\n",
      "\n",
      "Start of epoch 2345\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3267e-08 - val_loss: 1.2426e-08\n",
      "\n",
      "Start of epoch 2346\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3300e-08 - val_loss: 1.2449e-08\n",
      "\n",
      "Start of epoch 2347\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3257e-08 - val_loss: 1.2399e-08\n",
      "\n",
      "Start of epoch 2348\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3272e-08 - val_loss: 1.2415e-08\n",
      "\n",
      "Start of epoch 2349\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3254e-08 - val_loss: 1.2422e-08\n",
      "\n",
      "Start of epoch 2350\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3292e-08 - val_loss: 1.2427e-08\n",
      "\n",
      "Start of epoch 2351\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.3231e-08 - val_loss: 1.2499e-08\n",
      "\n",
      "Start of epoch 2352\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3205e-08 - val_loss: 1.2468e-08\n",
      "\n",
      "Start of epoch 2353\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3183e-08 - val_loss: 1.2433e-08\n",
      "\n",
      "Start of epoch 2354\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3241e-08 - val_loss: 1.2455e-08\n",
      "\n",
      "Start of epoch 2355\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3220e-08 - val_loss: 1.2442e-08\n",
      "\n",
      "Start of epoch 2356\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3282e-08 - val_loss: 1.2462e-08\n",
      "\n",
      "Start of epoch 2357\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3255e-08 - val_loss: 1.2405e-08\n",
      "\n",
      "Start of epoch 2358\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3227e-08 - val_loss: 1.2399e-08\n",
      "\n",
      "Start of epoch 2359\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3193e-08 - val_loss: 1.2457e-08\n",
      "\n",
      "Start of epoch 2360\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3164e-08 - val_loss: 1.2479e-08\n",
      "\n",
      "Start of epoch 2361\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3113e-08 - val_loss: 1.2435e-08\n",
      "\n",
      "Start of epoch 2362\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.3115e-08 - val_loss: 1.2415e-08\n",
      "\n",
      "Start of epoch 2363\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3112e-08 - val_loss: 1.2390e-08\n",
      "\n",
      "Start of epoch 2364\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3113e-08 - val_loss: 1.2375e-08\n",
      "\n",
      "Start of epoch 2365\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3131e-08 - val_loss: 1.2413e-08\n",
      "\n",
      "Start of epoch 2366\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3122e-08 - val_loss: 1.2355e-08\n",
      "\n",
      "Start of epoch 2367\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3130e-08 - val_loss: 1.2394e-08\n",
      "\n",
      "Start of epoch 2368\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3125e-08 - val_loss: 1.2363e-08\n",
      "\n",
      "Start of epoch 2369\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3170e-08 - val_loss: 1.2398e-08\n",
      "\n",
      "Start of epoch 2370\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3130e-08 - val_loss: 1.2385e-08\n",
      "\n",
      "Start of epoch 2371\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3172e-08 - val_loss: 1.2407e-08\n",
      "\n",
      "Start of epoch 2372\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3167e-08 - val_loss: 1.2436e-08\n",
      "\n",
      "Start of epoch 2373\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3200e-08 - val_loss: 1.2429e-08\n",
      "\n",
      "Start of epoch 2374\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3249e-08 - val_loss: 1.2453e-08\n",
      "\n",
      "Start of epoch 2375\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.3243e-08 - val_loss: 1.2445e-08\n",
      "\n",
      "Start of epoch 2376\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3207e-08 - val_loss: 1.2448e-08\n",
      "\n",
      "Start of epoch 2377\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3242e-08 - val_loss: 1.2461e-08\n",
      "\n",
      "Start of epoch 2378\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3198e-08 - val_loss: 1.2440e-08\n",
      "\n",
      "Start of epoch 2379\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3100e-08 - val_loss: 1.2372e-08\n",
      "\n",
      "Start of epoch 2380\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.3046e-08 - val_loss: 1.2336e-08\n",
      "\n",
      "Start of epoch 2381\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.3036e-08 - val_loss: 1.2331e-08\n",
      "\n",
      "Start of epoch 2382\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2980e-08 - val_loss: 1.2260e-08\n",
      "\n",
      "Start of epoch 2383\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2920e-08 - val_loss: 1.2225e-08\n",
      "\n",
      "Start of epoch 2384\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2896e-08 - val_loss: 1.2216e-08\n",
      "\n",
      "Start of epoch 2385\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2878e-08 - val_loss: 1.2202e-08\n",
      "\n",
      "Start of epoch 2386\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2862e-08 - val_loss: 1.2190e-08\n",
      "\n",
      "Start of epoch 2387\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2845e-08 - val_loss: 1.2208e-08\n",
      "\n",
      "Start of epoch 2388\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.2815e-08 - val_loss: 1.2254e-08\n",
      "\n",
      "Start of epoch 2389\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2794e-08 - val_loss: 1.2251e-08\n",
      "\n",
      "Start of epoch 2390\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2779e-08 - val_loss: 1.2231e-08\n",
      "\n",
      "Start of epoch 2391\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2754e-08 - val_loss: 1.2235e-08\n",
      "\n",
      "Start of epoch 2392\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2738e-08 - val_loss: 1.2208e-08\n",
      "\n",
      "Start of epoch 2393\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2731e-08 - val_loss: 1.2190e-08\n",
      "\n",
      "Start of epoch 2394\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2728e-08 - val_loss: 1.2208e-08\n",
      "\n",
      "Start of epoch 2395\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2757e-08 - val_loss: 1.2136e-08\n",
      "\n",
      "Start of epoch 2396\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2699e-08 - val_loss: 1.2177e-08\n",
      "\n",
      "Start of epoch 2397\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.2743e-08 - val_loss: 1.2103e-08\n",
      "\n",
      "Start of epoch 2398\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2715e-08 - val_loss: 1.2120e-08\n",
      "\n",
      "Start of epoch 2399\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2709e-08 - val_loss: 1.2079e-08\n",
      "\n",
      "Start of epoch 2400\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2688e-08 - val_loss: 1.2084e-08\n",
      "\n",
      "Start of epoch 2401\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2679e-08 - val_loss: 1.2056e-08\n",
      "\n",
      "Start of epoch 2402\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2663e-08 - val_loss: 1.2039e-08\n",
      "\n",
      "Start of epoch 2403\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2638e-08 - val_loss: 1.2062e-08\n",
      "\n",
      "Start of epoch 2404\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2614e-08 - val_loss: 1.2048e-08\n",
      "\n",
      "Start of epoch 2405\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2599e-08 - val_loss: 1.2033e-08\n",
      "\n",
      "Start of epoch 2406\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2577e-08 - val_loss: 1.2010e-08\n",
      "\n",
      "Start of epoch 2407\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2554e-08 - val_loss: 1.1983e-08\n",
      "\n",
      "Start of epoch 2408\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2547e-08 - val_loss: 1.1987e-08\n",
      "\n",
      "Start of epoch 2409\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2532e-08 - val_loss: 1.1951e-08\n",
      "\n",
      "Start of epoch 2410\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.2527e-08 - val_loss: 1.1950e-08\n",
      "\n",
      "Start of epoch 2411\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2507e-08 - val_loss: 1.1925e-08\n",
      "\n",
      "Start of epoch 2412\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2484e-08 - val_loss: 1.1895e-08\n",
      "\n",
      "Start of epoch 2413\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2486e-08 - val_loss: 1.1903e-08\n",
      "\n",
      "Start of epoch 2414\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2469e-08 - val_loss: 1.1870e-08\n",
      "\n",
      "Start of epoch 2415\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2528e-08 - val_loss: 1.1873e-08\n",
      "\n",
      "Start of epoch 2416\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2514e-08 - val_loss: 1.1866e-08\n",
      "\n",
      "Start of epoch 2417\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2439e-08 - val_loss: 1.1822e-08\n",
      "\n",
      "Start of epoch 2418\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2403e-08 - val_loss: 1.1914e-08\n",
      "\n",
      "Start of epoch 2419\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2362e-08 - val_loss: 1.1882e-08\n",
      "\n",
      "Start of epoch 2420\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2345e-08 - val_loss: 1.1869e-08\n",
      "\n",
      "Start of epoch 2421\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2335e-08 - val_loss: 1.1855e-08\n",
      "\n",
      "Start of epoch 2422\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2344e-08 - val_loss: 1.1827e-08\n",
      "\n",
      "Start of epoch 2423\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2301e-08 - val_loss: 1.1809e-08\n",
      "\n",
      "Start of epoch 2424\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2315e-08 - val_loss: 1.1786e-08\n",
      "\n",
      "Start of epoch 2425\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2285e-08 - val_loss: 1.1761e-08\n",
      "\n",
      "Start of epoch 2426\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2273e-08 - val_loss: 1.1740e-08\n",
      "\n",
      "Start of epoch 2427\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2254e-08 - val_loss: 1.1721e-08\n",
      "\n",
      "Start of epoch 2428\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2240e-08 - val_loss: 1.1704e-08\n",
      "\n",
      "Start of epoch 2429\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2226e-08 - val_loss: 1.1688e-08\n",
      "\n",
      "Start of epoch 2430\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.2211e-08 - val_loss: 1.1672e-08\n",
      "\n",
      "Start of epoch 2431\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.2197e-08 - val_loss: 1.1657e-08\n",
      "\n",
      "Start of epoch 2432\n",
      "1984/1984 [==============================] - 0s 43us/step - train_loss: 1.2187e-08 - val_loss: 1.1657e-08\n",
      "\n",
      "Start of epoch 2433\n",
      "1984/1984 [==============================] - 0s 40us/step - train_loss: 1.2179e-08 - val_loss: 1.1632e-08\n",
      "\n",
      "Start of epoch 2434\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.2160e-08 - val_loss: 1.1612e-08\n",
      "\n",
      "Start of epoch 2435\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.2147e-08 - val_loss: 1.1604e-08\n",
      "\n",
      "Start of epoch 2436\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.2150e-08 - val_loss: 1.1711e-08\n",
      "\n",
      "Start of epoch 2437\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2118e-08 - val_loss: 1.1647e-08\n",
      "\n",
      "Start of epoch 2438\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.2095e-08 - val_loss: 1.1621e-08\n",
      "\n",
      "Start of epoch 2439\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2078e-08 - val_loss: 1.1601e-08\n",
      "\n",
      "Start of epoch 2440\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2065e-08 - val_loss: 1.1581e-08\n",
      "\n",
      "Start of epoch 2441\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.2051e-08 - val_loss: 1.1561e-08\n",
      "\n",
      "Start of epoch 2442\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 1.2041e-08 - val_loss: 1.1550e-08\n",
      "\n",
      "Start of epoch 2443\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.2027e-08 - val_loss: 1.1544e-08\n",
      "\n",
      "Start of epoch 2444\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.2021e-08 - val_loss: 1.1505e-08\n",
      "\n",
      "Start of epoch 2445\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.2022e-08 - val_loss: 1.1581e-08\n",
      "\n",
      "Start of epoch 2446\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.2038e-08 - val_loss: 1.1590e-08\n",
      "\n",
      "Start of epoch 2447\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2060e-08 - val_loss: 1.1573e-08\n",
      "\n",
      "Start of epoch 2448\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.2033e-08 - val_loss: 1.1510e-08\n",
      "\n",
      "Start of epoch 2449\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2011e-08 - val_loss: 1.1474e-08\n",
      "\n",
      "Start of epoch 2450\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1991e-08 - val_loss: 1.1462e-08\n",
      "\n",
      "Start of epoch 2451\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1982e-08 - val_loss: 1.1445e-08\n",
      "\n",
      "Start of epoch 2452\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.2078e-08 - val_loss: 1.1391e-08\n",
      "\n",
      "Start of epoch 2453\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.1887e-08 - val_loss: 1.1382e-08\n",
      "\n",
      "Start of epoch 2454\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.2053e-08 - val_loss: 1.1356e-08\n",
      "\n",
      "Start of epoch 2455\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1976e-08 - val_loss: 1.1354e-08\n",
      "\n",
      "Start of epoch 2456\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.2005e-08 - val_loss: 1.1370e-08\n",
      "\n",
      "Start of epoch 2457\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1998e-08 - val_loss: 1.1334e-08\n",
      "\n",
      "Start of epoch 2458\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1979e-08 - val_loss: 1.1319e-08\n",
      "\n",
      "Start of epoch 2459\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1972e-08 - val_loss: 1.1489e-08\n",
      "\n",
      "Start of epoch 2460\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.1918e-08 - val_loss: 1.1421e-08\n",
      "\n",
      "Start of epoch 2461\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1867e-08 - val_loss: 1.1456e-08\n",
      "\n",
      "Start of epoch 2462\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1843e-08 - val_loss: 1.1419e-08\n",
      "\n",
      "Start of epoch 2463\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1798e-08 - val_loss: 1.1386e-08\n",
      "\n",
      "Start of epoch 2464\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1784e-08 - val_loss: 1.1435e-08\n",
      "\n",
      "Start of epoch 2465\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1764e-08 - val_loss: 1.1360e-08\n",
      "\n",
      "Start of epoch 2466\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1730e-08 - val_loss: 1.1317e-08\n",
      "\n",
      "Start of epoch 2467\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.1674e-08 - val_loss: 1.1265e-08\n",
      "\n",
      "Start of epoch 2468\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1646e-08 - val_loss: 1.1210e-08\n",
      "\n",
      "Start of epoch 2469\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1638e-08 - val_loss: 1.1200e-08\n",
      "\n",
      "Start of epoch 2470\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1626e-08 - val_loss: 1.1163e-08\n",
      "\n",
      "Start of epoch 2471\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1578e-08 - val_loss: 1.1114e-08\n",
      "\n",
      "Start of epoch 2472\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1550e-08 - val_loss: 1.1092e-08\n",
      "\n",
      "Start of epoch 2473\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1526e-08 - val_loss: 1.1080e-08\n",
      "\n",
      "Start of epoch 2474\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1555e-08 - val_loss: 1.1092e-08\n",
      "\n",
      "Start of epoch 2475\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1461e-08 - val_loss: 1.1037e-08\n",
      "\n",
      "Start of epoch 2476\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1449e-08 - val_loss: 1.1044e-08\n",
      "\n",
      "Start of epoch 2477\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1428e-08 - val_loss: 1.0973e-08\n",
      "\n",
      "Start of epoch 2478\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1381e-08 - val_loss: 1.0972e-08\n",
      "\n",
      "Start of epoch 2479\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.1391e-08 - val_loss: 1.0876e-08\n",
      "\n",
      "Start of epoch 2480\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.1322e-08 - val_loss: 1.0819e-08\n",
      "\n",
      "Start of epoch 2481\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1278e-08 - val_loss: 1.0802e-08\n",
      "\n",
      "Start of epoch 2482\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1235e-08 - val_loss: 1.0721e-08\n",
      "\n",
      "Start of epoch 2483\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1207e-08 - val_loss: 1.0681e-08\n",
      "\n",
      "Start of epoch 2484\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1173e-08 - val_loss: 1.0633e-08\n",
      "\n",
      "Start of epoch 2485\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1151e-08 - val_loss: 1.0594e-08\n",
      "\n",
      "Start of epoch 2486\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1131e-08 - val_loss: 1.0557e-08\n",
      "\n",
      "Start of epoch 2487\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.1142e-08 - val_loss: 1.0653e-08\n",
      "\n",
      "Start of epoch 2488\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1113e-08 - val_loss: 1.0600e-08\n",
      "\n",
      "Start of epoch 2489\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.1043e-08 - val_loss: 1.0558e-08\n",
      "\n",
      "Start of epoch 2490\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.0983e-08 - val_loss: 1.0500e-08\n",
      "\n",
      "Start of epoch 2491\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 1.0930e-08 - val_loss: 1.0440e-08\n",
      "\n",
      "Start of epoch 2492\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0888e-08 - val_loss: 1.0389e-08\n",
      "\n",
      "Start of epoch 2493\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0842e-08 - val_loss: 1.0333e-08\n",
      "\n",
      "Start of epoch 2494\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0812e-08 - val_loss: 1.0288e-08\n",
      "\n",
      "Start of epoch 2495\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.0767e-08 - val_loss: 1.0230e-08\n",
      "\n",
      "Start of epoch 2496\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0747e-08 - val_loss: 1.0208e-08\n",
      "\n",
      "Start of epoch 2497\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 1.0712e-08 - val_loss: 1.0194e-08\n",
      "\n",
      "Start of epoch 2498\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0677e-08 - val_loss: 1.0086e-08\n",
      "\n",
      "Start of epoch 2499\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0628e-08 - val_loss: 1.0033e-08\n",
      "\n",
      "Start of epoch 2500\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.0605e-08 - val_loss: 9.9926e-09\n",
      "\n",
      "Start of epoch 2501\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0573e-08 - val_loss: 9.9468e-09\n",
      "\n",
      "Start of epoch 2502\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0542e-08 - val_loss: 9.9044e-09\n",
      "\n",
      "Start of epoch 2503\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.0517e-08 - val_loss: 9.8724e-09\n",
      "\n",
      "Start of epoch 2504\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.0514e-08 - val_loss: 9.8926e-09\n",
      "\n",
      "Start of epoch 2505\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.0479e-08 - val_loss: 9.7760e-09\n",
      "\n",
      "Start of epoch 2506\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 1.0443e-08 - val_loss: 9.7476e-09\n",
      "\n",
      "Start of epoch 2507\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.0413e-08 - val_loss: 9.7004e-09\n",
      "\n",
      "Start of epoch 2508\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.0389e-08 - val_loss: 9.6648e-09\n",
      "\n",
      "Start of epoch 2509\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.0357e-08 - val_loss: 9.6315e-09\n",
      "\n",
      "Start of epoch 2510\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.0328e-08 - val_loss: 9.5971e-09\n",
      "\n",
      "Start of epoch 2511\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.0307e-08 - val_loss: 9.5637e-09\n",
      "\n",
      "Start of epoch 2512\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 1.0283e-08 - val_loss: 9.5289e-09\n",
      "\n",
      "Start of epoch 2513\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0271e-08 - val_loss: 9.5330e-09\n",
      "\n",
      "Start of epoch 2514\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0266e-08 - val_loss: 9.4668e-09\n",
      "\n",
      "Start of epoch 2515\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 1.0231e-08 - val_loss: 9.4446e-09\n",
      "\n",
      "Start of epoch 2516\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 1.0239e-08 - val_loss: 9.4904e-09\n",
      "\n",
      "Start of epoch 2517\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.0209e-08 - val_loss: 9.4892e-09\n",
      "\n",
      "Start of epoch 2518\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.0193e-08 - val_loss: 9.4130e-09\n",
      "\n",
      "Start of epoch 2519\n",
      "1984/1984 [==============================] - 0s 43us/step - train_loss: 1.0165e-08 - val_loss: 9.3907e-09\n",
      "\n",
      "Start of epoch 2520\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 1.0137e-08 - val_loss: 9.3416e-09\n",
      "\n",
      "Start of epoch 2521\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.0111e-08 - val_loss: 9.3024e-09\n",
      "\n",
      "Start of epoch 2522\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 1.0080e-08 - val_loss: 9.2696e-09\n",
      "\n",
      "Start of epoch 2523\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.0053e-08 - val_loss: 9.2297e-09\n",
      "\n",
      "Start of epoch 2524\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.0026e-08 - val_loss: 9.1954e-09\n",
      "\n",
      "Start of epoch 2525\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 1.0005e-08 - val_loss: 9.1656e-09\n",
      "\n",
      "Start of epoch 2526\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 9.9624e-09 - val_loss: 9.1277e-09\n",
      "\n",
      "Start of epoch 2527\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 9.9368e-09 - val_loss: 9.0995e-09\n",
      "\n",
      "Start of epoch 2528\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.9215e-09 - val_loss: 9.0713e-09\n",
      "\n",
      "Start of epoch 2529\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.8999e-09 - val_loss: 9.0441e-09\n",
      "\n",
      "Start of epoch 2530\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.8792e-09 - val_loss: 9.0174e-09\n",
      "\n",
      "Start of epoch 2531\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.8595e-09 - val_loss: 8.9923e-09\n",
      "\n",
      "Start of epoch 2532\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.8407e-09 - val_loss: 8.9725e-09\n",
      "\n",
      "Start of epoch 2533\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.8545e-09 - val_loss: 9.0290e-09\n",
      "\n",
      "Start of epoch 2534\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.8347e-09 - val_loss: 8.9279e-09\n",
      "\n",
      "Start of epoch 2535\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.8030e-09 - val_loss: 8.9043e-09\n",
      "\n",
      "Start of epoch 2536\n",
      "1984/1984 [==============================] - 0s 47us/step - train_loss: 9.7820e-09 - val_loss: 8.8811e-09\n",
      "\n",
      "Start of epoch 2537\n",
      "1984/1984 [==============================] - 0s 40us/step - train_loss: 9.7603e-09 - val_loss: 8.8553e-09\n",
      "\n",
      "Start of epoch 2538\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 9.7229e-09 - val_loss: 8.8289e-09\n",
      "\n",
      "Start of epoch 2539\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 9.7065e-09 - val_loss: 8.8208e-09\n",
      "\n",
      "Start of epoch 2540\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.6985e-09 - val_loss: 8.8325e-09\n",
      "\n",
      "Start of epoch 2541\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.6770e-09 - val_loss: 8.7649e-09\n",
      "\n",
      "Start of epoch 2542\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.6495e-09 - val_loss: 8.7456e-09\n",
      "\n",
      "Start of epoch 2543\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.6338e-09 - val_loss: 8.7260e-09\n",
      "\n",
      "Start of epoch 2544\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.6184e-09 - val_loss: 8.7065e-09\n",
      "\n",
      "Start of epoch 2545\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 9.5993e-09 - val_loss: 8.6759e-09\n",
      "\n",
      "Start of epoch 2546\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 9.5495e-09 - val_loss: 8.6246e-09\n",
      "\n",
      "Start of epoch 2547\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 9.4913e-09 - val_loss: 8.6080e-09\n",
      "\n",
      "Start of epoch 2548\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 9.4761e-09 - val_loss: 8.5925e-09\n",
      "\n",
      "Start of epoch 2549\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 9.4773e-09 - val_loss: 8.6400e-09\n",
      "\n",
      "Start of epoch 2550\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 9.4643e-09 - val_loss: 8.5882e-09\n",
      "\n",
      "Start of epoch 2551\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 9.4649e-09 - val_loss: 8.5884e-09\n",
      "\n",
      "Start of epoch 2552\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.3839e-09 - val_loss: 8.5345e-09\n",
      "\n",
      "Start of epoch 2553\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.2871e-09 - val_loss: 8.5072e-09\n",
      "\n",
      "Start of epoch 2554\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.2491e-09 - val_loss: 8.4911e-09\n",
      "\n",
      "Start of epoch 2555\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.2306e-09 - val_loss: 8.4820e-09\n",
      "\n",
      "Start of epoch 2556\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.2130e-09 - val_loss: 8.4722e-09\n",
      "\n",
      "Start of epoch 2557\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.1863e-09 - val_loss: 8.4561e-09\n",
      "\n",
      "Start of epoch 2558\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.1778e-09 - val_loss: 8.4508e-09\n",
      "\n",
      "Start of epoch 2559\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.1405e-09 - val_loss: 8.4345e-09\n",
      "\n",
      "Start of epoch 2560\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.1268e-09 - val_loss: 8.4268e-09\n",
      "\n",
      "Start of epoch 2561\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.1015e-09 - val_loss: 8.4122e-09\n",
      "\n",
      "Start of epoch 2562\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.0949e-09 - val_loss: 8.4077e-09\n",
      "\n",
      "Start of epoch 2563\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.0734e-09 - val_loss: 8.3943e-09\n",
      "\n",
      "Start of epoch 2564\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 9.0678e-09 - val_loss: 8.3903e-09\n",
      "\n",
      "Start of epoch 2565\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 9.0469e-09 - val_loss: 8.3777e-09\n",
      "\n",
      "Start of epoch 2566\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 9.0291e-09 - val_loss: 8.3687e-09\n",
      "\n",
      "Start of epoch 2567\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.9979e-09 - val_loss: 8.3396e-09\n",
      "\n",
      "Start of epoch 2568\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.9844e-09 - val_loss: 8.3415e-09\n",
      "\n",
      "Start of epoch 2569\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.9688e-09 - val_loss: 8.3216e-09\n",
      "\n",
      "Start of epoch 2570\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.9608e-09 - val_loss: 8.3091e-09\n",
      "\n",
      "Start of epoch 2571\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.9490e-09 - val_loss: 8.3059e-09\n",
      "\n",
      "Start of epoch 2572\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.9357e-09 - val_loss: 8.2906e-09\n",
      "\n",
      "Start of epoch 2573\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.9256e-09 - val_loss: 8.2815e-09\n",
      "\n",
      "Start of epoch 2574\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.9998e-09 - val_loss: 8.3228e-09\n",
      "\n",
      "Start of epoch 2575\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 9.0024e-09 - val_loss: 8.2773e-09\n",
      "\n",
      "Start of epoch 2576\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.9108e-09 - val_loss: 8.2587e-09\n",
      "\n",
      "Start of epoch 2577\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.8862e-09 - val_loss: 8.2475e-09\n",
      "\n",
      "Start of epoch 2578\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.8739e-09 - val_loss: 8.2386e-09\n",
      "\n",
      "Start of epoch 2579\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 8.8636e-09 - val_loss: 8.2299e-09\n",
      "\n",
      "Start of epoch 2580\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.8488e-09 - val_loss: 8.2178e-09\n",
      "\n",
      "Start of epoch 2581\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.8383e-09 - val_loss: 8.1953e-09\n",
      "\n",
      "Start of epoch 2582\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.8057e-09 - val_loss: 8.1566e-09\n",
      "\n",
      "Start of epoch 2583\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.7849e-09 - val_loss: 8.1325e-09\n",
      "\n",
      "Start of epoch 2584\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.7928e-09 - val_loss: 8.1241e-09\n",
      "\n",
      "Start of epoch 2585\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.7649e-09 - val_loss: 8.1111e-09\n",
      "\n",
      "Start of epoch 2586\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.7682e-09 - val_loss: 8.1058e-09\n",
      "\n",
      "Start of epoch 2587\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 8.7388e-09 - val_loss: 8.0943e-09\n",
      "\n",
      "Start of epoch 2588\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 8.7190e-09 - val_loss: 8.0823e-09\n",
      "\n",
      "Start of epoch 2589\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.7033e-09 - val_loss: 8.0709e-09\n",
      "\n",
      "Start of epoch 2590\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.6816e-09 - val_loss: 8.0450e-09\n",
      "\n",
      "Start of epoch 2591\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.6651e-09 - val_loss: 8.0280e-09\n",
      "\n",
      "Start of epoch 2592\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.6573e-09 - val_loss: 8.0172e-09\n",
      "\n",
      "Start of epoch 2593\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.6388e-09 - val_loss: 8.0027e-09\n",
      "\n",
      "Start of epoch 2594\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.6464e-09 - val_loss: 7.9921e-09\n",
      "\n",
      "Start of epoch 2595\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 8.6154e-09 - val_loss: 7.9763e-09\n",
      "\n",
      "Start of epoch 2596\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.6188e-09 - val_loss: 7.9693e-09\n",
      "\n",
      "Start of epoch 2597\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.5831e-09 - val_loss: 7.9594e-09\n",
      "\n",
      "Start of epoch 2598\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.5845e-09 - val_loss: 7.9510e-09\n",
      "\n",
      "Start of epoch 2599\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 8.5490e-09 - val_loss: 7.9369e-09\n",
      "\n",
      "Start of epoch 2600\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.5249e-09 - val_loss: 7.9203e-09\n",
      "\n",
      "Start of epoch 2601\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.5033e-09 - val_loss: 7.9080e-09\n",
      "\n",
      "Start of epoch 2602\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 8.4859e-09 - val_loss: 7.8934e-09\n",
      "\n",
      "Start of epoch 2603\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 8.4651e-09 - val_loss: 7.8798e-09\n",
      "\n",
      "Start of epoch 2604\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 8.4483e-09 - val_loss: 7.8654e-09\n",
      "\n",
      "Start of epoch 2605\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.4519e-09 - val_loss: 7.8605e-09\n",
      "\n",
      "Start of epoch 2606\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.4408e-09 - val_loss: 7.8490e-09\n",
      "\n",
      "Start of epoch 2607\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.4073e-09 - val_loss: 7.8576e-09\n",
      "\n",
      "Start of epoch 2608\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.3828e-09 - val_loss: 7.8049e-09\n",
      "\n",
      "Start of epoch 2609\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.3893e-09 - val_loss: 7.7836e-09\n",
      "\n",
      "Start of epoch 2610\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.3573e-09 - val_loss: 7.7775e-09\n",
      "\n",
      "Start of epoch 2611\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 8.3302e-09 - val_loss: 7.7622e-09\n",
      "\n",
      "Start of epoch 2612\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 8.3168e-09 - val_loss: 7.7559e-09\n",
      "\n",
      "Start of epoch 2613\n",
      "1984/1984 [==============================] - 0s 29us/step - train_loss: 8.2957e-09 - val_loss: 7.7470e-09\n",
      "\n",
      "Start of epoch 2614\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.2905e-09 - val_loss: 7.7444e-09\n",
      "\n",
      "Start of epoch 2615\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.2763e-09 - val_loss: 7.7404e-09\n",
      "\n",
      "Start of epoch 2616\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 8.2633e-09 - val_loss: 7.7382e-09\n",
      "\n",
      "Start of epoch 2617\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 8.2640e-09 - val_loss: 7.7393e-09\n",
      "\n",
      "Start of epoch 2618\n",
      "1984/1984 [==============================] - 0s 40us/step - train_loss: 8.2418e-09 - val_loss: 7.7322e-09\n",
      "\n",
      "Start of epoch 2619\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 8.2395e-09 - val_loss: 7.7380e-09\n",
      "\n",
      "Start of epoch 2620\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.2169e-09 - val_loss: 7.7304e-09\n",
      "\n",
      "Start of epoch 2621\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.2241e-09 - val_loss: 7.7370e-09\n",
      "\n",
      "Start of epoch 2622\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.2002e-09 - val_loss: 7.7352e-09\n",
      "\n",
      "Start of epoch 2623\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.2264e-09 - val_loss: 7.7641e-09\n",
      "\n",
      "Start of epoch 2624\n",
      "1984/1984 [==============================] - 0s 30us/step - train_loss: 8.1695e-09 - val_loss: 7.7357e-09\n",
      "\n",
      "Start of epoch 2625\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 8.2593e-09 - val_loss: 7.7893e-09\n",
      "\n",
      "Start of epoch 2626\n",
      "1984/1984 [==============================] - 0s 41us/step - train_loss: 8.1360e-09 - val_loss: 7.6913e-09\n",
      "\n",
      "Start of epoch 2627\n",
      "1984/1984 [==============================] - 0s 38us/step - train_loss: 8.3462e-09 - val_loss: 7.8493e-09\n",
      "\n",
      "Start of epoch 2628\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 8.0570e-09 - val_loss: 7.6238e-09\n",
      "\n",
      "Start of epoch 2629\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 8.9875e-09 - val_loss: 8.0823e-09\n",
      "\n",
      "Start of epoch 2630\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 8.6238e-09 - val_loss: 7.6161e-09\n",
      "\n",
      "Start of epoch 2631\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 1.1060e-08 - val_loss: 8.8094e-09\n",
      "\n",
      "Start of epoch 2632\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 8.8926e-09 - val_loss: 7.7132e-09\n",
      "\n",
      "Start of epoch 2633\n",
      "1984/1984 [==============================] - 0s 39us/step - train_loss: 7.9672e-09 - val_loss: 7.5362e-09\n",
      "\n",
      "Start of epoch 2634\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 8.0309e-09 - val_loss: 7.5951e-09\n",
      "\n",
      "Start of epoch 2635\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 8.0204e-09 - val_loss: 7.5840e-09\n",
      "\n",
      "Start of epoch 2636\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 8.0171e-09 - val_loss: 7.5767e-09\n",
      "\n",
      "Start of epoch 2637\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.0110e-09 - val_loss: 7.5756e-09\n",
      "\n",
      "Start of epoch 2638\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 8.0108e-09 - val_loss: 7.5694e-09\n",
      "\n",
      "Start of epoch 2639\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 8.0037e-09 - val_loss: 7.5667e-09\n",
      "\n",
      "Start of epoch 2640\n",
      "1984/1984 [==============================] - 0s 37us/step - train_loss: 8.0066e-09 - val_loss: 7.5731e-09\n",
      "\n",
      "Start of epoch 2641\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 8.0113e-09 - val_loss: 7.5855e-09\n",
      "\n",
      "Start of epoch 2642\n",
      "1984/1984 [==============================] - 0s 36us/step - train_loss: 8.0194e-09 - val_loss: 7.6564e-09\n",
      "\n",
      "Start of epoch 2643\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.0253e-09 - val_loss: 7.6264e-09\n",
      "\n",
      "Start of epoch 2644\n",
      "1984/1984 [==============================] - 0s 31us/step - train_loss: 8.0344e-09 - val_loss: 7.6099e-09\n",
      "\n",
      "Start of epoch 2645\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.0129e-09 - val_loss: 7.6041e-09\n",
      "\n",
      "Start of epoch 2646\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.0462e-09 - val_loss: 7.6503e-09\n",
      "\n",
      "Start of epoch 2647\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 7.9781e-09 - val_loss: 7.6233e-09\n",
      "\n",
      "Start of epoch 2648\n",
      "1984/1984 [==============================] - 0s 47us/step - train_loss: 8.1460e-09 - val_loss: 7.7059e-09\n",
      "\n",
      "Start of epoch 2649\n",
      "1984/1984 [==============================] - 0s 35us/step - train_loss: 7.9228e-09 - val_loss: 7.5692e-09\n",
      "\n",
      "Start of epoch 2650\n",
      "1984/1984 [==============================] - 0s 33us/step - train_loss: 8.7614e-09 - val_loss: 8.0805e-09\n",
      "\n",
      "Start of epoch 2651\n",
      "1984/1984 [==============================] - 0s 32us/step - train_loss: 8.5629e-09 - val_loss: 7.7211e-09\n",
      "\n",
      "Start of epoch 2652\n",
      "1984/1984 [==============================] - 0s 34us/step - train_loss: 1.2333e-08 - val_loss: 9.6625e-09\n",
      "\n",
      "Start of epoch 2653\n",
      "Validation loss did not improve for 20 epochs. Training stopped.\n",
      "Best Validation Loss : 7.53617435123033e-09\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ae = AutoEncoder()\n",
    "ae.build(input_shape=(None, 64, 8))\n",
    "ae.summary()\n",
    "\n",
    "trainer = Trainer(model=ae,\n",
    "                  epochs=epochs,\n",
    "                  loss_function=loss_function,\n",
    "                  optimizer=optimizer,\n",
    "                  batch=batch_size,\n",
    "                  patience=20)\n",
    "trainer.train(train_data=train_data,\n",
    "              valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(trainer.history, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_pred = ae.predict(train_data)\n",
    "train_mse = np.mean(np.power(train_data - train_pred, 2), axis=1)\n",
    "threshold = train_mse.max()\n",
    "\n",
    "pred = ae.predict(test_data)\n",
    "\n",
    "mse = np.mean(np.power(test_data - pred, 2), axis=1)\n",
    "error_df = pd.DataFrame({\"reconstruction_error\" : mse, 'type' : test.type})\n",
    "error_df[\"reconstruction_error\"].plot(kind=\"hist\",bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7027\n",
      "1     362\n",
      "Name: label, dtype: int64\n",
      "\n",
      "type별 개수\n",
      "0    143\n",
      "6    101\n",
      "2     40\n",
      "5     30\n",
      "3     26\n",
      "4     13\n",
      "1      5\n",
      "7      4\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv(\"./data/answer_sample.csv\")\n",
    "\n",
    "error_df['label'] = 0\n",
    "\n",
    "index = error_df.reconstruction_error >= threshold\n",
    "error_df.iloc[index, 2] = 1\n",
    "\n",
    "sub[\"label\"] = error_df[\"label\"]\n",
    "\n",
    "print(sub[\"label\"].value_counts())\n",
    "print(\"\\ntype별 개수\")\n",
    "print(sub[sub.label == 1].type.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "ori_test = pd.read_csv('./data/test_data.csv')\n",
    "ori_test[\"label\"] = sub[\"label\"].values\n",
    "\n",
    "outliers=sub.loc[ori_test['label']==1]\n",
    "outlier_index=list(outliers.index)\n",
    "\n",
    "pca = PCA(2)\n",
    "res = pd.DataFrame(pca.fit_transform(ori_test))\n",
    "\n",
    "plt.title(\"result - PCA\")\n",
    "b1 = plt.scatter(res[0], res[1], c='gray', s=20, label='normal')\n",
    "b2 = plt.scatter(res.iloc[outlier_index, 0], res.iloc[outlier_index, 1], c='red', s=20, edgecolor=\"red\", label='outliers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7006\n",
      "1     383\n",
      "Name: label, dtype: int64\n",
      "\n",
      "type별 개수\n",
      "0    146\n",
      "6    101\n",
      "2     55\n",
      "5     30\n",
      "3     27\n",
      "4     13\n",
      "1      7\n",
      "7      4\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "col_threshold = np.power(train_data - train_pred, 2).max(axis=0)\n",
    "\n",
    "error_df = pd.DataFrame({'anomalies' : (np.power(test_data - pred, 2) > col_threshold).sum(axis=1), 'type' : test.type})\n",
    "error_df[error_df['anomalies'] != 0] = 1\n",
    "\n",
    "sub = pd.read_csv(\"./data/answer_sample.csv\")\n",
    "\n",
    "sub[\"label\"] = error_df[\"anomalies\"]\n",
    "\n",
    "print(sub[\"label\"].value_counts())\n",
    "print(\"\\ntype별 개수\")\n",
    "print(sub[sub.label == 1].type.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "ori_test = pd.read_csv('./data/test_data.csv')\n",
    "ori_test[\"label\"] = sub[\"label\"].values\n",
    "\n",
    "outliers=sub.loc[ori_test['label']==1]\n",
    "outlier_index=list(outliers.index)\n",
    "\n",
    "pca = PCA(2)\n",
    "res = pd.DataFrame(pca.fit_transform(ori_test))\n",
    "\n",
    "plt.title(\"result - PCA\")\n",
    "b1 = plt.scatter(res[0], res[1], c='gray', s=20, label='normal')\n",
    "b2 = plt.scatter(res.iloc[outlier_index, 0], res.iloc[outlier_index, 1], c='red', s=20, edgecolor=\"red\", label='outliers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"AE_8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLOP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
